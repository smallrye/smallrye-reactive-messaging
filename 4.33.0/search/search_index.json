{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SmallRye Reactive Messaging","text":"<p>SmallRye Reactive Messaging is a framework for building event-driven, data streaming, and event-sourcing applications.</p> <p>It lets your application interact using various messaging technologies such as Apache Kafka, AMQP or MQTT. The framework provides a flexible programming model bridging CDI and event-driven.</p> <p>SmallRye Reactive Messaging is an implementation of the Eclipse MicroProfile Reactive Messaging specification 3.0.1.</p> Event-Driven Architectures <p>SmallRye Reactive Messaging is used in Quarkus, and Open Liberty.</p> <p>Start with the Getting Started guide to know how it can be used and introduce various features.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>The easiest way to start using SmallRye Reactive Messaging is from Quarkus. SmallRye Reactive Messaging can also be used standalone, or with Open Liberty.</p> <p>First, go to code.quarkus.io. Select the <code>smallrye-reactive-messaging</code> extension (already done if you use the link), and then click on the generate button to download the code.</p> <p>One downloaded, unzip the project and import it in your IDE.</p> <p>If you look at the <code>pom.xml</code> file, you will see the following dependency:</p> <pre><code> &lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-smallrye-reactive-messaging&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>It provides the support for SmallRye Reactive Messaging.</p> <p>Ok, so far so good, but we need event-driven beans.</p> <p>Create the <code>quickstart</code> package, and copy the following class into it:</p> <p>For instance:</p> <pre><code>package quickstart;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class ReactiveMessagingExample {\n\n    @Outgoing(\"source\")\n    public Multi&lt;String&gt; source() {\n        return Multi.createFrom().items(\"hello\", \"from\", \"SmallRye\", \"reactive\", \"messaging\");\n    }\n\n    @Incoming(\"source\")\n    @Outgoing(\"processed-a\")\n    public String toUpperCase(String payload) {\n        return payload.toUpperCase();\n    }\n\n    @Incoming(\"processed-a\")\n    @Outgoing(\"processed-b\")\n    public Multi&lt;String&gt; filter(Multi&lt;String&gt; input) {\n        return input.select().where(item -&gt; item.length() &gt; 4);\n    }\n\n    @Incoming(\"processed-b\")\n    public void sink(String word) {\n        System.out.println(\"&gt;&gt; \" + word);\n    }\n\n}\n</code></pre> <p>This class contains a set of methods:</p> <ul> <li>producing messages (<code>source</code>)</li> <li>processing messages (<code>toUpperCase</code>)</li> <li>transforming the stream by skipping messages (<code>filter</code>)</li> <li>consuming messages (<code>sink</code>)</li> </ul> <p>Each of these methods are connected through channels.</p> <p>Now, let's see this in action. For the terminal, run:</p> <pre><code>&gt; ./mvnw quarkus:dev\n</code></pre> <p>Running the previous example should give the following output:</p> <pre><code>&gt;&gt; HELLO\n&gt;&gt; SMALLRYE\n&gt;&gt; REACTIVE\n&gt;&gt; MESSAGE\n</code></pre> <p>Of course, this is a very simple example. To go further, let's have a look to the core concepts behind SmallRye Reactive Messaging.</p>"},{"location":"amqp/amqp/","title":"AMQP 1.0 Connector","text":"<p>The AMQP Connector adds support for AMQP 1.0 to Reactive Messaging.</p> <p>Advanced Message Queuing Protocol 1.0 (AMQP 1.0) is an open standard for passing business messages between applications or organizations.</p> <p>With this connector, your application can:</p> <ul> <li>receive messages from an AMQP Broker or Router.</li> <li>send <code>Message</code> to an AMQP address</li> </ul> <p>The AMQP connector is based on the Vert.x AMQP Client.</p>"},{"location":"amqp/amqp/#using-the-amqp-connector","title":"Using the AMQP connector","text":"<p>To use the AMQP Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-amqp&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-amqp</code>.</p> <p>So, to indicate that a channel is managed by this connector you need:</p> <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-amqp\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-amqp\n</code></pre> <p>RabbitMQ</p> <p>To use RabbitMQ, refer to Using RabbitMQ.</p>"},{"location":"amqp/client-customization/","title":"Customizing the underlying AMQP client","text":"<p>You can customize the underlying AMQP Client configuration by producing an instance of <code>AmqpClientOptions</code>:</p> <pre><code>@Produces\n@Identifier(\"my-options\")\npublic AmqpClientOptions getNamedOptions() {\n    // You can use the produced options to configure the TLS connection\n    PemKeyCertOptions keycert = new PemKeyCertOptions()\n            .addCertPath(\"./tls/tls.crt\")\n            .addKeyPath(\"./tls/tls.key\");\n    PemTrustOptions trust = new PemTrustOptions().addCertPath(\"./tlc/ca.crt\");\n\n    return new AmqpClientOptions()\n            .setSsl(true)\n            .setPemKeyCertOptions(keycert)\n            .setPemTrustOptions(trust)\n            .addEnabledSaslMechanism(\"EXTERNAL\")\n            .setHostnameVerificationAlgorithm(\"\") // Disable hostname verification\n            .setConnectTimeout(30000)\n            .setReconnectInterval(5000)\n            .setContainerId(\"my-container\");\n}\n</code></pre> <p>This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the <code>client-options-name</code> attribute:</p> <pre><code>mp.messaging.incoming.prices.client-options-name=my-named-options\n</code></pre> <p>If you experience frequent disconnections from the broker, the AmqpClientOptions can also be used to set a heartbeat if you need to keep the AMQP connection permanently.  Some brokers might terminate the AMQP connection after a certain idle timeout.  You can provide a heartbeat value which will be used by the Vert.x proton client to advertise the idle timeout when opening transport to a remote peer.</p> <pre><code>@Produces\n@Identifier(\"my-named-options\")\npublic AmqpClientOptions getNamedOptions() {\n  // set a heartbeat of 30s (in milliseconds)\n  return new AmqpClientOptions()\n        .setHeartbeat(30000);\n}\n</code></pre>"},{"location":"amqp/client-customization/#client-capabilities","title":"Client capabilities","text":"<p>Both incoming and outgoing AMQP channels can be configured with a list of capabilities to declare during sender and receiver connections with the AMQP broker. Note that supported capability names are broker specific.</p> <pre><code>mp.messaging.incoming.sink.capabilities=temporary-topic\n...\nmp.messaging.outgoing.source.capabilities=shared\n</code></pre>"},{"location":"amqp/health/","title":"Health reporting","text":"<p>The AMQP connector reports the startup, liveness, and readiness of each inbound (Receiving messages) and outbound (sending messages) channel managed by the connector:</p> <ul> <li> <p>Startup :: For both inbound and outbound, the startup probe reports OK when the connection with the broker is established, and the AMQP senders and receivers are opened (the links are attached to the broker).</p> </li> <li> <p>Liveness :: For both inbound and outbound, the liveness check verifies that the connection is established. The check still returns OK if the connection got cut, but we are attempting a reconnection.</p> </li> <li> <p>Readiness :: For the inbound, it checks that the connection is established and the receiver is opened. Unlike the liveness check, this probe reports KO until the connection is re-established. For the outbound, it checks that the connection is established and the sender is opened. Unlike the liveness check, this probe reports KO until the connection is re-established.</p> </li> </ul> <p>Note</p> <p>To disable health reporting, set the <code>health-enabled</code> attribute for the channel to <code>false</code>.</p> <p>Note that a message processing failures nacks the message, which is then handled by the failure-strategy. It is the responsibility of the <code>failure-strategy</code> to report the failure and influence the outcome of the checks. The <code>fail</code> failure strategy reports the failure, and so the check will report the fault.</p>"},{"location":"amqp/rabbitmq/","title":"Using RabbitMQ","text":"<p>This connector is for AMQP 1.0. RabbitMQ implements AMQP 0.9.1. RabbitMQ does not provide AMQP 1.0 by default, but there is a plugin for it. To use RabbitMQ with this connector, enable and configure the AMQP 1.0 plugin.</p> <p>Despite the plugin, a few features won\u2019t work with RabbitMQ. Thus, we recommend the following configurations.</p> <p>To receive messages from RabbitMQ:</p> <ul> <li>Set <code>durable</code> to <code>false</code></li> </ul> <pre><code>mp.messaging.incoming.prices.connector=smallrye-amqp\nmp.messaging.incoming.prices.durable=false\n</code></pre> <p>To send messages to RabbitMQ:</p> <ul> <li>set the destination <code>address</code> (anonymous sender are not supported)</li> </ul> <pre><code>mp.messaging.outgoing.generated-price.connector=smallrye-amqp\nmp.messaging.outgoing.generated-price.address=prices\n</code></pre> <p>It\u2019s not possible to change the destination dynamically (using message metadata) when using RabbitMQ. The connector automatically detects that the broker does not support anonymous sender (See http://docs.oasis-open.org/amqp/anonterm/v1.0/anonterm-v1.0.html).</p> <p>Alternatively, you can use the RabbitMQ connector.</p>"},{"location":"amqp/receiving-amqp-messages/","title":"Receiving messages from AMQP","text":"<p>The AMQP connector lets you retrieve messages from an AMQP broker or router. The AMQP connector retrieves AMQP Messages and maps each of them into Reactive Messaging <code>Messages</code>.</p>"},{"location":"amqp/receiving-amqp-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis) running, and accessible using the <code>amqp:5672</code> address (by default it would use <code>localhost:5672</code>). Configure your application to receive AMQP Messages on the <code>prices</code> channel as follows:</p> <pre><code>amqp-host=amqp # &lt;1&gt;\namqp-port=5672 # &lt;2&gt;\namqp-username=my-username # &lt;3&gt;\namqp-password=my-password # &lt;4&gt;\n\nmp.messaging.incoming.prices.connector=smallrye-amqp # &lt;5&gt;\n</code></pre> <ol> <li> <p>Configures the broker/router host name. You can do it per channel     (using the <code>host</code> attribute) or globally using <code>amqp-host</code></p> </li> <li> <p>Configures the broker/router port. You can do it per channel (using     the <code>port</code> attribute) or globally using <code>amqp-port</code>. The default is     <code>5672</code>.</p> </li> <li> <p>Configures the broker/router username if required. You can do it per     channel (using the <code>username</code> attribute) or globally using     <code>amqp-username</code>.</p> </li> <li> <p>Configures the broker/router password if required. You can do it per     channel (using the <code>password</code> attribute) or globally using     <code>amqp-password</code>.</p> </li> <li> <p>Instructs the <code>prices</code> channel to be managed by the AMQP connector</p> </li> </ol> <p>Note</p> <p>You don\u2019t need to set the AMQP address. By default, it uses the channel name (<code>prices</code>). You can configure the <code>address</code> attribute to override it.</p> <p>Then, your application receives <code>Message&lt;Double&gt;</code>. You can consume the payload directly:</p> <pre><code>package amqp.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class AmqpPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(double price) {\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;Double&gt;</code>:</p> <pre><code>package amqp.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class AmqpPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;Double&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message, marking the AMQP message as `accepted`.\n        return price.ack();\n    }\n\n}\n</code></pre>"},{"location":"amqp/receiving-amqp-messages/#deserialization","title":"Deserialization","text":"<p>The connector converts incoming AMQP Messages into Reactive Messaging <code>Message&lt;T&gt;</code> instances. <code>T</code> depends on the body of the received AMQP Message.</p> <p>The AMQP Type System defines the supported types.</p> AMQP Body Type <code>&lt;T&gt;</code> AMQP Value containing a AMQP Primitive Type the corresponding Java type AMQP Value using the <code>Binary</code> type <code>byte[]</code> AMQP Sequence <code>List</code> AMQP Data (with binary content) and the <code>content-type</code> is set to <code>application/json</code> <code>JsonObject</code> AMQP Data with a different <code>content-type</code> <code>byte[]</code> <p>If you send objects with this AMQP connector (outbound connector), it gets encoded as JSON and sent as binary. The <code>content-type</code> is set to <code>application/json</code>. You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want:</p> <pre><code>@ApplicationScoped\npublic static class Generator {\n\n    @Outgoing(\"to-amqp\")\n    public Multi&lt;Price&gt; prices() {                      // &lt;1&gt;\n        AtomicInteger count = new AtomicInteger();\n        return Multi.createFrom().ticks().every(Duration.ofMillis(1000))\n                .map(l -&gt; new Price().setPrice(count.incrementAndGet()))\n                .onOverflow().drop();\n    }\n\n}\n\n@ApplicationScoped\npublic static class Consumer {\n\n    List&lt;Price&gt; prices = new CopyOnWriteArrayList&lt;&gt;();\n\n    @Incoming(\"from-amqp\")\n    public void consume(JsonObject p) {             // &lt;2&gt;\n        Price price = p.mapTo(Price.class);         // &lt;3&gt;\n        prices.add(price);\n    }\n\n    public List&lt;Price&gt; list() {\n        return prices;\n    }\n}\n</code></pre> <ol> <li> <p>The <code>Price</code> instances are automatically encoded to JSON by the     connector</p> </li> <li> <p>You can receive it using a <code>JsonObject</code></p> </li> <li> <p>Then, you can reconstruct the instance using the <code>mapTo</code> method</p> </li> </ol>"},{"location":"amqp/receiving-amqp-messages/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from AMQP contains an instance of IncomingAmqpMetadata</p> <pre><code>Optional&lt;IncomingAmqpMetadata&gt; metadata = incoming.getMetadata(IncomingAmqpMetadata.class);\nmetadata.ifPresent(meta -&gt; {\n    String address = meta.getAddress();\n    String subject = meta.getSubject();\n    boolean durable = meta.isDurable();\n    // Use io.vertx.core.json.JsonObject\n    JsonObject properties = meta.getProperties();\n    // ...\n});\n</code></pre>"},{"location":"amqp/receiving-amqp-messages/#acknowledgement","title":"Acknowledgement","text":"<p>When a Reactive Messaging <code>Message</code> associated with an AMQP Message is acknowledged, it informs the broker that the message has been accepted.</p>"},{"location":"amqp/receiving-amqp-messages/#failure-management","title":"Failure Management","text":"<p>If a message produced from an AMQP message is nacked, a failure strategy is applied. The AMQP connector supports six strategies:</p> <ul> <li> <p><code>fail</code> - fail the application; no more AMQP messages will be     processed (default). The AMQP message is marked as rejected.</p> </li> <li> <p><code>accept</code> - this strategy marks the AMQP message as accepted. The     processing continues ignoring the failure. Refer to the accepted     delivery state     documentation.</p> </li> <li> <p><code>release</code> - this strategy marks the AMQP message as released. The     processing continues with the next message. The broker can redeliver     the message. Refer to the released delivery state     documentation.</p> </li> <li> <p><code>reject</code> - this strategy marks the AMQP message as rejected. The     processing continues with the next message. Refer to the rejected     delivery state     documentation.</p> </li> <li> <p><code>modified-failed</code> - this strategy marks the AMQP message as     modified and indicates that it failed (with the <code>delivery-failed</code>     attribute). The processing continues with the next message, but the     broker may attempt to redeliver the message. Refer to the modified     delivery state     documentation</p> </li> <li> <p><code>modified-failed-undeliverable-here</code> - this strategy marks the AMQP     message as modified and indicates that it failed (with the     <code>delivery-failed</code> attribute). It also indicates that the application     cannot process the message, meaning that the broker will not attempt     to redeliver the message to this node. The processing continues with     the next message. Refer to the modified delivery state     documentation</p> </li> </ul>"},{"location":"amqp/receiving-amqp-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false auto-acknowledgement Whether the received AMQP messages must be acknowledged when received boolean false <code>false</code> broadcast Whether the received AMQP messages must be dispatched to multiple subscribers boolean false <code>false</code> capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing, the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false <code>true</code> connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false <code>1000</code> container-id The AMQP container id string false durable Whether AMQP subscription is durable boolean false <code>false</code> failure-strategy Specify the failure strategy to apply when a message produced from an AMQP message is nacked. Accepted values are <code>fail</code> (default), <code>accept</code>, <code>release</code>, <code>reject</code>, <code>modified-failed</code>, <code>modified-failed-undeliverable-here</code> string false <code>fail</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false <code>3</code> host (amqp-host) The broker hostname string false <code>localhost</code> link-name The name of the link. If not set, the channel name is used. string false password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false <code>5672</code> reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false <code>100</code> reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false <code>10</code> retry-on-fail-attempts The number of tentative to retry on failure int false <code>6</code> retry-on-fail-interval The interval (in seconds) between two sending attempts int false <code>5</code> selector Sets a message selector. This attribute is used to define an <code>apache.org:selector-filter:string</code> filter on the source terminus, using SQL-based syntax to request the server filters which messages are delivered to the receiver (if supported by the server in question). Precise functionality supported and syntax needed can vary depending on the server. string false sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false <code>false</code> username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false <p>You can also pass any property supported by the Vert.x AMQP client as attribute.</p> <p>To use an existing address or queue, you need to configure the <code>address</code>, <code>container-id</code> and, optionally, the <code>link-name</code> attributes. For example, if you have an Apache Artemis broker configured with:</p> <pre><code>&lt;queues&gt;\n    &lt;queue name=\"people\"&gt;\n        &lt;address&gt;people&lt;/address&gt;\n        &lt;durable&gt;true&lt;/durable&gt;\n        &lt;user&gt;artemis&lt;/user&gt;\n    &lt;/queue&gt;\n&lt;/queues&gt;\n</code></pre> <p>You need the following configuration:</p> <pre><code>mp.messaging.incoming.people.connector=smallrye-amqp\nmp.messaging.incoming.people.durable=true\nmp.messaging.incoming.people.address=people\nmp.messaging.incoming.people.container-id=people\n</code></pre> <p>You may need to configure the <code>link-name</code> attribute, if the queue name is not the channel name:</p> <pre><code>mp.messaging.incoming.people-in.connector=smallrye-amqp\nmp.messaging.incoming.people-in.durable=true\nmp.messaging.incoming.people-in.address=people\nmp.messaging.incoming.people-in.container-id=people\nmp.messaging.incoming.people-in.link-name=people\n</code></pre>"},{"location":"amqp/receiving-amqp-messages/#receiving-cloud-events","title":"Receiving Cloud Events","text":"<p>The AMQP connector supports Cloud Events. When the connector detects a structured or binary Cloud Events, it adds a IncomingCloudEventMetadata into the metadata of the <code>Message</code>. <code>IncomingCloudEventMetadata</code> contains accessors to the mandatory and optional Cloud Event attributes.</p> <p>If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.</p>"},{"location":"amqp/receiving-amqp-messages/#binary-cloud-events","title":"Binary Cloud Events","text":"<p>For <code>binary</code> Cloud Events, all mandatory Cloud Event attributes must be set in the AMQP application properties, prefixed by <code>cloudEvents:</code> (as mandated by the protocol binding). The connector considers headers starting with the <code>cloudEvents:</code> prefix but not listed in the specification as extensions. You can access them using the <code>getExtension</code> method from <code>IncomingCloudEventMetadata</code>.</p> <p>The <code>datacontenttype</code> attribute is mapped to the <code>content-type</code> header of the record.</p>"},{"location":"amqp/receiving-amqp-messages/#structured-cloud-events","title":"Structured Cloud Events","text":"<p>For <code>structured</code> Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value.</p> <p>Structured Cloud Event must set the <code>content-type</code> header of the record to <code>application/cloudevents+json; charset=UTF-8</code>. The message body must be a valid JSON object containing at least all the mandatory Cloud Events attributes.</p> <p>If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event <code>data</code>.</p>"},{"location":"amqp/sending-amqp-messages/","title":"Sending messages to AMQP","text":"<p>The AMQP connector can write Reactive Messaging <code>Messages</code> as AMQP Messages.</p>"},{"location":"amqp/sending-amqp-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis) running, and accessible using the <code>amqp:5672</code> address (by default it would use <code>localhost:5672</code>). Configure your application to send the messages from the <code>prices</code> channel as AMQP Message as follows:</p> <p><pre><code>amqp-host=amqp  # &lt;1&gt;\namqp-port=5672  # &lt;2&gt;\namqp-username=my-username # &lt;3&gt;\namqp-password=my-password # &lt;4&gt;\n\nmp.messaging.outgoing.prices.connector=smallrye-amqp # &lt;5&gt;\n</code></pre> 1.  Configures the broker/router host name. You can do it per channel     (using the <code>host</code> attribute) or globally using <code>amqp-host</code></p> <ol> <li> <p>Configures the broker/router port. You can do it per channel (using     the <code>port</code> attribute) or globally using <code>amqp-port</code>. The default is     <code>5672</code>.</p> </li> <li> <p>Configures the broker/router username if required. You can do it per     channel (using the <code>username</code> attribute) or globally using     <code>amqp-username</code>.</p> </li> <li> <p>Configures the broker/router password if required. You can do it per     channel (using the <code>password</code> attribute) or globally using     <code>amqp-password</code>.</p> </li> <li> <p>Instructs the <code>prices</code> channel to be managed by the AMQP connector</p> </li> </ol> <p>Note</p> <p>You don\u2019t need to set the address. By default, it uses the channel name (<code>prices</code>). You can configure the <code>address</code> attribute to override it.</p> <p>Then, your application must send <code>Message&lt;Double&gt;</code> to the <code>prices</code> channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package amqp.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class AmqpPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package amqp.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class AmqpPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble()));\n    }\n\n}\n</code></pre>"},{"location":"amqp/sending-amqp-messages/#serialization","title":"Serialization","text":"<p>When receiving a <code>Message&lt;T&gt;</code>, the connector convert the message into an AMQP Message. The payload is converted to the AMQP Message body.</p> <code>T</code> AMQP Message Body primitive types or <code>String</code> AMQP Value with the payload <code>Instant</code> or <code>UUID</code> AMQP Value using the corresponding AMQP Type <code>JsonObject</code> or <code>JsonArray</code> AMQP Data using a binary content. The <code>content-type</code> is set to <code>application/json</code> <code>io.vertx.mutiny.core.buffer.Buffer</code> AMQP Data using a binary content. No <code>content-type</code> set Any other class The payload is converted to JSON (using a Json Mapper). The result is wrapped into AMQP Data using a binary content. The <code>content-type</code> is set to <code>application/json</code> <p>If the message payload cannot be serialized to JSON, the message is nacked.</p>"},{"location":"amqp/sending-amqp-messages/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of OutgoingAmqpMetadata to influence how the message is going to be sent to AMQP. For example, you can configure the subjects, properties:</p> <pre><code>OutgoingAmqpMetadata metadata = OutgoingAmqpMetadata.builder()\n        .withDurable(true)\n        .withSubject(\"my-subject\")\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre>"},{"location":"amqp/sending-amqp-messages/#dynamic-address-names","title":"Dynamic address names","text":"<p>Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the address inside your application configuration file, but instead, use the outbound metadata to set the address.</p> <p>For example, you can send to a dynamic address based on the incoming message:</p> <pre><code>String addressName = selectAddressFromIncommingMessage(incoming);\nOutgoingAmqpMetadata metadata = OutgoingAmqpMetadata.builder()\n        .withAddress(addressName)\n        .withDurable(true)\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre> <p>Note</p> <p>To be able to set the address per message, the connector is using an anonymous sender.</p>"},{"location":"amqp/sending-amqp-messages/#acknowledgement","title":"Acknowledgement","text":"<p>By default, the Reactive Messaging <code>Message</code> is acknowledged when the broker acknowledged the message. When using routers, this acknowledgement may not be enabled. In this case, configure the <code>auto-acknowledgement</code> attribute to acknowledge the message as soon as it has been sent to the router.</p> <p>If an AMQP message is rejected/released/modified by the broker (or cannot be sent successfully), the message is nacked.</p>"},{"location":"amqp/sending-amqp-messages/#back-pressure-and-credits","title":"Back Pressure and Credits","text":"<p>The back-pressure is handled by the <code>max-inflight-messages</code> attribute and AMQP credits. The outbound connector requests messages minimum between <code>max-inflight-messages</code> and credits allowed by the broker. When the amount of credits reaches 0, it waits (in a non-blocking fashion) until the broker grants more credits to the AMQP sender.</p> <p>When <code>max-inflight-messages</code> is set to 0, only AMQP credits apply to limit the requests.</p> <p>Note that if an AMQP message send fails, it is retried until <code>retry-on-fail-attempts</code> is reached. If the client reconnects to the broker during the retry, failing messages are sent again but the message order is not preserved.</p> <p>To preserve the message order in this case you can set <code>max-inflight-messages</code> to</p>"},{"location":"amqp/sending-amqp-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing, the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false <code>true</code> cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default <code>datacontenttype</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>datacontenttype</code> attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default <code>dataschema</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>dataschema</code> attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the <code>time</code> attribute into the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>time</code> attribute itself boolean false <code>true</code> cloud-events-mode The Cloud Event mode (<code>structured</code> or <code>binary</code> (default)). Indicates how are written the cloud events in the outgoing record string false <code>binary</code> cloud-events-source (cloud-events-default-source) Configure the default <code>source</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>source</code> attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default <code>subject</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>subject</code> attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default <code>type</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>type</code> attribute itself string false connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false <code>1000</code> container-id The AMQP container id string false credit-retrieval-period The period (in milliseconds) between two attempts to retrieve the credits granted by the broker. This time is used when the sender run out of credits. int false <code>2000</code> durable Whether sent AMQP messages are marked durable boolean false <code>false</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false <code>3</code> host (amqp-host) The broker hostname string false <code>localhost</code> lazy-client Whether to create the connection and sender at startup or at first send request boolean false <code>true</code> link-name The name of the link. If not set, the channel name is used. string false max-inflight-messages The maximum number of messages to be written to the broker concurrently. The number of sent messages waiting to be acknowledged by the broker are limited by this value and credits granted by the broker. The default value <code>0</code> means only credits apply. long false <code>0</code> merge Whether the connector should allow multiple upstreams boolean false <code>false</code> password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false <code>5672</code> reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false <code>100</code> reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false <code>10</code> retry-on-fail-attempts The number of tentative to retry on failure int false <code>6</code> retry-on-fail-interval The interval (in seconds) between two sending attempts int false <code>5</code> sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> ttl The time-to-live of the send AMQP messages. 0 to disable the TTL long false <code>0</code> use-anonymous-sender Whether or not the connector should use an anonymous sender. Default value is <code>true</code> if the broker supports it, <code>false</code> otherwise. If not supported, it is not possible to dynamically change the destination address. boolean false use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false <code>false</code> username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false <p>You can also pass any property supported by the Vert.x AMQP client as attribute.</p>"},{"location":"amqp/sending-amqp-messages/#using-existing-destinations","title":"Using existing destinations","text":"<p>To use an existing address or queue, you need to configure the <code>address</code>, <code>container-id</code> and, optionally, the <code>link-name</code> attributes. For example, if you have an Apache Artemis broker configured with:</p> <pre><code>&lt;queues&gt;\n    &lt;queue name=\"people\"&gt;\n        &lt;address&gt;people&lt;/address&gt;\n        &lt;durable&gt;true&lt;/durable&gt;\n        &lt;user&gt;artemis&lt;/user&gt;\n    &lt;/queue&gt;\n&lt;/queues&gt;\n</code></pre> <p>You need the following configuration:</p> <pre><code>mp.messaging.outgoing.people.connector=smallrye-amqp\nmp.messaging.outgoing.people.durable=true\nmp.messaging.outgoing.people.address=people\nmp.messaging.outgoing.people.container-id=people\n</code></pre> <p>You may need to configure the <code>link-name</code> attribute, if the queue name is not the channel name:</p> <pre><code>mp.messaging.outgoing.people-out.connector=smallrye-amqp\nmp.messaging.outgoing.people-out.durable=true\nmp.messaging.outgoing.people-out.address=people\nmp.messaging.outgoing.people-out.container-id=people\nmp.messaging.outgoing.people-out.link-name=people\n</code></pre> <p>To use a <code>MULTICAST</code> queue, you need to provide the FQQN (Fully-qualified queue name) instead of just the name of the queue:</p> <pre><code>mp.messaging.outgoing.people-out.connector=smallrye-amqp\nmp.messaging.outgoing.people-out.durable=true\nmp.messaging.outgoing.people-out.address=foo\nmp.messaging.outgoing.people-out.container-id=foo\n\nmp.messaging.incoming.people-out.connector=smallrye-amqp\nmp.messaging.incoming.people-out.durable=true\nmp.messaging.incoming.people-out.address=foo::bar # Note the syntax: address-name::queue-name\nmp.messaging.incoming.people-out.container-id=bar\nmp.messaging.incoming.people-out.link-name=people\n</code></pre> <p>More details about the AMQP Address model can be found on the Artemis documentation.</p>"},{"location":"amqp/sending-amqp-messages/#sending-cloud-events","title":"Sending Cloud Events","text":"<p>The AMQP connector supports Cloud Events. The connector sends the outbound record as Cloud Events if:</p> <ul> <li> <p>the message metadata contains an     <code>io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata</code>     instance,</p> </li> <li> <p>the channel configuration defines the <code>cloud-events-type</code> and     <code>cloud-events-source</code> attributes.</p> </li> </ul> <p>You can create <code>io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata</code> instances using:</p> <pre><code>package amqp.outbound;\n\nimport java.net.URI;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata;\n\n@ApplicationScoped\npublic class AmqpCloudEventProcessor {\n\n    @Outgoing(\"cloud-events\")\n    public Message&lt;String&gt; toCloudEvents(Message&lt;String&gt; in) {\n        return in.addMetadata(OutgoingCloudEventMetadata.builder()\n                .withId(\"id-\" + in.getPayload())\n                .withType(\"greetings\")\n                .withSource(URI.create(\"http://example.com\"))\n                .withSubject(\"greeting-message\")\n                .build());\n    }\n\n}\n</code></pre> <p>If the metadata does not contain an id, the connector generates one (random UUID). The <code>type</code> and <code>source</code> can be configured per message or at the channel level using the <code>cloud-events-type</code> and <code>cloud-events-source</code> attributes. Other attributes are also configurable.</p> <p>The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values:</p> <pre><code>package amqp.outbound;\n\nimport java.net.URI;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata;\n\n@ApplicationScoped\npublic class AmqpCloudEventMultipleProcessors {\n\n    @Incoming(\"source\")\n    @Outgoing(\"processed\")\n    public Message&lt;String&gt; process(Message&lt;String&gt; in) {\n        return in.addMetadata(OutgoingCloudEventMetadata.builder()\n                .withId(\"id-\" + in.getPayload())\n                .withType(\"greeting\")\n                .build());\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Incoming(\"processed\")\n    @Outgoing(\"cloud-events\")\n    public Message&lt;String&gt; process2(Message&lt;String&gt; in) {\n        OutgoingCloudEventMetadata&lt;String&gt; metadata = in\n                .getMetadata(OutgoingCloudEventMetadata.class)\n                .orElseGet(() -&gt; OutgoingCloudEventMetadata.builder().build());\n\n        return in.addMetadata(OutgoingCloudEventMetadata.from(metadata)\n                .withSource(URI.create(\"source://me\"))\n                .withSubject(\"test\")\n                .build());\n    }\n\n}\n</code></pre> <p>By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the <code>cloud-events-mode</code> to <code>structured</code>. Only JSON is supported, so the created records had its <code>content-type</code> header set to <code>application/cloudevents+json; charset=UTF-8</code></p> <p>Note</p> <p>you can disable the Cloud Event support by setting the <code>cloud-events</code> attribute to <code>false</code></p>"},{"location":"camel/camel-processor/","title":"The processor pattern using Camel","text":"<p>Using the processor pattern, you can consume on a channel using a Camel component, and produce on a channel using another Camel component. In that case, the headers present in the incoming metadata will be forwarded in the outgoing metadata.</p>"},{"location":"camel/camel-processor/#example","title":"Example","text":"<p>Let\u2019s imagine you want to read messages from a Nats subject, process it and produce a message on a Kafka topic.</p> <p><pre><code>mp.messaging.incoming.mynatssubject.connector=smallrye-camel # &lt;1&gt;\nmp.messaging.incoming.mynatssubject.endpoint-uri=nats:mynatssubject # &lt;2&gt;\nmp.messaging.outgoing.mykafkatopic.connector=smallrye-camel # &lt;3&gt;\nmp.messaging.outgoing.mykafkatopic.endpoint-uri=kafka:mykafkatopic# &lt;4&gt;\n\ncamel.component.nats.servers=127.0.0.1:5555 # &lt;5&gt;\ncamel.component.kafka.brokers=127.0.0.1:9092 # &lt;6&gt;\n</code></pre> 1.  Sets the connector for the <code>mynatssubject</code> channel 2.  Configures the <code>endpoint-uri</code> for nats subject 3.  Sets the connector for the <code>mykafkatopic</code> channel 4.  Configures the <code>endpoint-uri</code> for the kafka topic 5.  Sets the URL of the nats server to use 6.  Sets the URL of a kafka broker to use</p> <pre><code>package camel.processor;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n@ApplicationScoped\npublic class CamelProcessor {\n\n    @Incoming(\"mynatssubject\")\n    @Outgoing(\"mykafkatopic\")\n    public byte[] process(byte[] message) {\n        // do some logic\n        return message;\n    }\n\n}\n</code></pre>"},{"location":"camel/camel/","title":"Apache Camel Connector","text":"<p>The Camel connector adds support for Apache Camel to Reactive Messaging.</p> <p>Camel is an open source integration framework let you integrate various systems consuming or producing data. Camel implements the Enterprise Integration Patterns and provides several hundred of components used to access databases, message queues, APIs or basically anything under the sun.</p> <p>Important</p> <p>Smallrye Reactive Messaging 4.x moved from <code>javax</code> to <code>jakarta</code> APIs therefore only supports Camel 4 releases. Similarly, this connector drops the Java 11 support as this is the case for Camel 4.</p>"},{"location":"camel/camel/#introduction","title":"Introduction","text":"<p>Camel is not a messaging broker. But, it allows your Reactive Messaging application to retrieve data from almost anything and send data to almost anything.</p> <p>So if you want to send Reactive Messaging <code>Message</code> to Telegram or retrieve data from Salesforce or SAP, this is the connector you need.</p> <p>One of the Camel cornerstone is the <code>endpoint</code> and its <code>uri</code> encoding the connection to an external system. For example, <code>file:orders/?delete=true&amp;charset=utf-8</code> instructs Camel to read the files from the <code>orders</code> directory. URI format and parameters are listed on the component documentation, such as the File component.</p>"},{"location":"camel/camel/#using-the-camel-connector","title":"Using the camel connector","text":"<p>To you the camel Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-camel&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>You will also need the dependency of the Camel component you are using. For example, if you want to process files, you would need to add the Camel File Component artifact:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.camel&lt;/groupId&gt;\n  &lt;artifactId&gt;camel-file&lt;/artifactId&gt;\n  &lt;version&gt;4.17.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-camel</code>.</p> <p>So, to indicate that a channel is managed by this connector you need:</p> <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-camel\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-camel\n</code></pre>"},{"location":"camel/receiving-messages-from-camel/","title":"Retrieving data using Camel","text":"<p>Camel provides many components. To keep this documentation focused on the integration with Camel, we use the File component. This component let use read files from a directory. So the connector configured with this component creates a <code>Message</code> for each file located in the directory. As soon as a file is dropped in the directory, a new <code>Message</code> is created.</p>"},{"location":"camel/receiving-messages-from-camel/#example","title":"Example","text":"<p>Let\u2019s imagine you want to read the files from the <code>orders</code> directory and send them to the <code>files</code> channel. Configuring the Camel connector to gets the file from this directory only requires 2 properties:</p> <p><pre><code>mp.messaging.incoming.files.connector=smallrye-camel # &lt;1&gt;\nmp.messaging.incoming.files.endpoint-uri=file:orders/?delete=true&amp;charset=utf-8 # &lt;2&gt;\n</code></pre> 1.  Sets the connector for the <code>files</code> channel 2.  Configures the <code>endpoint-uri</code></p> <p>Then, your application receives <code>Message&lt;GenericFile&lt;File&gt;&gt;</code>.</p> <p>Note</p> <p>The Camel File component produces <code>org.apache.camel.component.file.GenericFile</code> instances. You can retrieve the actual <code>File</code> using <code>getFile()</code>.</p> <p>You can consume the payload directly:</p> <pre><code>package camel.inbound;\n\nimport java.io.File;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.camel.component.file.GenericFile;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class CamelFileConsumer {\n\n    @Incoming(\"files\")\n    public void consume(GenericFile&lt;File&gt; gf) {\n        File file = gf.getFile();\n        // process the file\n\n    }\n\n}\n</code></pre> <p>You can also retrieve the <code>Message&lt;GenericFile&lt;File&gt;&gt;</code>:</p> <pre><code>package camel.inbound;\n\nimport java.io.File;\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.camel.component.file.GenericFile;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class CamelFileMessageConsumer {\n\n    @Incoming(\"files\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;GenericFile&lt;File&gt;&gt; msg) {\n        File file = msg.getPayload().getFile();\n        // process the file\n\n        return msg.ack();\n    }\n\n}\n</code></pre>"},{"location":"camel/receiving-messages-from-camel/#deserialization","title":"Deserialization","text":"<p>Each Camel component is producing specific objects. As we have seen, the File component produces <code>GenericFile</code>.</p> <p>Refer to the component documentation to check which type is produced.</p>"},{"location":"camel/receiving-messages-from-camel/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from Camel contains an instance of IncomingExchangeMetadata in the metadata.</p> <pre><code>package camel.inbound;\n\nimport java.io.File;\nimport java.util.Optional;\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.camel.Exchange;\nimport org.apache.camel.component.file.GenericFile;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.reactive.messaging.camel.IncomingExchangeMetadata;\n\n@ApplicationScoped\npublic class IncomingCamelMetadataExample {\n\n    @Incoming(\"files\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;GenericFile&lt;File&gt;&gt; msg) {\n        Optional&lt;IncomingExchangeMetadata&gt; metadata = msg.getMetadata(IncomingExchangeMetadata.class);\n        if (metadata.isPresent()) {\n            // Retrieve the camel exchange:\n            Exchange exchange = metadata.get().getExchange();\n        }\n        return msg.ack();\n    }\n\n}\n</code></pre> <p>This object lets you retrieve the Camel <code>Exchange</code>.</p>"},{"location":"camel/receiving-messages-from-camel/#failure-management","title":"Failure Management","text":"<p>If a message produced from a Camel exchange is nacked, a failure strategy is applied. The Camel connector supports 3 strategies:</p> <ul> <li><code>fail</code> - fail the application, no more MQTT messages will be     processed. (default) The offset of the record that has not been     processed correctly is not committed.</li> <li><code>ignore</code> - the failure is logged, but the processing continue.</li> </ul> <p>In both cases, the <code>exchange</code> is marked as rollback only and the nack reason is attached to the exchange.</p>"},{"location":"camel/receiving-messages-from-camel/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default endpoint-uri The URI of the Camel endpoint (read from or written to) string true failure-strategy Specify the failure strategy to apply when a message produced from a Camel exchange is nacked. Values can be <code>fail</code> (default) or <code>ignore</code> string false <code>fail</code>"},{"location":"camel/sending-messages-to-camel/","title":"Sending data with Camel","text":"<p>You can use the Camel connector to send data to almost any type of system.</p> <p>To keep this document focused on the Camel connector, we use the Camel File component. However, the connector can be used with any Camel component.</p>"},{"location":"camel/sending-messages-to-camel/#example","title":"Example","text":"<p>Let\u2019s imagine you want to write generated prices into files. Configure your application to write the messages from the <code>prices</code> channel into a files as follows:</p> <p><pre><code>mp.messaging.outgoing.prices.connector=smallrye-camel # &lt;1&gt;\nmp.messaging.outgoing.prices.endpoint-uri=file:prices/?fileName=${date:now:yyyyMMddssSS}.txt&amp;charset=utf-8 # &lt;2&gt;\n</code></pre> 1.  Sets the connector for the <code>prices</code> channel 2.  Configure the <code>endpoint-uri</code> to write into files in the <code>prices</code>     directory</p> <p>Important</p> <p>Depending on your implementation of MicroProfile Reactive Messaging, the <code>$</code> may need to be escaped as follows: <code>$${...}</code></p> <p>Then, your application must send <code>Message&lt;String&gt;</code> to the <code>prices</code> channel. It can use <code>String</code> payloads as in the following snippet:</p> <pre><code>package camel.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class CamelPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;String&gt; generate() {\n        // Build an infinite stream of random prices\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .onOverflow().drop()\n                .map(x -&gt; random.nextDouble())\n                .map(p -&gt; Double.toString(p));\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package camel.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class CamelPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;String&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble())\n                .map(p -&gt; Double.toString(p))\n                .map(Message::of);\n    }\n\n}\n</code></pre>"},{"location":"camel/sending-messages-to-camel/#serialization","title":"Serialization","text":"<p>The serialization is handled by the Camel component. Refer to the Camel documentation to check which payload type is supported by the component.</p>"},{"location":"camel/sending-messages-to-camel/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of OutgoingExchangeMetadata to the message metadata. You can then influence how the outbound Camel <code>Exchange</code> is created, for example by adding properties:</p> <pre><code>return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n        .map(x -&gt; random.nextDouble())\n        .map(p -&gt; Double.toString(p))\n        .map(s -&gt; Message.of(s)\n                .addMetadata(new OutgoingExchangeMetadata()\n                        .putProperty(\"my-property\", \"my-value\")));\n</code></pre>"},{"location":"camel/sending-messages-to-camel/#acknowledgement","title":"Acknowledgement","text":"<p>The incoming messages are acknowledged when the underlying Camel exchange completes. If the exchange fails, the message is nacked.</p>"},{"location":"camel/sending-messages-to-camel/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default endpoint-uri The URI of the Camel endpoint (read from or written to) string true merge Whether the connector should allow multiple upstreams boolean false <code>false</code>"},{"location":"camel/using-existing-routes/","title":"Using the Camel API","text":"<p>The Camel connector is based on the Reactive Streams support from Camel. If you have an application already using the Camel API (routes, <code>from</code>...), you can integrate it with Reactive Messaging.</p>"},{"location":"camel/using-existing-routes/#getting-the-camelreactivestreamsservice","title":"Getting the CamelReactiveStreamsService","text":"<p>Once you add the Camel connector to your application, you can retrieve the <code>org.apache.camel.component.reactive.streams.api.CamelReactiveStreamsService</code> object:</p> <pre><code>@Inject\nCamelReactiveStreamsService reactiveStreamsService;\n</code></pre> <p>This <code>CamelReactiveStreamsService</code> lets you create <code>Publisher</code> and <code>Subscriber</code> instances from existing routes.</p>"},{"location":"camel/using-existing-routes/#using-camel-route-with-outgoing","title":"Using Camel Route with @Outgoing","text":"<p>If you have an existing Camel route, you can transform it as a <code>Publisher</code> using the <code>CamelReactiveStreamsService</code>. Then, you can return this <code>Publisher</code> from a method annotated with <code>@Outgoing</code>:</p> <pre><code>@Outgoing(\"camel\")\npublic Publisher&lt;Exchange&gt; retrieveDataFromCamelRoute() {\n    return reactiveStreamsService.from(\"seda:camel\");\n}\n</code></pre> <p>You can also expose a <code>RouteBuilder</code> bean, making sure to use the <code>Singleton</code> scope, as <code>RouteBuilder</code> is no longer proxyable:</p> <pre><code>@Singleton\nstatic class MyRouteBuilder extends RouteBuilder {\n    @Inject\n    CamelReactiveStreamsService reactiveStreamsService;\n\n    @Outgoing(\"sink\")\n    public Publisher&lt;String&gt; getDataFromCamelRoute() {\n        return reactiveStreamsService.fromStream(\"my-stream\", String.class);\n    }\n\n    @Override\n    public void configure() {\n        from(\"seda:camel\").process(\n                exchange -&gt; exchange.getMessage().setBody(exchange.getIn().getBody(String.class).toUpperCase()))\n                .to(\"reactive-streams:my-stream\");\n    }\n}\n</code></pre> <p>Alternatively you can use the <code>LambdaRouteBuilder</code>:</p> <pre><code>@ApplicationScoped\nstatic class MyLambdaRouteBuilder {\n    @Inject\n    CamelReactiveStreamsService reactiveStreamsService;\n\n    @Outgoing(\"sink\")\n    public Publisher&lt;String&gt; getDataFromCamelRoute() {\n        return reactiveStreamsService.fromStream(\"my-stream\", String.class);\n    }\n\n    @Produces\n    @BindToRegistry\n    public LambdaRouteBuilder route() {\n        return rb -&gt; rb.from(\"seda:camel\").process(\n                exchange -&gt; exchange.getMessage().setBody(exchange.getIn().getBody(String.class).toUpperCase()))\n                .to(\"reactive-streams:my-stream\");\n    }\n}\n</code></pre>"},{"location":"camel/using-existing-routes/#using-camel-route-with-incoming","title":"Using Camel Route with @Incoming","text":"<p>If you have an existing Camel route, you can transform it as a <code>Subscriber</code> using the <code>CamelReactiveStreamsService</code>. Then, you can return this <code>Subscriber</code> from a method annotated with <code>@Incoming</code>:</p> <pre><code>@Incoming(\"to-camel\")\npublic Subscriber&lt;String&gt; sendDataToCamelRoute() {\n    return reactiveStreamsService.subscriber(\"file:./target?fileName=values.txt&amp;fileExist=append\",\n            String.class);\n}\n</code></pre> <p>You can also use a producer:</p> <pre><code>@Inject\nCamelContext camel;\n\n@Incoming(\"to-camel\")\npublic CompletionStage&lt;Void&gt; sink(String value) {\n    return camel.createProducerTemplate()\n            .asyncSendBody(\"file:./target?fileName=values.txt&amp;fileExist=append\", value).thenApply(x -&gt; null);\n}\n</code></pre>"},{"location":"concepts/acknowledgement/","title":"Acknowledgement","text":"<p>Acknowledgment is an essential concept in messaging. A message is acknowledged when its processing or reception has been successful. It allows the broker to move to the next message.</p> <p>How acknowledgment is used, and the exact behavior in terms of retry and resilience depends on the broker. For example, for Kafka, it would commit the offset. For AMQP, it would inform the broker that the message has been accepted.</p> <p>Reactive Messaging supports acknowledgement. The default acknowledgement depends on the method signature. Also, the acknowledgement policy can be configured using the <code>@Acknowledgement</code> annotation.</p>"},{"location":"concepts/acknowledgement/#chain-of-acknowledgment","title":"Chain of acknowledgment","text":"<p>If we reuse this example:</p> <pre><code>@Outgoing(\"source\")\npublic Multi&lt;String&gt; generate() {\n    return Multi.createFrom().items(\"Hello\", \"from\", \"reactive\", \"messaging\");\n}\n\n@Incoming(\"source\")\n@Outgoing(\"sink\")\npublic String process(String in) {\n    return in.toUpperCase();\n}\n\n@Incoming(\"sink\")\npublic void consume(String processed) {\n    System.out.println(processed);\n}\n</code></pre> <p>The framework automatically acknowledges the message received from the <code>sink</code> channel when the <code>consume</code> method returns. As a consequence, the message received by the <code>process</code> method is acknowledged, and so on. In other words, it creates a chain of acknowledgement - from the outbound channel to the inbound channel.</p> <p>When using connectors to receive and consume messages, the outbound connector acknowledges the messages when they are dispatched successfully to the broker. The acknowledgment chain would, as a result, acknowledges the inbound connector, which would be able to send an acknowledgment to the broker.</p> <p>This chain of acknowledgment is automatically implemented when processing payloads.</p>"},{"location":"concepts/acknowledgement/#acknowledgment-when-using-messages","title":"Acknowledgment when using Messages","text":"<p>When using <code>Messages</code>, the user controls the acknowledgment, and so the chain is not formed automatically. It gives you more flexibility about when and how the incoming messages are acknowledged.</p> <p>If you create a <code>Message</code> using the <code>with</code> method, is copy the acknowledgment function from the incoming message:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Message&lt;Integer&gt; process(Message&lt;Integer&gt; in) {\n    // The acknowledgement is forwarded, when the consumer\n    // acknowledges the message, `in` will be acknowledged\n    return in.withPayload(in.getPayload() + 1);\n}\n</code></pre> <p>To have more control over the acknowledgment, you can create a brand new <code>Message</code> and pass the acknowledgment function:</p> <pre><code>Message&lt;String&gt; message = Message.of(\"hello\", () -&gt; {\n    // called when the consumer acknowledges the message\n\n    // return a CompletionStage completed when the\n    // acknowledgment of the created message is\n    // completed.\n    // For immediate ack use:\n    return CompletableFuture.completedFuture(null);\n\n});\n</code></pre> <p>However, you may need to create the acknowledgment chain, to acknowledge the incoming message:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Message&lt;Integer&gt; processAndProduceNewMessage(Message&lt;Integer&gt; in) {\n    // The acknowledgement is forwarded, when the consumer\n    // acknowledges the message, `in` will be acknowledged\n    return Message.of(in.getPayload() + 1,\n            () -&gt; {\n                // Called when the consumer acknowledges the message\n                // ...\n                // Don't forget to acknowledge the incoming message:\n                return in.ack();\n            });\n}\n</code></pre> <p>To trigger the acknowledgment of the incoming message, use the <code>ack()</code> method. It returns a <code>CompletionStage</code>, receiving <code>null</code> as value when the acknowledgment has completed.</p>"},{"location":"concepts/acknowledgement/#acknowledgment-when-using-streams","title":"Acknowledgment when using streams","text":"<p>When transforming streams of <code>Message</code>, the acknowledgment is delegated to the user. It means that it\u2019s up to the user to acknowledge the incoming messages:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Publisher&lt;Message&lt;String&gt;&gt; transform(Multi&lt;Message&lt;String&gt;&gt; stream) {\n    return stream\n            .map(message -&gt; message.withPayload(message.getPayload().toUpperCase()));\n}\n</code></pre> <p>In the previous example, we only generate a single message per incoming message so that we can use the <code>with</code> method. It becomes more sophisticated when grouping incoming messages or when each incoming message produces multiple messages.</p> <p>In the case of a stream of payloads, the default strategy acknowledges the incoming messages before being processed by the method (regardless of the outcome).</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Publisher&lt;String&gt; transformPayload(Multi&lt;String&gt; stream) {\n    return stream\n            // The incoming messages are already acknowledged\n            .map(String::toUpperCase);\n}\n</code></pre> <p>For method receiving a single payload and producing a stream of payloads, it defaults to pre-processing acknowledgement. However, in this case, post-processing is supported. It waits for all the produced messages to be acknowledged before acknowledging the received one. If one of the produced message is nacked, the received one is nacked immediately.</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\n// Defaults to pre-processing, but post-processing is also supported\n@Acknowledgment(Acknowledgment.Strategy.POST_PROCESSING)\npublic Multi&lt;String&gt; transformPayload(String one) {\n    return Multi.createFrom().items(one, one);\n}\n</code></pre>"},{"location":"concepts/acknowledgement/#controlling-acknowledgement","title":"Controlling acknowledgement","text":"<p>The  Acknowledgment annotation lets you customize the default strategy presented in the previous sections. The <code>@Acknowledgement</code> annotation takes a strategy as parameter. Reactive Messaging proposed 4 strategies:</p> <ul> <li> <p><code>POST_PROCESSING</code> - the acknowledgement of the incoming message is     executed once the produced message is acknowledged.</p> </li> <li> <p><code>PRE_PROCESSING</code> - the acknowledgement of the incoming message is     executed before the message is processed by the method.</p> </li> <li> <p><code>MANUAL</code> - the acknowledgement is done by the user.</p> </li> <li> <p><code>NONE</code> - No acknowledgment is performed, neither manually or     automatically.</p> </li> </ul> <p>It is recommended to use <code>POST_PROCESSING</code> as it guarantees that the full processing has completed before acknowledging the incoming message. However, sometimes it\u2019s not possible, and this strategy is not available if you manipulate streams of <code>Messages</code> or payloads.</p> <p>The <code>PRE_PROCESSING</code> strategy can be useful to acknowledge a message early in the process:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\n@Acknowledgment(Acknowledgment.Strategy.PRE_PROCESSING)\npublic String process(String input) {\n    // The message wrapping the payload is already acknowledged\n    // The default would have waited the produced message to be\n    // acknowledged\n    return input.toUpperCase();\n}\n</code></pre> <p>It cuts the acknowledgment chain, meaning that the rest of the processing is not linked to the incoming message anymore. This strategy is the default strategy when manipulating streams of payloads.</p> <p>Refer to the signature list to determine which strategies are available for a specific method signature and what\u2019s the default strategy.</p>"},{"location":"concepts/acknowledgement/#negative-acknowledgement","title":"Negative acknowledgement","text":"<p>Messages can also be nacked, which indicates that the message was not processed correctly. The <code>Message.nack</code> method indicates failing processing (and supply the reason), and, as for successful acknowledgment, the nack is propagated through the chain of messages.</p> <p>If the message has been produced by a connector, this connector implements specific behavior when receiving a nack. It can fail (default), or ignore the failing, or implement a dead-letter queue mechanism. Refer to the connector documentation for further details about the available strategies.</p> <p>If the message is sent by an emitter using the <code>send(P)</code> method, the returned <code>CompletionStage</code> is completed exceptionally with the nack reason.</p> <pre><code>@Inject\n@Channel(\"data\")\nEmitter&lt;String&gt; emitter;\n\npublic void emitPayload() {\n    CompletionStage&lt;Void&gt; completionStage = emitter.send(\"hello\");\n    completionStage.whenComplete((acked, nacked) -&gt; {\n        if (nacked != null) {\n            // the processing has failed\n        }\n    });\n}\n</code></pre> <p>Negative acknowledgment can be manual or automatic. If your method handles instances of <code>Message</code> and the acknowledgment strategy is <code>MANUAL</code>, you can nack a message explicitly. You must indicate the reason (an exception) when calling the <code>nack</code> method. As for successful acknowledgment, the <code>nack</code> returns a <code>CompletionStage</code> completed when the <code>nack</code> has been processed.</p> <p>If your method uses the <code>POST_PROCESSING</code> acknowledgment strategy, and the method fails (either by throwing an exception or by producing a failure), the message is automatically nacked with the caught exception:</p> <pre><code>@Incoming(\"data\")\n@Outgoing(\"out\")\npublic String process(String s) {\n    if (s.equalsIgnoreCase(\"b\")) {\n        // Throwing an exception triggers a nack\n        throw new IllegalArgumentException(\"b\");\n    }\n\n    if (s.equalsIgnoreCase(\"e\")) {\n        // Returning null would skip the message (it will be acked)\n        return null;\n    }\n\n    return s.toUpperCase();\n}\n\n@Incoming(\"data\")\n@Outgoing(\"out\")\npublic Uni&lt;String&gt; processAsync(String s) {\n    if (s.equalsIgnoreCase(\"a\")) {\n        // Returning a failing Uni triggers a nack\n        return Uni.createFrom().failure(new Exception(\"a\"));\n    }\n\n    if (s.equalsIgnoreCase(\"b\")) {\n        // Throwing an exception triggers a nack\n        throw new IllegalArgumentException(\"b\");\n    }\n\n    if (s.equalsIgnoreCase(\"e\")) {\n        // Returning null would skip the message (it will be acked not nacked)\n        return Uni.createFrom().nullItem();\n    }\n\n    if (s.equalsIgnoreCase(\"f\")) {\n        // returning `null` is invalid for method returning Unis, the message is nacked\n        return null;\n    }\n\n    return Uni.createFrom().item(s.toUpperCase());\n}\n</code></pre>"},{"location":"concepts/advanced-config/","title":"Advanced configuration","text":""},{"location":"concepts/advanced-config/#strict-binding-mode","title":"Strict Binding Mode","text":"<p>By default, SmallRye Reactive Messaging does not enforce whether all mediators are connected. It just prints a warning message. The strict mode fails the deployment if some \"incomings\" are not bound to \"outgoings\". To enable this mode, you can pass the <code>-Dsmallrye-messaging-strict-binding=true</code> via the command line, or you can set the <code>smallrye-messaging-strict-binding</code> attribute to <code>true</code> in the configuration:</p> <pre><code>smallrye-messaging-strict-binding=true\n</code></pre>"},{"location":"concepts/advanced-config/#disabling-channels","title":"Disabling channels","text":"<p>You can disable a channel in the configuration by setting the <code>enabled</code> attribute to <code>false</code>:</p> <pre><code>mp.messaging.outgoing.dummy-sink.connector=dummy\nmp.messaging.outgoing.dummy-sink.enabled=false # Disable this channel\n</code></pre> <p>SmallRye Reactive Messaging does not register disabled channels, so make sure the rest of the application does not rely on them.</p>"},{"location":"concepts/advanced-config/#publisher-metrics","title":"Publisher metrics","text":"<p>SmallRye Reactive Messaging integrates MicroProfile Metrics and Micrometer for registering counter metrics (named <code>mp.messaging.message.count</code>) of published messages per channel.</p> <p>Both MicroProfile and Micrometer publisher metrics are enabled by default if found on the classpath. They can be disabled with <code>smallrye.messaging.metrics.mp.enabled</code> and <code>smallrye.messaging.metrics.micrometer.enabled</code> properties respectively.</p>"},{"location":"concepts/blocking/","title":"@Blocking","text":"<p>The <code>io.smallrye.reactive.messaging.annotations.Blocking</code> annotation can be used on a method annotated with <code>@Incoming</code>, or <code>@Outgoing</code> to indicate that the method should be executed on a worker pool:</p> <pre><code>@Outgoing(\"Y\")\n@Incoming(\"X\")\n@Blocking\npublic String process(String s) {\n  return s.toUpperCase();\n}\n</code></pre> <p>If method execution does not need to be ordered, it can be indicated on the <code>@Blocking</code> annotation:</p> <pre><code>@Outgoing(\"Y\")\n@Incoming(\"X\")\n@Blocking(ordered = false)\npublic String process(String s) {\n  return s.toUpperCase();\n}\n</code></pre> <p>When unordered, the invocation can happen concurrently.</p> <p>By default, use of <code>@Blocking</code> results in the method being executed in the Vert.x worker pool. If it\u2019s desired to execute methods on a custom worker pool, with specific concurrency needs, it can be defined on <code>@Blocking</code>:</p> <pre><code>@Outgoing(\"Y\")\n@Incoming(\"X\")\n@Blocking(\"my-custom-pool\")\npublic String process(String s) {\n  return s.toUpperCase();\n}\n</code></pre> <p>Specifying the concurrency for the above worker pool requires the following configuration property to be defined:</p> <pre><code>smallrye.messaging.worker.my-custom-pool.max-concurrency=3\n</code></pre>"},{"location":"concepts/blocking/#custom-worker-pool-shutdown-timeout","title":"Custom worker pool shutdown timeout","text":"<p>When using a custom worker pool, it is possible to define a shutdown timeout in order to wait for the scheduled tasks to complete before shutting down the pool. This can be done by setting the following configuration property:</p> <pre><code>smallrye.messaging.worker.my-custom-pool.max-concurrency=3\nsmallrye.messaging.worker.my-custom-pool.shutdown-timeout=10000\nsmallrye.messaging.worker.my-custom-pool.shutdown-check-interval=2000\n</code></pre> <p>The <code>shutdown-timeout</code> property defines the maximum time to wait in milliseconds for the scheduled tasks to complete before shutting down the pool, defaults to 60 seconds. The <code>shutdown-check-interval</code> property defines the interval in milliseconds at which the pool will check if the tasks have completed, defaults to 5 seconds.</p>"},{"location":"concepts/blocking/#supported-signatures","title":"Supported signatures","text":"<p><code>@Blocking</code> does not support every signature. The following table lists the supported ones.</p> Shape Signature Comment Publisher <code>@Outgoing(\"in\") @Blocking O generator()</code> Invokes the generator from a worker thread. If <code>ordered</code> is set to <code>false</code>, the generator can be called concurrently. Publisher <code>@Outgoing(\"in\")  @Blocking  Message&lt;O&gt; generator()</code> Invokes the generator from a worker thread. If <code>ordered</code> is set to <code>false</code>, the generator can be called concurrently. Processor <code>@Incoming(\"in\") @Outgoing(\"bar\") @Blocking O process(I in)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Processor <code>@Incoming(\"in\") @Outgoing(\"bar\") @Blocking Message&lt;O&gt; process(I in)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Subscriber <code>@Incoming(\"in\") @Blocking void consume(I in)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Subscriber <code>@Incoming(\"in\") @Blocking Uni&lt;Void&gt; consume(I in)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Subscriber <code>@Incoming(\"in\") @Blocking Uni&lt;Void&gt; consume(Message&lt;I&gt; msg)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Subscriber <code>@Incoming(\"in\") @Blocking CompletionStage&lt;Void&gt; consume(I in)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. Subscriber <code>@Incoming(\"in\") @Blocking CompletionStage&lt;Void&gt; consume(Message&lt;I&gt; msg)</code> Invokes the method on a worker thread. If <code>ordered</code> is set to <code>false</code>, the method can be called concurrently. <p>When a method can be called concurrently, the max concurrency depends on the number of threads from the worker thread pool.</p>"},{"location":"concepts/blocking/#using-iosmallryecommonannotationblocking","title":"Using io.smallrye.common.annotation.Blocking","text":"<p><code>io.smallrye.common.annotation.Blocking</code> is another annotation with the same semantic. <code>io.smallrye.common.annotation.Blocking</code> is used by multiple SmallRye projects and Quarkus.</p> <p>SmallRye Reactive Messaging also supports <code>io.smallrye.common.annotation.Blocking</code>. However, <code>io.smallrye.common.annotation.Blocking</code> does not allow configuring the ordering (it defaults to <code>ordered=true</code>).</p> <p>When both annotations are used, <code>io.smallrye.reactive.messaging.annotations.Blocking</code> is preferred.</p>"},{"location":"concepts/broadcast/","title":"Broadcast","text":"<p>Experimental</p> <p><code>@Broadcast</code> is an experimental feature.</p> <p>By default, messages transiting in a channel are only dispatched to a single consumer. Having multiple consumers is considered as an error, and is reported at deployment time.</p> <p>The Broadcast annotation changes this behavior and indicates that messages transiting in the channel are dispatched to all the consumers. <code>@Broadcast</code> must be used with the <code>@Outgoing</code> annotation:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\n@Broadcast\npublic int increment(int i) {\n    return i + 1;\n}\n\n@Incoming(\"out\")\npublic void consume1(int i) {\n    //...\n}\n\n@Incoming(\"out\")\npublic void consume2(int i) {\n    //...\n}\n</code></pre> <p>In the previous example, both consumers get the messages.</p> <p>You can also control the number of consumers to wait before starting to dispatch the messages. This allows waiting for the complete graph to be woven:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\n@Broadcast(2)\npublic int increment(int i) {\n    return i + 1;\n}\n\n@Incoming(\"out\")\npublic void consume1(int i) {\n    //...\n}\n\n@Incoming(\"out\")\npublic void consume2(int i) {\n    //...\n}\n</code></pre> <p>Note</p> <p>Inbound connectors also support a <code>broadcast</code> attribute that allows broadcasting the messages to multiple downstream subscribers.</p>"},{"location":"concepts/broadcast/#use-with-emitter","title":"Use with Emitter","text":"<p>For details on how to use <code>@Broadcast</code> with <code>Emitter</code> see the documentation.</p>"},{"location":"concepts/client-customizers/","title":"Client Customizers","text":"<p>Client customizers allow to customize the client instance created by the connector. Only connectors which create their own client instance support customizers.</p> <p>To define a client customizer, you need to provide a CDI bean that implements the <code>ClientCustomizer&lt;T&gt;</code> interface, parameterized with the config type:</p> <pre><code>package customizers;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.config.Config;\n\nimport io.smallrye.reactive.messaging.ClientCustomizer;\n\n@ApplicationScoped\npublic class MyClientCustomizer implements ClientCustomizer&lt;ClientConfig&gt; {\n    @Override\n    public ClientConfig customize(String channel, Config channelConfig, ClientConfig config) {\n        // customize the client configuration\n        return config;\n    }\n\n}\n</code></pre> <p>Connectors which support client customizers will discover all beans and call the <code>customize</code> method, with the channel name, the channel configuration and the configuration that'll be used to create the client instance. If the customizer returns <code>null</code> it'll be skipped.</p> <p>If you have multiple customizers, customizers can override the <code>getPriority</code> method to define the order in which they are called.</p> <p>Currently, the following core connectors support client customizers:</p> <ul> <li>Kafka:    <code>ClientCustomizer&lt;Map&lt;String, Object&gt;&gt;</code></li> <li>RabbitMQ: <code>ClientCustomizer&lt;RabbitMQOptions&gt;</code></li> <li>AMQP 1.0: <code>ClientCustomizer&lt;AmqpClientOptions&gt;</code></li> <li>MQTT:     <code>ClientCustomizer&lt;MqttClientSessionOptions&gt;</code></li> <li>Pulsar:   <code>ClientCustomizer&lt;ClientBuilder&gt;</code>, <code>ClientCustomizer&lt;ConsumerBuilder&lt;?&gt;&gt;</code>, <code>ClientCustomizer&lt;ProducerBuilder&lt;?&gt;&gt;</code></li> </ul>"},{"location":"concepts/concepts/","title":"Concepts","text":"<p>When dealing with event-driven or data streaming applications, there are a few concepts and vocabulary to introduce.</p>"},{"location":"concepts/concepts/#messages-payload-metadata","title":"Messages, Payload, Metadata","text":"<p>A <code>Message</code> is an envelope around a <code>payload</code>. Your application is going to receive, process, and send <code>Messages</code>.</p> <p>Your application\u2019s logic can generate these <code>Messages</code> or receive them from a message broker. They can also be consumed by your application or sent to a message broker.</p> An application can receive a message, process it and send a resulting message <p>In Reactive Messaging, <code>Message</code> are represented by the Message interface. Each <code>Message&lt;T&gt;</code> contains a payload of type <code>&lt;T&gt;</code>. This payload can be retrieved using <code>message.getPayload()</code>:</p> <pre><code>String payload = message.getPayload();\nOptional&lt;MyMetadata&gt; metadata = message.getMetadata(MyMetadata.class);\n</code></pre> <p>As you can see in the previous snippet, <code>Messages</code> can also have metadata. Metadata is a way to extend messages with additional data. It can be metadata related to the message broker (like KafkaMessageMetadata), or contain operational data (such as tracing metadata), or business-related data.</p> <p>Note</p> <p>When retrieving metadata, you get an <code>Optional</code> as it may not be  present.</p> <p>Tip</p> <p>Metadata is also used to influence the outbound dispatching (how the  message will be sent to the broker).</p>"},{"location":"concepts/concepts/#channels-and-streams","title":"Channels and Streams","text":"<p>Inside your application, <code>Messages</code> transit on channel. A channel is a virtual destination identified by a name.</p> The application is a set of channels <p>SmallRye Reactive Messaging connects the component to the channel they read and to the channel they populate. The resulting structure is a  stream: Messages flow between components through channels.</p> <p>What about Reactive Streams?</p> <p>You may wonder why Reactive Messaging has Reactive in the name. The Messaging part is kind of obvious. The Reactive part comes from the streams that are created by binding components. These streams are Reactive Streams. They follow the subscription and request protocol and implement back-pressure. It also means that Connectors are intended to use non-blocking IO to interact with the various message brokers.</p>"},{"location":"concepts/concepts/#connectors","title":"Connectors","text":"<p>Your application is interacting with messaging brokers or event backbone  using connectors. A connector is a piece of code that connects to a broker and:</p> <ol> <li> <p>subscribe/poll/receive messages from the broker and propagate them to the application</p> </li> <li> <p>send/write/dispatch messages provided by the application to the broker</p> </li> </ol> <p>Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. These collected messages are sent to the external broker.</p> Connectors manages the communication between the application and the brokers <p>Each connector is dedicated to a specific technology. For example, a  Kafka Connector only deals with Kafka.</p> <p>You don\u2019t necessarily need a connector. When your application does not  use connectors, everything happens in-memory, and the streams are created by chaining methods altogether. Each chain is still a reactive  stream and enforces the back-pressure protocol. When you don\u2019t use  connectors, you need to make sure the chain is complete, meaning it  starts with a message source, and it ends with a sink. In other words,  you need to generate messages from within the application (using a method with only <code>@Outgoing</code>, or an <code>Emitter</code>) and consume the messages from within the application (using a method with only <code>@Incoming</code> or using an unmanaged stream).</p>"},{"location":"concepts/connectors/","title":"Connectors","text":"<p>Reactive Messaging can handle messages generated from within the application but also interact with remote brokers. Reactive Messaging Connectors interacts with these remote brokers to retrieve messages and send messages using various protocols and technology.</p> <p>Each connector handles to a specific technology. For example, a Kafka Connector is responsible for interacting with Kafka, while an MQTT Connector is responsible for MQTT interactions.</p>"},{"location":"concepts/connectors/#connector-name","title":"Connector name","text":"<p>Each connector has a name. This name is referenced by the application to indicate that this connector manages a specific channel.</p> <p>For example, the SmallRye Kafka Connector is named: <code>smallrye-kafka</code>.</p>"},{"location":"concepts/connectors/#inbound-and-outbound-connectors","title":"Inbound and Outbound connectors","text":"<p>Connector can:</p> <ol> <li> <p>retrieve messages from a remote broker (inbound)</p> </li> <li> <p>send messages to a remote broker (outbound)</p> </li> </ol> <p>A connector can, of course, implement both directions.</p> <p>Inbound connectors are responsible for:</p> <ol> <li> <p>Getting messages from the remote broker,</p> </li> <li> <p>Creating a Reactive Messaging <code>Message</code> associated with the     retrieved message.</p> </li> <li> <p>Potentially associating technical metadata with the message. It     includes unmarshalling the payload.</p> </li> <li> <p>Associating an acknowledgment callback to acknowledge the incoming     message when the Reactive Messaging message is     processed/acknowledged.</p> </li> </ol> <p>Reactive matters</p> <p>The first step should follow the reactive streams principle: uses non-blocking technology, respects downstream requests.</p> <p>Outbound connectors are responsible for:</p> <ol> <li> <p>Receiving Reactive Messaging <code>Message</code> and transform it into a     structure understood by the remote broker. It includes marshaling     the payload.</p> </li> <li> <p>If the <code>Message</code> contains outbound metadata (metadata set during the     processing to influence the outbound structure and routing), taking     them into account.</p> </li> <li> <p>Sending the message to the remote broker.</p> </li> <li> <p>Acknowledging the Reactive Messaging <code>Message</code> when the broker has     accepted/acknowledged the message.</p> </li> </ol>"},{"location":"concepts/connectors/#configuring-connectors","title":"Configuring connectors","text":"<p>Applications need to configure the connector used by expressing which channel is managed by which connector. Non-mapped channels are local / in-memory.</p> <p>To configure connectors, you need to have an implementation of MicroProfile Config. If you don\u2019t have one, add an implementation of MicroProfile Config in your classpath, such as:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.config&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-config&lt;/artifactId&gt;\n  &lt;version&gt;3.15.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then edit the application configuration, generally <code>src/main/resources/META-INF/microprofile-config.properties</code>.</p> <p>The application configures the connector with a set of properties structured as follows:</p> <pre><code>mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value]\n</code></pre> <p>For example:</p> <pre><code>mp.messaging.incoming.dummy-incoming-channel.connector=dummy\nmp.messaging.incoming.dummy-incoming-channel.attribute=value\nmp.messaging.outgoing.dummy-outgoing-channel.connector=dummy\nmp.messaging.outgoing.dummy-outgoing-channel.attribute=value\n</code></pre> <p>You configure each channel (both incoming and outgoing) individually.</p> <p>The <code>[incoming|outgoing]</code> segment indicates the direction.</p> <ul> <li> <p>an <code>incoming</code> channel consumes data from a message broker or     something producing data. It\u2019s an inbound interaction. It relates to     methods annotated with an <code>@Incoming</code> using the same channel name.</p> </li> <li> <p>an <code>outgoing</code> consumes data from the application and forwards it to     a message broker or something consuming data. It\u2019s an outbound     interaction. It relates to methods annotated with an <code>@Outgoing</code>     using the same channel name.</p> </li> </ul> <p>The <code>[channel-name]</code> is the name of the channel. If the channel name contains a <code>.</code> (dot), you would need to use <code>\"</code> (double-quote) around it. For example, to configure the <code>dummy.incoming.channel</code> channel, you would need:</p> <pre><code>mp.messaging.incoming.\"dummy.incoming.channel\".connector=dummy\nmp.messaging.incoming.\"dummy.incoming.channel\".attribute=value\n</code></pre> <p>The <code>[attribute]=[value]</code> sets a specific connector attribute to the given value. Attributes depend on the used connector. So, refer to the connector documentation to check the supported attributes.</p> <p>The <code>connector</code> attribute must be set for each mapped channel and indicates the name of the connector responsible for the channel.</p> <p>Here is an example of a channel using an MQTT connector, consuming data from a MQTT broker, and a channel using a Kafka connector (writing data to Kafka):</p> <pre><code># Configure the incoming health channel\nmp.messaging.incoming.health.topic=neo\nmp.messaging.incoming.health.connector=smallrye-mqtt\nmp.messaging.incoming.health.host=localhost\nmp.messaging.incoming.health.broadcast=true\n\n# Configure outgoing data channel\nmp.messaging.outgoing.data.connector=smallrye-kafka\nmp.messaging.outgoing.data.bootstrap.servers=localhost:9092\nmp.messaging.outgoing.data.key.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.data.value.serializer=io.vertx.kafka.client.serialization.JsonObjectSerializer\nmp.messaging.outgoing.data.acks=1\n</code></pre> <p>Important</p> <p>To use a connector, you need to add it to your CLASSPATH. Generally, adding the dependency to your project is enough. Then, you need to know the connector\u2019s name and set the <code>connector</code> attribute for each channel managed by this connector.</p> <p>Connector names</p> <p>Connector managed by the SmallRye Reactive Messaging project are typically prefixed with <code>smallrye-</code>. For example, the SmallRye Kafka Connector is named <code>smallrye-kafka</code>. When using Smallrye connectors, you can omit the prefix and just use <code>kafka</code> as the connector name:</p> <pre><code>mp.messaging.incoming.my-channel.connector=kafka\nmp.messaging.connector.kafka.bootstrap.servers=localhost:9092\n</code></pre>"},{"location":"concepts/connectors/#connector-attribute-table","title":"Connector attribute table","text":"<p>In the connector documentation, you will find a table listing the attribute supported by the connector. Be aware that attributes for inbound and outbound interactions may be different.</p> <p>These tables contain the following entries:</p> <ol> <li> <p>The name of the attribute, and potentially an alias. The name of     the attribute is used with the     <code>mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value]</code>     syntax (the <code>attribute</code> segment). The alias (if set) is the name     of a global MicroProfile Config property that avoids having to     configure the attribute for each managed channel. For example, to     set the location of your Kafka broker globally, you can use the     <code>kafka.bootstrap.servers</code> alias.</p> </li> <li> <p>The description of the attribute, including the type.</p> </li> <li> <p>Whether that attribute is mandatory. If so, it fails the     deployment if not set</p> </li> <li> <p>The default value, if any.</p> </li> </ol>"},{"location":"concepts/contributing-connectors/","title":"Connector Contribution Guide","text":"<p>A connector implementation is a CDI-managed bean, typically an <code>@ApplicationScoped</code> bean, which is identified by the <code>@Connector</code> identifier. In order to provide inbound and outbound channels, the connector implements two interfaces <code>InboundConnector</code> and <code>OutboundConnector</code> respectively. In addition to that, the connector bean is annotated with <code>@ConnectorAttribute</code>, which describes attributes to configure channels.</p> <p>Maven Archetype</p> <p>Smallrye Reactive Messaging provides a Maven archetype to bootstrap a new connector. You can generate a new connector project with the code described in this guide using: <pre><code>mvn -N archetype:generate \\\n-DarchetypeGroupId=io.smallrye.reactive \\\n-DarchetypeArtifactId=smallrye-reactive-messaging-connector-archetype \\\n-DarchetypeVersion=4.33.0 \\\n-DgroupId=io.smallrye.reactive \\\n-Dpackage=io.smallrye.reactive.messaging.my \\\n-Dversion=4.33.0 \\\n-DartifactId=smallrye-reactive-messaging-my \\\n-DconnectorName=my\n</code></pre></p> <p>The following is an example of a connector skeleton :</p> <pre><code>package connectors;\n\nimport static io.smallrye.reactive.messaging.annotations.ConnectorAttribute.Direction.INCOMING;\nimport static io.smallrye.reactive.messaging.annotations.ConnectorAttribute.Direction.INCOMING_AND_OUTGOING;\nimport static io.smallrye.reactive.messaging.annotations.ConnectorAttribute.Direction.OUTGOING;\n\nimport java.util.List;\nimport java.util.concurrent.CopyOnWriteArrayList;\nimport java.util.concurrent.Flow;\n\nimport jakarta.annotation.PostConstruct;\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.config.Config;\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.spi.Connector;\n\nimport connectors.api.BrokerClient;\nimport io.smallrye.reactive.messaging.annotations.ConnectorAttribute;\nimport io.smallrye.reactive.messaging.connector.InboundConnector;\nimport io.smallrye.reactive.messaging.connector.OutboundConnector;\nimport io.smallrye.reactive.messaging.providers.connectors.ExecutionHolder;\nimport io.vertx.mutiny.core.Vertx;\n\n@ApplicationScoped\n@Connector(MyConnector.CONNECTOR_NAME)\n@ConnectorAttribute(name = \"client-id\", type = \"string\", direction = INCOMING_AND_OUTGOING, description = \"The client id \", mandatory = true)\n@ConnectorAttribute(name = \"buffer-size\", type = \"int\", direction = INCOMING, description = \"The size buffer of incoming messages waiting to be processed\", defaultValue = \"128\")\n@ConnectorAttribute(name = \"topic\", type = \"string\", direction = OUTGOING, description = \"The default topic to send the messages, defaults to channel name if not set\")\n@ConnectorAttribute(name = \"maxPendingMessages\", type = \"int\", direction = OUTGOING, description = \"The maximum size of a queue holding pending messages\", defaultValue = \"1000\")\n@ConnectorAttribute(name = \"waitForWriteCompletion\", type = \"boolean\", direction = OUTGOING, description = \"Whether the outgoing channel waits for the write completion\", defaultValue = \"true\")\npublic class MyConnector implements InboundConnector, OutboundConnector {\n\n    public static final String CONNECTOR_NAME = \"smallrye-my-connector\";\n\n    @Inject\n    ExecutionHolder executionHolder;\n\n    Vertx vertx;\n\n    List&lt;MyIncomingChannel&gt; incomingChannels = new CopyOnWriteArrayList&lt;&gt;();\n    List&lt;MyOutgoingChannel&gt; outgoingChannels = new CopyOnWriteArrayList&lt;&gt;();\n\n    @PostConstruct\n    void init() {\n        this.vertx = executionHolder.vertx();\n    }\n\n    @Override\n    public Flow.Publisher&lt;? extends Message&lt;?&gt;&gt; getPublisher(Config config) {\n        MyConnectorIncomingConfiguration ic = new MyConnectorIncomingConfiguration(config);\n        String channelName = ic.getChannel();\n        String clientId = ic.getClientId();\n        int bufferSize = ic.getBufferSize();\n        // ...\n        BrokerClient client = BrokerClient.create(clientId);\n        MyIncomingChannel channel = new MyIncomingChannel(vertx, ic, client);\n        incomingChannels.add(channel);\n        return channel.getStream();\n    }\n\n    @Override\n    public Flow.Subscriber&lt;? extends Message&lt;?&gt;&gt; getSubscriber(Config config) {\n        MyConnectorOutgoingConfiguration oc = new MyConnectorOutgoingConfiguration(config);\n        String channelName = oc.getChannel();\n        String clientId = oc.getClientId();\n        int pendingMessages = oc.getMaxPendingMessages();\n        // ...\n        BrokerClient client = BrokerClient.create(clientId);\n        MyOutgoingChannel channel = new MyOutgoingChannel(vertx, oc, client);\n        outgoingChannels.add(channel);\n        return channel.getSubscriber();\n    }\n}\n</code></pre> <p>Note that the <code>getPublisher</code> and <code>getSubscriber</code> methods receive MicroProfile Config <code>Config</code> instance and wrap it with <code>MyConnectorIncomingConfiguration</code> and <code>MyConnectorOutgoingConfiguration</code> objects.</p> <p>These custom channel configuration types ease getting channel configuration, including the optional or default values. They are generated by the <code>smallrye-connector-attribute-processor</code> annotation processor, and can be configured in project pom like the following:</p> <pre><code>    &lt;dependencies&gt;\n      &lt;dependency&gt;\n        &lt;groupId&gt;${project.groupId}&lt;/groupId&gt;\n        &lt;artifactId&gt;smallrye-reactive-messaging-provider&lt;/artifactId&gt;\n        &lt;version&gt;${project.version}&lt;/version&gt;\n      &lt;/dependency&gt;\n      &lt;dependency&gt;\n        &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n        &lt;artifactId&gt;smallrye-connector-attribute-processor&lt;/artifactId&gt;\n        &lt;version&gt;${project.version}&lt;/version&gt;\n      &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n    &lt;build&gt;\n      &lt;plugins&gt;\n        &lt;plugin&gt;\n          &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n          &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n          &lt;configuration&gt;\n            &lt;generatedSourcesDirectory&gt;${project.build.directory}/generated-sources/&lt;/generatedSourcesDirectory&gt;\n            &lt;annotationProcessors&gt;\n              &lt;annotationProcessor&gt;\n                io.smallrye.reactive.messaging.connector.ConnectorAttributeProcessor\n              &lt;/annotationProcessor&gt;\n              &lt;annotationProcessor&gt;\n                org.jboss.logging.processor.apt.LoggingToolsProcessor\n              &lt;/annotationProcessor&gt;\n            &lt;/annotationProcessors&gt;\n          &lt;/configuration&gt;\n        &lt;/plugin&gt;\n      &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre> <p>The <code>smallrye-reactive-messaging-provider</code> is the minimum required dependency for a connector implementation. You'll also note that the <code>LoggingToolsProcessor</code> annotation processor is also configured. This enables generating internationalized log statements and exceptions. Typically, you would create following interfaces in <code>i18n</code> sub-package: <code>[Connector]Exceptions</code>, <code>[Connector]Logging</code> and <code>[Connector]Messages</code>. More information can be found in JBoss Logging Tools documentation.</p>"},{"location":"concepts/contributing-connectors/#implementing-inbound-channels","title":"Implementing Inbound Channels","text":"<p>The <code>InboundConnector</code> implementation returns, for a given channel configuration, a reactive stream of <code>Message</code>s. The returned reactive stream is an instance of <code>Flow.Publisher</code> and typically can be implemented using Mutiny <code>Multi</code> type.</p> <p><code>IncomingConnectorFactory</code></p> <p>The inbound channels can also implement the <code>IncomingConnectorFactory</code> from the MicroProfile Reactive Messaging specification. However, the <code>PublisherBuilder</code> type can be more challenging to work with and Smallrye Reactive Messaging converts the provided stream to Mutiny types to do the wiring anyway.</p> <p>The returned <code>Flow.Publisher</code> would allow controlling the flow of ingestion using backpressure. It would be preferable to use pull-based APIs of the underlying messaging library to receive messages from the message broker. You can refer to the <code>Mutiny</code> \"How to use polling?\" guide to construct a <code>Multi</code> using Mutiny APIs, or implement the <code>Flow.Subscription</code> from scratch and wrap it in an <code>AbstractMulti</code>.</p> <p>Here is an example channel implementation which constructs the reactive stream using the polling API: <pre><code>package connectors;\n\nimport java.util.concurrent.Flow;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport connectors.api.BrokerClient;\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.health.HealthReport;\nimport io.vertx.core.impl.VertxInternal;\nimport io.vertx.mutiny.core.Context;\nimport io.vertx.mutiny.core.Vertx;\n\npublic class MyIncomingChannel {\n\n    private final String channel;\n    private final BrokerClient client;\n    private final Context context;\n    private final MyAckHandler ackHandler;\n    private final MyFailureHandler failureHandler;\n    private final AtomicBoolean closed = new AtomicBoolean(false);\n    private Flow.Publisher&lt;? extends Message&lt;?&gt;&gt; stream;\n\n    public MyIncomingChannel(Vertx vertx, MyConnectorIncomingConfiguration cfg, BrokerClient client) {\n        // create and configure the client with MyConnectorIncomingConfiguration\n        this.channel = cfg.getChannel();\n        this.client = client;\n        this.context = Context.newInstance(((VertxInternal) vertx.getDelegate()).createEventLoopContext());\n        this.ackHandler = MyAckHandler.create(this.client);\n        this.failureHandler = MyFailureHandler.create(this.client);\n        this.stream = Multi.createBy().repeating()\n                .uni(() -&gt; Uni.createFrom().completionStage(this.client.poll()))\n                .until(__ -&gt; closed.get())\n                .emitOn(context::runOnContext)\n                .map(consumed -&gt; new MyMessage&lt;&gt;(consumed, ackHandler, failureHandler));\n    }\n\n    public String getChannel() {\n        return channel;\n    }\n\n    public Flow.Publisher&lt;? extends Message&lt;?&gt;&gt; getStream() {\n        return this.stream;\n    }\n\n    public void close() {\n        closed.compareAndSet(false, true);\n        client.close();\n    }\n\n    void isReady(HealthReport.HealthReportBuilder healthReportBuilder) {\n\n    }\n\n}\n</code></pre></p>"},{"location":"concepts/contributing-connectors/#connector-threading-and-vertx","title":"Connector Threading and Vert.x","text":"<p>Whether the external API call is blocking or non-blocking, managing the thread on which the message processing will be dispatched can be challenging. Smallrye Reactive Messaging depends on Eclipse Vert.x to consistently dispatch messages in event-loop or worker threads, propagating the message context along different processing stages. You can read more on Message Context and Vert.x Context.</p> <p>Some connectors already use Vert.x clients, such as RabbitMQ, AMQP 1.0 or MQTT. Other connectors such as Kafka or Pulsar directly use the client library of the messaging technology, therefore they create a Vert.x Context per channel to dispatch messages on that context. Connectors can access the <code>Vertx</code> instance by injecting the <code>ExecutionHolder</code> bean. Mutiny operators <code>runSubscribtionOn</code> and <code>emitOn</code> can be used to switch threads the events are dispatched on.</p>"},{"location":"concepts/contributing-connectors/#custom-message-and-metadata-implementations","title":"Custom Message and Metadata implementations","text":"<p>Reactive messaging Message type is a thin wrapper around a payload and some metadata, which lets implementing acknowledgment and negative acknowledgment of that message.</p> <p>Messages received from the underlying library very often return with a custom type, wrapping the payload and some properties such as key, timestamp, schema information, or any other metadata.</p> <p>Tip</p> <p>The client may propose different strategies for consuming messages individually or in batches. If a batch consuming is available the incoming channel may receive wrap and dispatch message as a batch or individually.</p> <p>While it is possible to use <code>Message.of</code> builder methods to wrap the incoming message type, a custom type implementing <code>Message</code> interface helps to deal with different aspects we'll cover later, such as deserialization, message acknowledgment or tracing.</p> <p>An example message implementation would be like the following: <pre><code>package connectors;\n\nimport java.util.concurrent.CompletionStage;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\n\nimport connectors.api.ConsumedMessage;\nimport io.smallrye.reactive.messaging.providers.locals.ContextAwareMessage;\n\npublic class MyMessage&lt;T&gt; implements Message&lt;T&gt;, ContextAwareMessage&lt;T&gt; {\n\n    private final T payload;\n    private final Metadata metadata;\n    private final MyAckHandler ackHandler;\n\n    private final MyFailureHandler nackHandler;\n    private ConsumedMessage&lt;?&gt; consumed;\n\n    public MyMessage(ConsumedMessage&lt;T&gt; message, MyAckHandler ackHandler, MyFailureHandler nackHandler) {\n        this.consumed = message;\n        this.payload = message.body();\n        this.ackHandler = ackHandler;\n        this.nackHandler = nackHandler;\n        this.metadata = ContextAwareMessage.captureContextMetadata(new MyIncomingMetadata&lt;&gt;(message));\n    }\n\n    public ConsumedMessage&lt;?&gt; getConsumedMessage() {\n        return consumed;\n    }\n\n    @Override\n    public T getPayload() {\n        return payload;\n    }\n\n    @Override\n    public Metadata getMetadata() {\n        return metadata;\n    }\n\n    @Override\n    public CompletionStage&lt;Void&gt; ack() {\n        return ackHandler.handle(this);\n    }\n\n    @Override\n    public CompletionStage&lt;Void&gt; nack(Throwable reason, Metadata nackMetadata) {\n        return nackHandler.handle(this, reason, nackMetadata);\n    }\n}\n</code></pre></p> <p>Note that <code>MyMessage</code> implements the <code>ContextAwareMessage</code>. In the constructor <code>captureContextMetadata</code> helper method is used to capture the Vert.x context which created the object and capturing it into the <code>LocalContextMetadata</code>. This metadata allows running each message in its own Vert.x context, supporting context propagation.</p> <p>The <code>MyMessage</code> type implements the accessors for the <code>metadata</code> and <code>payload</code> from the <code>Message</code> interface. If the messaging technology doesn't have a built-in unmarshalling mechanism, the message can deserialize the raw payload to a primitive or a complex object.</p> <p>Warning</p> <p>The custom message implementation is usually not the type consumed by the application injecting channels. Applications usually inject in the payload, the raw consumed type (in the above example the <code>ConsumedMessage</code>), or some other type provided by the <code>MessageConverter</code>s. Handling of <code>Message</code> types by the application is restricted only to advanced use cases, because handling of message acknowledgment is manual Even then the message may be intercepted before and changed, conserving the <code>metadata</code>, <code>ack</code> and <code>nack</code> handlers but not the original type created by the connector.</p> <p>The <code>MyIncomingMetadata</code> gives access to the underlying consumed message attributes, and applications can inject this object for accessing message details: <pre><code>package connectors;\n\nimport java.util.Map;\n\nimport connectors.api.ConsumedMessage;\n\npublic class MyIncomingMetadata&lt;T&gt; {\n\n    private final ConsumedMessage&lt;T&gt; msg;\n\n    public MyIncomingMetadata(ConsumedMessage&lt;T&gt; msg) {\n        this.msg = msg;\n    }\n\n    public ConsumedMessage&lt;T&gt; getCustomMessage() {\n        return msg;\n    }\n\n    public T getBody() {\n        return msg.body();\n    }\n\n    public String getKey() {\n        return msg.key();\n    }\n\n    public long getTimestamp() {\n        return msg.timestamp();\n    }\n\n    public Map&lt;String, String&gt; getProperties() {\n        return msg.properties();\n    }\n}\n</code></pre></p> <p>Also note that <code>ack</code> and <code>nack</code> method implementations are delegated to handler objects. This allows configuring different strategies at channel level.</p>"},{"location":"concepts/contributing-connectors/#acknowledgment-strategies","title":"Acknowledgment strategies","text":"<p>The acknowledgement is the way for message consumers to inform the broker that the message has been successfully received and processed. Depending on the messaging technology the broker then can decide to remove the message from the queue, flag as consumed or purge it completely. In Reactive Messaging there are different policies to trigger the acknowledgement but the canonical one is to acknowledge a message when the processing (potentially asynchronous) has completed (<code>POST_PROCESSING</code>).</p> <p>The Reactive Messaging defines <code>Message#ack</code> method as non-blocking asynchronous, returning a <code>CompletionStage&lt;Void&gt;</code>, because potentially the acknowledgement action sends a network call to the broker.</p> <p>The following example simply calls the client <code>ack</code> method using the Mutiny <code>Uni</code> and switch the emitter to the Message context. Returning back to the message context is essential for chaining asynchronous actions without losing the context and for keeping the consistency on message consumption flow.</p> <pre><code>package connectors;\n\nimport java.util.concurrent.CompletionStage;\n\nimport connectors.api.BrokerClient;\nimport io.smallrye.mutiny.Uni;\n\npublic class MyAckHandler {\n\n    private final BrokerClient client;\n\n    static MyAckHandler create(BrokerClient client) {\n        return new MyAckHandler(client);\n    }\n\n    public MyAckHandler(BrokerClient client) {\n        this.client = client;\n    }\n\n    public CompletionStage&lt;Void&gt; handle(MyMessage&lt;?&gt; msg) {\n        return Uni.createFrom().completionStage(client.ack(msg.getConsumedMessage()))\n                .emitOn(msg::runOnMessageContext)\n                .subscribeAsCompletionStage();\n    }\n}\n</code></pre> <p>While this ack handler strategy acknowledges each message to the broker, the messaging technology can allow employing different strategies for acknowledging messages. For example an ack strategy can track processed messages and acknowledge them altogether or call a different client side endpoint to acknowledge the message batch.</p>"},{"location":"concepts/contributing-connectors/#failure-handling-strategies","title":"Failure handling strategies","text":"<p>The failure handling, or the negative acknowledgment allows indicating that a message was not processed correctly. Similar to the acknowledgment the Reactive Messaging defines <code>Message#nack(Throwable reason, Metadata metadata)</code> method as non-blocking asynchronous, returning a <code>CompletionStage&lt;Void&gt;</code>.</p> <pre><code>package connectors;\n\nimport java.util.concurrent.CompletionStage;\n\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\n\nimport connectors.api.BrokerClient;\nimport io.smallrye.mutiny.Uni;\n\npublic class MyFailureHandler {\n\n    private final BrokerClient client;\n\n    static MyFailureHandler create(BrokerClient client) {\n        return new MyFailureHandler(client);\n    }\n\n    public MyFailureHandler(BrokerClient client) {\n        this.client = client;\n    }\n\n    public CompletionStage&lt;Void&gt; handle(MyMessage&lt;?&gt; msg, Throwable reason, Metadata metadata) {\n        return Uni.createFrom().completionStage(() -&gt; client.reject(msg.getConsumedMessage(), reason.getMessage()))\n                .emitOn(msg::runOnMessageContext)\n                .subscribeAsCompletionStage();\n    }\n}\n</code></pre> <p>Different failure handling strategies can, for example, - Ignore the failure, log and call the <code>ack</code> instead - Send the message to a dead letter queue and call the <code>ack</code> - Employ a different strategy depending on the <code>Metadata</code> associated with the <code>nack</code> method call.</p>"},{"location":"concepts/contributing-connectors/#message-converters","title":"Message Converters","text":"<p>The connector can propose default <code>MessageConverter</code> implementations for converting the payload to a custom type. As an example the following converter extracts the <code>CustomMessage</code> and puts it in the payload: <pre><code>package connectors;\n\nimport java.lang.reflect.Type;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport connectors.api.ConsumedMessage;\nimport io.smallrye.reactive.messaging.MessageConverter;\nimport io.smallrye.reactive.messaging.providers.helpers.TypeUtils;\n\n@ApplicationScoped\npublic class MyMessageConverter implements MessageConverter {\n    @Override\n    public boolean canConvert(Message&lt;?&gt; in, Type target) {\n        return TypeUtils.isAssignable(target, ConsumedMessage.class)\n                &amp;&amp; in.getMetadata(MyIncomingMetadata.class).isPresent();\n    }\n\n    @Override\n    public Message&lt;?&gt; convert(Message&lt;?&gt; in, Type target) {\n        return in.withPayload(in.getMetadata(MyIncomingMetadata.class)\n                .map(MyIncomingMetadata::getCustomMessage)\n                .orElse(null));\n    }\n}\n</code></pre></p>"},{"location":"concepts/contributing-connectors/#implementing-outbound-channels","title":"Implementing Outbound Channels","text":"<p>The <code>OutboundConnector</code> implementation returns, for a given channel configuration, a <code>Flow.Subscriber</code> of messages. This is typically implemented by a custom <code>Flow.Processor</code> and using the <code>MultiUtils.via</code> helper methods to apply message transformations.</p> <p><code>OutgoingConnectorFactory</code></p> <p>The outbound channels can also implement the <code>OutgoingConnectorFactory</code> from the MicroProfile Reactive Messaging specification. However, it is usually more friendly to work with the <code>MultiUtils.via</code> methods to construct and transform outgoing messages.</p> <p>Here is an example outgoing channel implementation: <pre><code>package connectors;\n\nimport java.util.concurrent.Flow;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport connectors.api.BrokerClient;\nimport connectors.api.SendMessage;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.providers.helpers.MultiUtils;\nimport io.vertx.mutiny.core.Vertx;\n\npublic class MyOutgoingChannel {\n    private final String channel;\n    private Flow.Subscriber&lt;? extends Message&lt;?&gt;&gt; subscriber;\n    private final BrokerClient client;\n    private final String topic;\n\n    public MyOutgoingChannel(Vertx vertx, MyConnectorOutgoingConfiguration oc, BrokerClient client) {\n        this.channel = oc.getChannel();\n        this.client = client;\n        this.topic = oc.getTopic().orElse(oc.getChannel());\n        this.subscriber = MultiUtils.via(multi -&gt; multi.call(m -&gt; publishMessage(this.client, m)));\n    }\n\n    private Uni&lt;Void&gt; publishMessage(BrokerClient client, Message&lt;?&gt; m) {\n        // construct the outgoing message\n        SendMessage sendMessage = new SendMessage();\n        Object payload = m.getPayload();\n        sendMessage.setPayload(payload);\n        sendMessage.setTopic(topic);\n        m.getMetadata(MyOutgoingMetadata.class).ifPresent(out -&gt; {\n            sendMessage.setTopic(out.getTopic());\n            sendMessage.setKey(out.getKey());\n            //...\n        });\n        return Uni.createFrom().completionStage(() -&gt; client.send(sendMessage))\n                .onItem().transformToUni(receipt -&gt; Uni.createFrom().completionStage(m.ack()))\n                .onFailure().recoverWithUni(t -&gt; Uni.createFrom().completionStage(m.nack(t)));\n    }\n\n    public Flow.Subscriber&lt;? extends Message&lt;?&gt;&gt; getSubscriber() {\n        return this.subscriber;\n    }\n\n    public String getChannel() {\n        return this.channel;\n    }\n}\n</code></pre> The <code>MultiUtils.via</code> helper method allows using the <code>Multi</code> chaining methods and in the same time provides a <code>Flow.Subscriber</code>. However, this implementation allows sending messages one at a time: one only after the previous outgoing message send is completed.</p> <p>Some messaging technologies provide publish receipt, a message back from the broker to the sender that asynchronously acknowledges the sent operation. In this case the connector can only be sure of the send operation when it receives the publish receipt of that message. Some technologies may provide blocking sending calls, in that case the connector needs to delegate the sending call to a worker thread.</p> <p>Depending on whether the client supports multiple in-flight outgoing messages, you can also use a <code>SenderProcessor</code>, which allows receiving configuration for the maximum number of in-flight messages and whether it waits for completion (publish receipt from the broker):</p> <pre><code>long requests = oc.getMaxPendingMessages();\nboolean waitForWriteCompletion = oc.getWaitForWriteCompletion();\nif (requests &lt;= 0) {\n    requests = Long.MAX_VALUE;\n}\nthis.processor = new SenderProcessor(requests, waitForWriteCompletion, m -&gt; publishMessage(client, m));\nthis.subscriber = MultiUtils.via(processor, m -&gt; m.onFailure().invoke(f -&gt; {\n    // log the failure\n}));\n</code></pre> <p>Other more advanced scenarios can be implemented to retry the transmission in case of a retryable failure, or batch multiple outgoing messages to a single send operation.</p>"},{"location":"concepts/contributing-connectors/#outgoing-message-builder","title":"Outgoing <code>Message</code> Builder","text":"<p>In order to convey all attributes of the outgoing message to the client library connectors provide outgoing <code>Message</code> implementation and a corresponding outgoing message metadata. These allow the application developer to build the message attributes that will be sent to the broker.</p> <pre><code>package connectors;\n\nimport java.util.concurrent.CompletionStage;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\n\nimport io.smallrye.reactive.messaging.providers.locals.ContextAwareMessage;\n\npublic class MyOutgoingMessage&lt;T&gt; implements Message&lt;T&gt;, ContextAwareMessage&lt;T&gt; {\n\n    private final T payload;\n    private final Metadata metadata;\n\n    private final Supplier&lt;CompletionStage&lt;Void&gt;&gt; ack;\n    private final Function&lt;Throwable, CompletionStage&lt;Void&gt;&gt; nack;\n\n    public static &lt;T&gt; MyOutgoingMessage&lt;T&gt; from(Message&lt;T&gt; message) {\n        return new MyOutgoingMessage&lt;&gt;(message.getPayload(), message.getMetadata(), message.getAck(), message.getNack());\n    }\n\n    public MyOutgoingMessage(T payload, Metadata metadata,\n            Supplier&lt;CompletionStage&lt;Void&gt;&gt; ack,\n            Function&lt;Throwable, CompletionStage&lt;Void&gt;&gt; nack) {\n        this.payload = payload;\n        this.metadata = metadata;\n        this.ack = ack;\n        this.nack = nack;\n    }\n\n    public MyOutgoingMessage(T payload, String key, String topic,\n            Supplier&lt;CompletionStage&lt;Void&gt;&gt; ack,\n            Function&lt;Throwable, CompletionStage&lt;Void&gt;&gt; nack) {\n        this(payload, Metadata.of(new MyOutgoingMetadata(topic, key)), ack, nack);\n    }\n\n    @Override\n    public T getPayload() {\n        return payload;\n    }\n\n    @Override\n    public Metadata getMetadata() {\n        return metadata;\n    }\n\n    @Override\n    public Supplier&lt;CompletionStage&lt;Void&gt;&gt; getAck() {\n        return this.ack;\n    }\n\n    @Override\n    public Function&lt;Throwable, CompletionStage&lt;Void&gt;&gt; getNack() {\n        return this.nack;\n    }\n\n    public MyOutgoingMessage&lt;T&gt; withKey(String key) {\n        this.metadata.with(this.metadata.get(MyOutgoingMetadata.class)\n                .map(m -&gt; MyOutgoingMetadata.builder(m).withKey(key).build()));\n        return this;\n    }\n\n    public MyOutgoingMessage&lt;T&gt; withTopic(String topic) {\n        this.metadata.with(this.metadata.get(MyOutgoingMetadata.class)\n                .map(m -&gt; MyOutgoingMetadata.builder(m).withTopic(topic).build()));\n        return this;\n    }\n}\n</code></pre> <pre><code>package connectors;\n\npublic class MyOutgoingMetadata {\n    private String topic;\n    private String key;\n\n    public static MyOutgoingMetadataBuilder builder() {\n        return new MyOutgoingMetadataBuilder();\n    }\n\n    public static MyOutgoingMetadataBuilder builder(MyOutgoingMetadata metadata) {\n        return new MyOutgoingMetadataBuilder(metadata);\n    }\n\n    public MyOutgoingMetadata(String topic, String key) {\n        this.topic = topic;\n        this.key = key;\n    }\n\n    public String getTopic() {\n        return topic;\n    }\n\n    public String getKey() {\n        return key;\n    }\n\n    public static class MyOutgoingMetadataBuilder {\n        private String topic;\n        private String key;\n\n        public MyOutgoingMetadataBuilder() {\n\n        }\n\n        public MyOutgoingMetadataBuilder(MyOutgoingMetadata metadata) {\n            this.key = metadata.getKey();\n            this.topic = metadata.getTopic();\n        }\n\n        public MyOutgoingMetadataBuilder withTopic(String topic) {\n            this.topic = topic;\n            return this;\n        }\n\n        public MyOutgoingMetadataBuilder withKey(String key) {\n            this.key = key;\n            return this;\n        }\n\n        public MyOutgoingMetadata build() {\n            return new MyOutgoingMetadata(topic, key);\n        }\n    }\n\n}\n</code></pre> <p>The outgoing channel implementation then will construct the client library object that represents the outbound message, <code>SendMessage</code> in this example:</p> <pre><code>private Uni&lt;Void&gt; publishMessage(BrokerClient client, Message&lt;?&gt; message) {\n    // construct the outgoing message\n    SendMessage sendMessage;\n    Object payload = message.getPayload();\n    if (payload instanceof SendMessage) {\n        sendMessage = (SendMessage) message.getPayload();\n    } else {\n        sendMessage = new SendMessage();\n        sendMessage.setPayload(payload);\n        sendMessage.setTopic(topic);\n        message.getMetadata(MyOutgoingMetadata.class).ifPresent(out -&gt; {\n            sendMessage.setTopic(out.getTopic());\n            sendMessage.setKey(out.getKey());\n            //...\n        });\n    }\n    return Uni.createFrom().completionStage(() -&gt; client.send(sendMessage))\n            .onItem().transformToUni(receipt -&gt; Uni.createFrom().completionStage(message.ack()))\n            .onFailure().recoverWithUni(t -&gt; Uni.createFrom().completionStage(message.nack(t)));\n}\n</code></pre> <p>It is a best practice to also allow the application to return a payload of the client outbound library object (<code>SendMessage</code>).</p>"},{"location":"concepts/contributing-connectors/#outgoing-message-acknowledgement","title":"Outgoing message acknowledgement","text":"<p>Because the Reactive Messaging chains acknowledgements from incoming message until the outgoing message, it is crucial for the outgoing channel to correctly ack and nack the message.</p>"},{"location":"concepts/contributing-connectors/#smallrye-health-integration","title":"Smallrye Health Integration","text":"<p>Smallrye Reactive Messaging allows connectors to integrate with Smallrye Health to contribute channel state to the health reports. Connectors need to implement the <code>HealthReporter</code> interface and implement some or all of the <code>getReadiness</code>, <code>getLiveness</code> and <code>getStartup</code> methods:</p> <pre><code>@ApplicationScoped\n@Connector(MyConnectorWithPartials.CONNECTOR_NAME)\npublic class MyConnectorWithPartials implements InboundConnector, OutboundConnector, HealthReporter {\n\n    public static final String CONNECTOR_NAME = \"smallrye-my-connector\";\n\n    List&lt;MyIncomingChannel&gt; incomingChannels = new CopyOnWriteArrayList&lt;&gt;();\n    List&lt;MyOutgoingChannel&gt; outgoingChannels = new CopyOnWriteArrayList&lt;&gt;();\n\n    @Override\n    public HealthReport getReadiness() {\n        HealthReport.HealthReportBuilder builder = HealthReport.builder();\n        for (MyIncomingChannel channel : incomingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        for (MyOutgoingChannel channel : outgoingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        return builder.build();\n    }\n\n    @Override\n    public HealthReport getLiveness() {\n        HealthReport.HealthReportBuilder builder = HealthReport.builder();\n        for (MyIncomingChannel channel : incomingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        for (MyOutgoingChannel channel : outgoingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        return builder.build();\n    }\n\n    @Override\n    public HealthReport getStartup() {\n        HealthReport.HealthReportBuilder builder = HealthReport.builder();\n        for (MyIncomingChannel channel : incomingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        for (MyOutgoingChannel channel : outgoingChannels) {\n            builder.add(channel.getChannel(), true);\n        }\n        return builder.build();\n    }\n</code></pre> <p>Implementing health reports per channel depends on what information is available to the connector. For more information on different health check probes you can check out Configure Liveness, Readiness and Startup Probes </p> <p>You may want to add a connector attribute to enable/disable the health reporting per channel: <pre><code>import io.smallrye.reactive.messaging.annotations.ConnectorAttribute;\n\n@ConnectorAttribute(name = \"health-enabled\", type = \"boolean\", direction = ConnectorAttribute.Direction.INCOMING_AND_OUTGOING, description = \"Whether health reporting is enabled (default) or disabled\", defaultValue = \"true\")\n</code></pre></p>"},{"location":"concepts/contributing-connectors/#opentelemetry-tracing-integration","title":"OpenTelemetry Tracing Integration","text":"<p>Smallrye Reactive Messaging allows connectors to easily integrate with OpenTelemetry tracing. It propagates the tracing context from inbound messages and to outbound messages. The <code>smallrye-reactive-messaging-otel</code> module provides necessary dependencies to the OpenTelemetry artifacts and also provides <code>TracingUtils</code> helper class for setting up the tracing.</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.smallrye.reactive.messaging&lt;/groupId&gt;\n    &lt;artifactId&gt;smallrye-reactive-messaging-otel&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>For integrating tracing you'd need to create a couple of classes:</p> <ul> <li>Holder class for tracing information</li> <li>Implementation of <code>io.opentelemetry.context.propagation.TextMapGetter</code>, which retrieves for a given key the value of a tracing attributed from the holder object</li> <li>Implementation of <code>io.opentelemetry.context.propagation.TextMapSetter</code>, which sets on the holder object the key and value of tracing attributes</li> <li>Implementations of <code>io.opentelemetry.instrumentation.api.instrumenter.AttributesExtractor</code> and <code>io.opentelemetry.instrumentation.api.instrumenter.messaging.MessagingAttributesGetter</code></li> </ul> <p>Then you'd need to configure instrumenters per incoming and outgoing channel:</p> <pre><code>    public static Instrumenter&lt;MyTrace, Void&gt; createInstrumenter(boolean incoming) {\n        MessageOperation messageOperation = incoming ? MessageOperation.RECEIVE : MessageOperation.PUBLISH;\n\n        MyAttributesExtractor myExtractor = new MyAttributesExtractor();\n        MessagingAttributesGetter&lt;MyTrace, Void&gt; attributesGetter = myExtractor.getMessagingAttributesGetter();\n        var spanNameExtractor = MessagingSpanNameExtractor.create(attributesGetter, messageOperation);\n        InstrumenterBuilder&lt;MyTrace, Void&gt; builder = Instrumenter.builder(GlobalOpenTelemetry.get(),\n                \"io.smallrye.reactive.messaging\", spanNameExtractor);\n        var attributesExtractor = MessagingAttributesExtractor.create(attributesGetter, messageOperation);\n\n        builder\n                .addAttributesExtractor(attributesExtractor)\n                .addAttributesExtractor(myExtractor);\n\n        if (incoming) {\n            return builder.buildConsumerInstrumenter(MyTraceTextMapGetter.INSTANCE);\n        } else {\n            return builder.buildProducerInstrumenter(MyTraceTextMapSetter.INSTANCE);\n        }\n    }\n    // &lt;/create-instrumener&gt;\n\n    public Message&lt;?&gt; traceIncoming(Message&lt;?&gt; message, MyTrace myTrace, boolean makeCurrent) {\n        return TracingUtils.traceIncoming(instrumenter, message, myTrace, makeCurrent);\n    }\n\n    public void traceOutgoing(Message&lt;?&gt; message, MyTrace myTrace) {\n        TracingUtils.traceOutgoing(instrumenter, message, myTrace);\n    }\n}\n</code></pre> <p>Finally, you'd need to configure instrumenters per incoming and outgoing channels and wire the call to instrumenter using <code>TracingUtils</code>.</p> <p>For an incoming channel, you'd need to call the instrumenter on an inbound message:</p> <pre><code>Multi&lt;? extends Message&lt;?&gt;&gt; receiveMulti = Multi.createBy().repeating()\n        .uni(() -&gt; Uni.createFrom().completionStage(this.client.poll()))\n        .until(__ -&gt; closed.get())\n        .emitOn(context::runOnContext)\n        .map(consumed -&gt; new MyMessage&lt;&gt;(consumed, ackHandler, failureHandler));\n\nInstrumenter&lt;MyTrace, Void&gt; instrumenter = MyOpenTelemetryInstrumenter.createInstrumenter(true);\nif (tracingEnabled) {\n    receiveMulti = receiveMulti.map(message -&gt; {\n        ConsumedMessage&lt;?&gt; consumedMessage = message.getMetadata(MyIncomingMetadata.class).get().getCustomMessage();\n        return TracingUtils.traceIncoming(instrumenter, message, new MyTrace.Builder()\n                .withClientId(consumedMessage.clientId())\n                .withTopic(consumedMessage.topic())\n                .withProperties(consumedMessage.properties())\n                .build());\n    });\n}\n</code></pre> <p>For an outgoing channel, you'd need to call the instrumenter on constructing the outbound message:</p> <pre><code>private Uni&lt;Void&gt; publishMessageWithTracing(BrokerClient client, Message&lt;?&gt; message) {\n    // construct the outgoing message\n    SendMessage sendMessage;\n    Object payload = message.getPayload();\n    if (payload instanceof SendMessage) {\n        sendMessage = (SendMessage) message.getPayload();\n    } else {\n        sendMessage = new SendMessage();\n        sendMessage.setPayload(payload);\n        sendMessage.setTopic(topic);\n        message.getMetadata(MyOutgoingMetadata.class).ifPresent(out -&gt; {\n            sendMessage.setTopic(out.getTopic());\n            sendMessage.setKey(out.getKey());\n            //...\n        });\n    }\n    if (tracingEnabled) {\n        Map&lt;String, String&gt; properties = new HashMap&lt;&gt;();\n        TracingUtils.traceOutgoing(instrumenter, message, new MyTrace.Builder()\n                .withProperties(properties)\n                .withTopic(sendMessage.getTopic())\n                .build());\n        sendMessage.setProperties(properties);\n    }\n    return Uni.createFrom().completionStage(() -&gt; client.send(sendMessage))\n            .onItem().transformToUni(receipt -&gt; Uni.createFrom().completionStage(message.ack()))\n            .onFailure().recoverWithUni(t -&gt; Uni.createFrom().completionStage(message.nack(t)));\n}\n</code></pre> <p>You may want to add a connector attribute to enable/disable the tracing per channel: <pre><code>import io.smallrye.reactive.messaging.annotations.ConnectorAttribute;\n\n@ConnectorAttribute(name = \"tracing-enabled\", type = \"boolean\", direction = ConnectorAttribute.Direction.INCOMING_AND_OUTGOING, description = \"Whether tracing is enabled (default) or disabled\", defaultValue = \"true\")\n</code></pre></p>"},{"location":"concepts/contributing-connectors/#testing-the-connector","title":"Testing the connector","text":"<p>While unit tests are highly encouraged for validating ad-hoc logic in connector code, by nature connector tests are mostly integration tests validating the correct configuration and functioning of channels. Most of the time tests need to run against a broker instance. This instance can be mocked or embedded in the test JVM, or provisioned in a container runtime using Testcontainers. The testcontainers approach is encouraged as it'll provide a testing environment closest to reality.</p> <p>It may take too much time and resources to start a broker per test method or per test class, so may want to share the same broker instance between all test classes. In that case you can checkout how to write a JUnit 5 Extension and start only one container instance in the beginning of tests and stop it at the end of all the tests.</p> <p>There are essentially two ways of creating the connector behavior to test against:</p> <ol> <li>Instantiating channels directly by passing the custom configuration. With this you can get the Reactive stream directly from the channel implementation and send/receive messages. You can use AssertSubscriber from Mutiny to regulate demand and write assertions.</li> <li>CDI-based tests which write configuration and instantiate application beans. You can use Weld, the reference implementation of CDI specification with configured set of beans and extensions: <pre><code>package connectors.test;\n\nimport jakarta.enterprise.inject.spi.BeanManager;\n\nimport org.eclipse.microprofile.config.ConfigProvider;\nimport org.eclipse.microprofile.reactive.messaging.spi.ConnectorLiteral;\nimport org.jboss.weld.environment.se.Weld;\nimport org.jboss.weld.environment.se.WeldContainer;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\n\nimport connectors.MyConnector;\nimport connectors.MyMessageConverter;\nimport io.smallrye.config.SmallRyeConfigProviderResolver;\nimport io.smallrye.config.inject.ConfigExtension;\nimport io.smallrye.reactive.messaging.providers.MediatorFactory;\nimport io.smallrye.reactive.messaging.providers.connectors.ExecutionHolder;\nimport io.smallrye.reactive.messaging.providers.connectors.WorkerPoolRegistry;\nimport io.smallrye.reactive.messaging.providers.extension.ChannelProducer;\nimport io.smallrye.reactive.messaging.providers.extension.EmitterFactoryImpl;\nimport io.smallrye.reactive.messaging.providers.extension.HealthCenter;\nimport io.smallrye.reactive.messaging.providers.extension.LegacyEmitterFactoryImpl;\nimport io.smallrye.reactive.messaging.providers.extension.MediatorManager;\nimport io.smallrye.reactive.messaging.providers.extension.MutinyEmitterFactoryImpl;\nimport io.smallrye.reactive.messaging.providers.extension.ReactiveMessagingExtension;\nimport io.smallrye.reactive.messaging.providers.impl.ConfiguredChannelFactory;\nimport io.smallrye.reactive.messaging.providers.impl.ConnectorFactories;\nimport io.smallrye.reactive.messaging.providers.impl.InternalChannelRegistry;\nimport io.smallrye.reactive.messaging.providers.metrics.MetricDecorator;\nimport io.smallrye.reactive.messaging.providers.metrics.MicrometerDecorator;\nimport io.smallrye.reactive.messaging.providers.wiring.Wiring;\nimport io.smallrye.reactive.messaging.test.common.config.MapBasedConfig;\n\npublic class WeldTestBase {\n\n    protected Weld weld;\n    protected WeldContainer container;\n\n    @BeforeEach\n    public void initWeld() {\n        weld = new Weld();\n\n        // SmallRye config\n        ConfigExtension extension = new ConfigExtension();\n        weld.addExtension(extension);\n\n        weld.addBeanClass(MediatorFactory.class);\n        weld.addBeanClass(MediatorManager.class);\n        weld.addBeanClass(InternalChannelRegistry.class);\n        weld.addBeanClass(ConnectorFactories.class);\n        weld.addBeanClass(ConfiguredChannelFactory.class);\n        weld.addBeanClass(ChannelProducer.class);\n        weld.addBeanClass(ExecutionHolder.class);\n        weld.addBeanClass(WorkerPoolRegistry.class);\n        weld.addBeanClass(HealthCenter.class);\n        weld.addBeanClass(Wiring.class);\n        weld.addExtension(new ReactiveMessagingExtension());\n\n        weld.addBeanClass(EmitterFactoryImpl.class);\n        weld.addBeanClass(MutinyEmitterFactoryImpl.class);\n        weld.addBeanClass(LegacyEmitterFactoryImpl.class);\n\n        weld.addBeanClass(MyConnector.class);\n        weld.addBeanClass(MyMessageConverter.class);\n        weld.addBeanClass(MetricDecorator.class);\n        weld.addBeanClass(MicrometerDecorator.class);\n        weld.disableDiscovery();\n    }\n\n    @AfterEach\n    public void stopContainer() {\n        if (container != null) {\n            // TODO Explicitly close the connector\n            getBeanManager().createInstance()\n                    .select(MyConnector.class, ConnectorLiteral.of(MyConnector.CONNECTOR_NAME)).get();\n            container.close();\n        }\n        // Release the config objects\n        SmallRyeConfigProviderResolver.instance().releaseConfig(ConfigProvider.getConfig());\n    }\n\n    public BeanManager getBeanManager() {\n        if (container == null) {\n            runApplication(new MapBasedConfig());\n        }\n        return container.getBeanManager();\n    }\n\n    public void addBeans(Class&lt;?&gt;... clazzes) {\n        weld.addBeanClasses(clazzes);\n    }\n\n    public &lt;T&gt; T get(Class&lt;T&gt; clazz) {\n        return getBeanManager().createInstance().select(clazz).get();\n    }\n\n    public &lt;T&gt; T runApplication(MapBasedConfig config, Class&lt;T&gt; clazz) {\n        weld.addBeanClass(clazz);\n        runApplication(config);\n        return get(clazz);\n    }\n\n    public void runApplication(MapBasedConfig config) {\n        if (config != null) {\n            config.write();\n        } else {\n            MapBasedConfig.cleanup();\n        }\n\n        container = weld.initialize();\n    }\n\n    public static void addConfig(MapBasedConfig config) {\n        if (config != null) {\n            config.write();\n        } else {\n            MapBasedConfig.cleanup();\n        }\n    }\n\n    public HealthCenter getHealth() {\n        if (container == null) {\n            throw new IllegalStateException(\"Application not started\");\n        }\n        return container.getBeanManager().createInstance().select(HealthCenter.class).get();\n    }\n\n    public boolean isStarted() {\n        return getHealth().getStartup().isOk();\n    }\n\n    public boolean isReady() {\n        return getHealth().getReadiness().isOk();\n    }\n\n    public boolean isAlive() {\n        return getHealth().getLiveness().isOk();\n    }\n\n}\n</code></pre></li> </ol> <p>You would need following test dependencies for enabling Weld in tests: <pre><code>    &lt;dependency&gt;\n      &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n      &lt;artifactId&gt;test-common&lt;/artifactId&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.jboss.weld.se&lt;/groupId&gt;\n      &lt;artifactId&gt;weld-se-shaded&lt;/artifactId&gt;\n      &lt;version&gt;${weld.version}&lt;/version&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.jboss.weld&lt;/groupId&gt;\n      &lt;artifactId&gt;weld-core-impl&lt;/artifactId&gt;\n      &lt;version&gt;${weld.version}&lt;/version&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.awaitility&lt;/groupId&gt;\n      &lt;artifactId&gt;awaitility&lt;/artifactId&gt;\n      &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n</code></pre></p> <p>Your test classes can therefore extend the <code>WeldTestBase</code> and provide configuration and application beans:</p> <pre><code>package connectors.test;\n\nimport static org.awaitility.Awaitility.await;\n\nimport java.util.List;\nimport java.util.UUID;\nimport java.util.concurrent.CopyOnWriteArrayList;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.junit.jupiter.api.Test;\n\nimport connectors.MyConnector;\nimport io.smallrye.reactive.messaging.test.common.config.MapBasedConfig;\n\npublic class MyConnectorTest extends WeldTestBase {\n\n    @Test\n    void incomingChannel() {\n        String host = \"\";\n        int port = 0;\n        String myTopic = UUID.randomUUID().toString();\n        MapBasedConfig config = new MapBasedConfig()\n                .with(\"mp.messaging.incoming.data.topic\", myTopic)\n                .with(\"mp.messaging.incoming.data.host\", host)\n                .with(\"mp.messaging.incoming.data.port\", port)\n                .with(\"mp.messaging.incoming.data.connector\", MyConnector.CONNECTOR_NAME);\n        MyApp app = runApplication(config, MyApp.class);\n\n        int expected = 10;\n        // produce expected number of messages to myTopic\n\n        // wait until app received\n        await().until(() -&gt; app.received().size() == expected);\n    }\n\n    @ApplicationScoped\n    public static class MyApp {\n\n        List&lt;String&gt; received = new CopyOnWriteArrayList&lt;&gt;();\n\n        @Incoming(\"data\")\n        void consume(String msg) {\n            received.add(msg);\n        }\n\n        public List&lt;String&gt; received() {\n            return received;\n        }\n    }\n}\n</code></pre> <p>Awaitility</p> <p>Because connector tests are usually asynchronous, awaitility provides a DSL to await on expressed assertions.</p>"},{"location":"concepts/contributing-connectors/#common-tests-for-validating-the-connector","title":"Common tests for validating the connector","text":"<ul> <li>Message consumption through incoming channels</li> <li>Message producing through outgoing channels</li> <li>Ack and failure handler strategies test</li> <li>Message Context propagation test <code>LocalPropagationTest</code></li> <li><code>HealthCheckTest</code></li> <li><code>MessageConverterTest</code></li> <li><code>TracingPropagationTest</code></li> <li>Configuration test</li> <li>Authentication test</li> <li>Tests for</li> </ul>"},{"location":"concepts/converters/","title":"Message Converters","text":"<p>SmallRye Reactive Messaging supports message converters, allowing to transform an incoming message into a version accepted by the method. If the incoming messages or payload does not match the invoked method\u2019s expectation, SmallRye Reactive Messaging looks for a suitable converter. If found, it converts the incoming message with this converter.</p> <p>Converters can have multiple purposes, but the main use case is about transforming the message\u2019s payload:</p> <pre><code>@ApplicationScoped\npublic class MyConverter implements MessageConverter {\n    @Override\n    public boolean canConvert(Message&lt;?&gt; in, Type target) {\n        // Checks whether this converter can be used to convert\n        // the incoming message into a message containing a payload\n        // of the type `target`.\n        return in.getPayload().getClass().equals(String.class)\n                &amp;&amp; target.equals(Person.class);\n    }\n\n    @Override\n    public Message&lt;?&gt; convert(Message&lt;?&gt; in, Type target) {\n        // Convert the incoming message into the new message.\n        // It's important to build the new message **from**\n        // the received one.\n        return in.withPayload(new Person((String) in.getPayload()));\n    }\n}\n</code></pre> <p>To provide a converter, implement a bean exposing the <code>MessageConverter</code> interface. The <code>canConvert</code> method is called during the lookup and verifies if it can handle the conversion. The <code>target</code> type is the expected payload type. If the converter returns <code>true</code> to <code>canConvert</code>, SmallRye Reactive Messaging calls the <code>convert</code> method to proceed to the conversion.</p> <p>The previous converter can be used in application like the following, to convert <code>Message&lt;String&gt;</code> to <code>Message&lt;Person&gt;</code>:</p> <pre><code>@Outgoing(\"persons\")\npublic Multi&lt;String&gt; source() {\n    return Multi.createFrom().items(\"Neo\", \"Morpheus\", \"Trinity\");\n}\n\n// The messages need to be converted as they are emitted as Message&lt;String&gt;\n// and consumed as Message&lt;Person&gt;\n@Incoming(\"persons\")\npublic void consume(Person p) {\n    // ...\n}\n</code></pre> <p>Converters work for all supported method signatures. However, the signature must be well-formed to allow the extraction of the expected payload type. Wildcards and raw types do not support conversion. If the expected payload type cannot be extracted, or no converter fits, the message is passed as received.</p> <p>If multiple suitable converters are present, implementations should override the <code>getPriority</code> method returning the priority. The default priority is <code>100</code>. The converter lookup invokes converters with higher priority (from the least value to the greatest) first.</p>"},{"location":"concepts/decorators/","title":"Channel Decorators","text":"<p>SmallRye Reactive Messaging supports decorating reactive streams of incoming and outgoing channels for implementing cross-cutting concerns such as monitoring, tracing or message interception.</p> <p>Two symmetrical APIs are proposed for decorating publisher and subscriber channels, PublisherDecorator and SubscriberDecorator respectively.</p> <p>Important</p> <p><code>@Incoming</code> channels and channels bound to an outbound connector are both <code>Subscriber</code>s. Conversely <code>@Outgoing</code> channels and channels bound to an inbound connector are <code>Publisher</code>s.</p> <p>For example, to provide a decorator which counts consumed messages from incoming connector, implement a bean exposing the interface <code>PublisherDecorator</code>:</p> <pre><code>@ApplicationScoped\npublic class ConsumedMessageDecorator implements PublisherDecorator {\n\n    private final Map&lt;String, AtomicLong&gt; counters = new HashMap&lt;&gt;();\n\n    @Override\n    public Multi&lt;? extends Message&lt;?&gt;&gt; decorate(Multi&lt;? extends Message&lt;?&gt;&gt; publisher, String channelName,\n            boolean isConnector) {\n        if (isConnector) {\n            AtomicLong counter = new AtomicLong();\n            counters.put(channelName, counter);\n            return publisher.onItem().invoke(counter::incrementAndGet);\n        } else {\n            return publisher;\n        }\n    }\n\n    @Override\n    public int getPriority() {\n        return 10;\n    }\n\n    public long getMessageCount(String channel) {\n        return counters.get(channel).get();\n    }\n}\n</code></pre> <p>Decorators' <code>decorate</code> method is called only once per channel at application deployment when graph wiring is taking place. Decorators are very powerful because they receive the stream of messages (Mutiny <code>Multi&lt;Message&lt;?&gt;&gt;</code>) and potentially return a new stream of messages.</p> <p>Note that if a decorator is available it will be called on every channel. The <code>decorate</code> method receives the channel name and whether the channel is a connector or not as parameters. Decorators are called ordered from highest to lowest priority (from the least value to the greatest), obtained using the <code>jakarta.enterprise.inject.spi.Prioritized#getPriority</code> method.</p> <p>Note</p> <p>The <code>SubscriberDecorator</code> receive a list of channel names because <code>@Incoming</code> annotation is repeatable and consuming methods can be linked to multiple channels.</p>"},{"location":"concepts/decorators/#intercepting-incoming-and-outgoing-messages","title":"Intercepting Incoming and Outgoing Messages","text":"<p>Decorators (<code>PublisherDecorator</code> and <code>SubscriberDecorator</code>) can be used to intercept and alter messages, both on incoming and outgoing channels.</p> <p>Smallrye Reactive Messaging allows defining intercepting incoming and outgoing messages for a specific channel, using <code>IncomingInterceptor</code> and <code>OutgoingInterceptor</code> respectively.</p> <p>Only one interceptor is allowed to be bound for interception per outgoing channel. If no interceptors are found with a <code>@Identifier</code> but a <code>@Default</code> one is available, it is used. When multiple interceptors are available, the bean with the highest priority is used.</p>"},{"location":"concepts/decorators/#incominginterceptor","title":"<code>IncomingInterceptor</code>","text":"<p>To provide an incoming interceptor implement a bean exposing the interface IncomingInterceptor, qualified with a <code>@Identifier</code> with the channel name to intercept.</p> <pre><code>package interceptors;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.IncomingInterceptor;\n\n@Identifier(\"channel-a\")\n@ApplicationScoped\npublic class MyIncomingInterceptor implements IncomingInterceptor {\n\n    @Override\n    public Message&lt;?&gt; afterMessageReceive(Message&lt;?&gt; message) {\n        return message.withPayload(\"changed \" + message.getPayload());\n    }\n\n    @Override\n    public void onMessageAck(Message&lt;?&gt; message) {\n        // Called after message ack\n    }\n\n    @Override\n    public void onMessageNack(Message&lt;?&gt; message, Throwable failure) {\n        // Called after message nack\n    }\n}\n</code></pre> <p>An <code>IncomingInterceptor</code> can implement these three methods:</p> <ul> <li><code>Message&lt;?&gt; afterMessageReceive(Message&lt;?&gt; message)</code> : Called after receiving the message from an incoming connector.   The message can be altered by returning a new message from this method. The modified message will be consumed in incoming channels.</li> <li><code>void onMessageAck(Message&lt;?&gt; message)</code> : Called after message acknowledgment.</li> <li><code>void onMessageNack(Message&lt;?&gt; message, Throwable failure)</code> : Called after message negative-acknowledgment.</li> </ul> <p>Note</p> <p>If you are willing to adapt an incoming message payload to fit a consuming method receiving type, you can use <code>MessageConverter</code>s.</p>"},{"location":"concepts/decorators/#outgoinginterceptor","title":"<code>OutgoingInterceptor</code>","text":"<p>To provide an outgoing interceptor implement a bean exposing the interface OutgoingInterceptor, qualified with a <code>@Identifier</code> with the channel name to intercept.</p> <pre><code>package interceptors;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.OutgoingInterceptor;\nimport io.smallrye.reactive.messaging.OutgoingMessageMetadata;\n\n@Identifier(\"channel-a\")\n@ApplicationScoped\npublic class MyOutgoingInterceptor implements OutgoingInterceptor {\n\n    @Override\n    public Message&lt;?&gt; onMessage(Message&lt;?&gt; message) {\n        return message.withPayload(\"changed \" + message.getPayload());\n    }\n\n    @Override\n    public void onMessageAck(Message&lt;?&gt; message) {\n        message.getMetadata(OutgoingMessageMetadata.class)\n                .ifPresent(m -&gt; m.getResult());\n    }\n\n    @Override\n    public void onMessageNack(Message&lt;?&gt; message, Throwable failure) {\n\n    }\n}\n</code></pre> <p>An <code>OutgoingInterceptor</code> can implement these three methods:</p> <ul> <li><code>Message&lt;?&gt; beforeMessageSend(Message&lt;?&gt; message)</code> : Called before passing the message to the outgoing connector for transmission. The message can be altered by returning a new message from this method.</li> <li><code>void onMessageAck(Message&lt;?&gt; message)</code> : Called after message acknowledgment. This callback can access <code>OutgoingMessageMetadata</code> which will hold the result of the message transmission to the broker, if supported by the connector. This is only supported by MQTT and Kafka connectors.</li> <li><code>void onMessageNack(Message&lt;?&gt; message, Throwable failure)</code> : Called after message negative-acknowledgment.</li> </ul>"},{"location":"concepts/emitter/","title":"Emitter and Channels","text":"<p>It is not rare to combine in a single application imperative parts (Jax-RS, regular CDI beans) and reactive parts (beans with <code>@Incoming</code> and <code>@Outgoing</code> annotations). In these case, it\u2019s often required to send messages from the imperative part to the reactive part. In other words, send messages to channels handled by reactive messaging and how can you retrieve messages.</p>"},{"location":"concepts/emitter/#emitter-and-channel","title":"Emitter and @Channel","text":"<p>To send things (payload or <code>Message</code>) from imperative code to a specific channel you need to use:</p> <ol> <li>the org.eclipse.microprofile.reactive.messaging.Channel annotations</li> <li>the org.eclipse.microprofile.reactive.messaging.Emitter type</li> </ol> <p>The <code>@Channel</code> lets you indicate to which channel you are going to send your payloads or messages. The <code>Emitter</code> is the object to use to send these payloads or messages.</p> <pre><code>import org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Emitter;\n\n@ApplicationScoped\npublic class MyImperativeBean {\n\n    @Inject\n    @Channel(\"prices\")\n    Emitter&lt;Double&gt; emitter;\n\n    // ...\n\n    public void send(double d) {\n        emitter.send(d);\n    }\n}\n</code></pre> <p>The <code>Emitter</code> class takes a type parameter. It\u2019s the type of payload. Even if you want to send <code>Messages</code>, the type is the payload type.</p> <p>Important</p> <p>You must have a <code>@Incoming(\"prices\")</code> somewhere in your application (meaning a method consuming messages transiting on the channel <code>prices</code>), or an outbound connector configured to manage the <code>prices</code> channel (<code>mp.messaging.outgoing.prices...</code>)</p>"},{"location":"concepts/emitter/#sending-payloads","title":"Sending payloads","text":"<p>Sending payloads is done as follows:</p> <pre><code>@Inject\n@Channel(\"prices\")\nEmitter&lt;Double&gt; emitterForPrices;\n\npublic void send(double d) {\n    emitterForPrices.send(d);\n}\n</code></pre> <p>When sending a payload, the emitter returns a <code>CompletionStage</code>. This <code>CompletionStage</code> gets completed once the message created from the payload is acknowledged:</p> <pre><code>public void sendAndAwaitAcknowledgement(double d) {\n    CompletionStage&lt;Void&gt; acked = emitterForPrices.send(d);\n    // sending a payload returns a CompletionStage completed\n    // when the message is acknowledged\n    acked.toCompletableFuture().join();\n}\n</code></pre> <p>If the processing fails, the <code>CompletionStage</code> gets completed exceptionally (with the reason of the nack).</p>"},{"location":"concepts/emitter/#sending-messages","title":"Sending messages","text":"<p>You can also send <code>Messages</code>:</p> <pre><code>public void sendAsMessage(double d) {\n    emitterForPrices.send(Message.of(d));\n}\n</code></pre> <p>When sending a <code>Message</code>, the emitter does not return a <code>CompletionStage</code>, but you can pass the ack/nack callback, and be called when the message is acked/nacked.</p> <pre><code>public void sendAsMessageWithAck(double d) {\n    emitterForPrices.send(Message.of(d, () -&gt; {\n        // Called when the message is acknowledged.\n        return CompletableFuture.completedFuture(null);\n    },\n            reason -&gt; {\n                // Called when the message is acknowledged negatively.\n                return CompletableFuture.completedFuture(null);\n            }));\n}\n</code></pre> <p>Sending messages also let you pass metadata.</p> <pre><code>public void sendAsMessageWithAckAndMetadata(double d) {\n    MyMetadata metadata = new MyMetadata();\n    emitterForPrices.send(Message.of(d, Metadata.of(metadata),\n            () -&gt; {\n                // Called when the message is acknowledged.\n                return CompletableFuture.completedFuture(null);\n            },\n            reason -&gt; {\n                // Called when the message is acknowledged negatively.\n                return CompletableFuture.completedFuture(null);\n            }));\n}\n</code></pre> <p>Metadata can be used to propagate some context objects with the message.</p>"},{"location":"concepts/emitter/#overflow-management","title":"Overflow management","text":"<p>When sending messages from imperative code to reactive code, you must be aware of back-pressure. Indeed, messages sent using the emitter and stored in a queue. If the consumer does not process the messages quickly enough, this queue can become a memory hog and you may even run out of memory.</p> <p>To control what need to happen when the queue becomes out of control, use the OnOverflow annotation. <code>@OnOverflow</code> lets you configure:</p> <ul> <li>the maximum size of the queue (default is 256)</li> <li>what needs to happen when this size is reached (fail, drop...)</li> </ul> <pre><code>// Set the max size to 10 and fail if reached\n@OnOverflow(value = OnOverflow.Strategy.BUFFER, bufferSize = 10)\n@Inject\n@Channel(\"channel\")\nEmitter&lt;String&gt; emitterWithBuffer;\n\n// [DANGER ZONE] no limit\n@OnOverflow(OnOverflow.Strategy.UNBOUNDED_BUFFER)\n@Inject\n@Channel(\"channel\")\nEmitter&lt;String&gt; danger;\n\n// Drop the new messages if the size is reached\n@OnOverflow(OnOverflow.Strategy.DROP)\n@Inject\n@Channel(\"channel\")\nEmitter&lt;String&gt; dropping;\n\n// Drop the previously sent messages if the size is reached\n@OnOverflow(OnOverflow.Strategy.LATEST)\n@Inject\n@Channel(\"channel\")\nEmitter&lt;String&gt; dropOldMessages;\n</code></pre> <p>The supported strategies are:</p> <ul> <li> <p><code>OnOverflow.Strategy.BUFFER</code> - use a buffer to store the elements     until they are consumed. If the buffer is full, a failure is     propagated (and the thread using the emitted gets an exception)</p> </li> <li> <p><code>OnOverflow.Strategy.UNBOUNDED_BUFFER</code> - use an unbounded buffer to     store the elements</p> </li> <li> <p><code>OnOverflow.Strategy.DROP</code> - drops the most recent value if the     downstream can\u2019t keep up. It means that new value emitted by the     emitter are ignored.</p> </li> <li> <p><code>OnOverflow.Strategy.FAIL</code> - propagates a failure in case the     downstream can\u2019t keep up.</p> </li> <li> <p><code>OnOverflow.Strategy.LATEST</code> - keeps only the latest value, dropping     any previous value if the downstream can\u2019t keep up.</p> </li> <li> <p><code>OnOverflow.Strategy.NONE</code> - ignore the back-pressure signals     letting the downstream consumer to implement a strategy.</p> </li> </ul>"},{"location":"concepts/emitter/#defensive-emission","title":"Defensive emission","text":"<p>Having an emitter injected into your code does not guarantee that someone is ready to consume the message. For example, a subscriber may be connecting to a remote broker. If there are no subscribers, using the <code>send</code> method will throw an exception.</p> <p>The <code>emitter.hasRequests()</code> method indicates that a subscriber subscribes to the channel and requested items. So, you can wrap your emission with:</p> <pre><code>if (emitter.hasRequests()) {\n    emitter.send(\"hello\");\n}\n</code></pre> <p>If you use the <code>OnOverflow.Strategy.DROP</code>, you can use the <code>send</code> method even with no subscribers nor demands. The message will be nacked immediately.</p>"},{"location":"concepts/emitter/#retrieving-channels","title":"Retrieving channels","text":"<p>You can use the <code>@Channel</code> annotation to inject in your bean the underlying stream. Note that in this case, you will be responsible for the subscription:</p> <pre><code>@Inject\n@Channel(\"my-channel\")\nMulti&lt;String&gt; streamOfPayloads;\n\n@Inject\n@Channel(\"my-channel\")\nMulti&lt;Message&lt;String&gt;&gt; streamOfMessages;\n\n@Inject\n@Channel(\"my-channel\")\nPublisher&lt;String&gt; publisherOfPayloads;\n\n@Inject\n@Channel(\"my-channel\")\nPublisher&lt;Message&lt;String&gt;&gt; publisherOfMessages;\n</code></pre> <p>Important</p> <p>You must have a <code>@Outgoing(\"my-channel\")</code> somewhere in your application (meaning a method generating messages transiting on the channel <code>my-channel</code>), or an inbound connector configured to manage the <code>prices</code> channel (<code>mp.messaging.incoming.prices...</code>)</p> <p>Injected channels merge all the matching outgoing - so if you have multiple <code>@Outgoing(\"out\")</code>, <code>@Inject @Channel(\"out\")</code> gets all the messages.</p> <p>If your injected channel receives payloads (<code>Multi&lt;T&gt;</code>), it acknowledges the message automatically, and support multiple subscribers. If you injected channel receives <code>Message</code> (<code>Multi&lt;Message&lt;T&gt;&gt;</code>), you will be responsible for the acknowledgement and broadcasting.</p>"},{"location":"concepts/emitter/#emitter-and-broadcast","title":"Emitter and @Broadcast","text":"<p>When using an <code>Emitter</code>, you can now <code>@Broadcast</code> what is emitted to all subscribers.</p> <p>Here is an example of emitting a price with two methods marked <code>@Incoming</code> to receive the broadcast:</p> <pre><code>@Inject\n@Broadcast\n@Channel(\"prices\")\nEmitter&lt;Double&gt; emitter;\n\npublic void emit(double d) {\n    emitter.send(d);\n}\n\n@Incoming(\"prices\")\npublic void handle(double d) {\n    // Handle the new price\n}\n\n@Incoming(\"prices\")\npublic void audit(double d) {\n    // Audit the price change\n}\n</code></pre> <p>For more details see @Broadcast documentation.</p>"},{"location":"concepts/emitter/#mutiny-emitter","title":"Mutiny Emitter","text":"<p>If you prefer to utilize <code>Uni</code> in all your code, there is now a <code>MutinyEmitter</code> that will return <code>Uni&lt;Void&gt;</code> instead of <code>void</code>.</p> <pre><code>@Inject\n@Channel(\"prices\")\nMutinyEmitter&lt;Double&gt; emitter;\n\npublic Uni&lt;Void&gt; send(double d) {\n    return emitter.send(d);\n}\n</code></pre> <p>There\u2019s also the ability to block on sending the event to the emitter. It will only return from the method when the event is acknowledged, or nacked, by the receiver:</p> <pre><code>public void sendAwait(double d) {\n    emitter.sendAndAwait(d);\n}\n</code></pre> <p>And if you don\u2019t need to worry about the success or failure of sending an event, you can <code>sendAndForget</code>:</p> <pre><code>public Cancellable sendForget(double d) {\n    return emitter.sendAndForget(d);\n}\n</code></pre>"},{"location":"concepts/emitter/#custom-emitter-implementations","title":"Custom Emitter Implementations","text":"<p>Experimental</p> <p>Custom emitter implementations is an experimental feature.</p> <p><code>Emitter</code> and <code>MutinyEmitter</code> are two implementations of the emitter concept, where imperative code in your application can send messages to Reactive Messaging channels.</p> <p>With <code>EmitterFactory</code> it is possible to provide custom implementations, and application facing emitter interfaces.</p> <p>In the following example, the injectable custom emitter interface is <code>CustomEmitter</code>, and it is implemented by <code>CustomEmitterImpl</code>:</p> <pre><code>public interface CustomEmitter&lt;T&gt; extends EmitterType {\n\n    &lt;M extends Message&lt;? extends T&gt;&gt; void sendAndForget(M msg);\n\n}\n\npublic static class CustomEmitterImpl&lt;T&gt; implements CustomEmitter&lt;T&gt;, MessagePublisherProvider&lt;Object&gt; {\n\n    Publisher&lt;Message&lt;?&gt;&gt; publisher;\n\n    public CustomEmitterImpl(EmitterConfiguration configuration, long defaultBufferSize) {\n        //... initialize emitter with configuration\n    }\n\n    @Override\n    public Publisher&lt;Message&lt;?&gt;&gt; getPublisher() {\n        return publisher;\n    }\n\n    @Override\n    public &lt;M extends Message&lt;? extends T&gt;&gt; void sendAndForget(M msg) {\n        //... send to stream\n    }\n}\n</code></pre> <p>Note that <code>CustomEmitter</code> interface extends <code>EmitterType</code>, which is a marker interface for discovering custom emitter types. Also, <code>CustomEmitterImpl</code> implements the <code>MessagePublisherProvider</code>, which is used by the framework to transform this emitter to a channel.</p> <p>Then we need to provide an implementation of the <code>EmitterFactory</code> interface:</p> <pre><code>@EmitterFactoryFor(CustomEmitter.class)\n@ApplicationScoped\npublic static class CustomEmitterFactory implements EmitterFactory&lt;CustomEmitterImpl&lt;Object&gt;&gt; {\n\n    @Inject\n    ChannelRegistry channelRegistry;\n\n    @Override\n    public CustomEmitterImpl&lt;Object&gt; createEmitter(EmitterConfiguration configuration, long defaultBufferSize) {\n        return new CustomEmitterImpl&lt;&gt;(configuration, defaultBufferSize);\n    }\n\n    @Produces\n    @Channel(\"\") // Stream name is ignored during type-safe resolution\n    &lt;T&gt; CustomEmitter&lt;T&gt; produce(InjectionPoint injectionPoint) {\n        String channelName = ChannelProducer.getChannelName(injectionPoint);\n        return channelRegistry.getEmitter(channelName, CustomEmitter.class);\n    }\n}\n</code></pre> <p>The <code>CustomEmitterFactory</code> is a CDI managed bean, which implements the <code>EmitterFactory</code>. It is qualified with <code>EmitterFactoryFor</code> annotation which is configured with the emitter interface <code>CustomEmitter</code> that this factory provides.</p> <p>Smallrye Reactive Messaging discovers the emitter factory during the CDI deployment validation and verifies that custom emitters used by the application have corresponding emitter factories. It'll use the emitter factory to create the emitter implementation and will register the implementation into the <code>ChannelRegistry</code>.</p> <p>Note that the <code>CustomEmitterFactory</code> also uses the <code>ChannelRegistry</code> and provides the custom emitter with <code>@Produces</code>.</p> <p>Finally, the application can inject and use the <code>CustomEmitter</code> as a normal emitter channel:</p> <pre><code>@Inject\n@Channel(\"custom-emitter-channel\")\nCustomEmitter&lt;String&gt; customEmitter;\n\n//...\n\npublic void emitMessage() {\n    customEmitter.sendAndForget(Message.of(\"a\"));\n    customEmitter.sendAndForget(Message.of(\"b\"));\n    customEmitter.sendAndForget(Message.of(\"c\"));\n}\n</code></pre>"},{"location":"concepts/generic-payloads/","title":"Generic Payloads","text":"<p>Experimental</p> <p>Generic payloads are an experimental feature and the API is subject to change.</p> <p>When using reactive messaging, <code>Message</code> flow in your system each message has a payload but can also contain metadata, as explained in Messages, Payload, Metadata. The metadata can hold information, for example in an outgoing channel, additional properties of the outgoing message to be sent to the broker.</p> <p>It is sometimes preferable to continue using the payload signatures, and also being able to attach metadata. Using <code>GenericPayload</code> allows customizing metadata when handling payloads in SmallRye Reactive Messaging <code>@Incoming</code> and <code>@Outgoing</code> methods. <code>GenericPayload</code> is a wrapper type, like the <code>Message</code>, containing a payload and metadata, without requiring handling acknowledgments manually.</p> <pre><code>@Outgoing(\"out\")\nMulti&lt;GenericPayload&lt;String&gt;&gt; produce() {\n    return Multi.createFrom().range(0, 100)\n            .map(i -&gt; GenericPayload.of(\"&gt;&gt; \" + i, Metadata.of(new MyMetadata())));\n}\n</code></pre> <p>You can combine generic payloads with metadata injection :</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\nGenericPayload&lt;String&gt; process(int payload, MyMetadata metadata) {\n    // use the injected metadata\n    String id = metadata.getId();\n    return GenericPayload.of(\"&gt;&gt; \" + payload + \" \" + id,\n            Metadata.of(metadata, new MyMetadata(\"Bob\", \"Alice\")));\n}\n</code></pre> <p>Note that the metadata provided with the outgoing generic payload is merged with the incoming message metadata.</p> <p>Limitations</p> <p>There are several limitations for the use of <code>GenericPayload</code>: <code>GenericPayload</code> is not supported in emitters, as normal outgoing <code>Message</code> can be used for that purpose. While <code>GenericPayload&lt;T&gt;</code> can be used as an incoming payload type, message converters are not applied to the payload type <code>T</code>.</p>"},{"location":"concepts/incoming-concurrency/","title":"Incoming Channel Concurrency","text":"<p>Experimental</p> <p>Incoming channel <code>concurrency</code> config is an experimental feature.</p> <p>The <code>concurrency</code> attribute for incoming channels provides a mechanism to enable concurrent non-blocking processing of incoming messages. When applied to a channel, this attribute specifies the number of copies of that channel to be created and wired to the processing method, allowing multiple messages to be processed concurrently.</p> <p>For example, concurrency configuration for a Kafka incoming channel the configuration will look like:</p> <pre><code>mp.messaging.incoming.my-channel.connector=smallrye-kafka\nmp.messaging.incoming.my-channel.topic=orders\nmp.messaging.incoming.my-channel.value.deserializer=org.apache.kafka.common.serialization.DoubleDeserializer\nmp.messaging.incoming.my-channel.concurrency=4\n</code></pre> <p>In this example, there will be 4 copies of the <code>my-channel</code> running concurrently, with distinctive internal channel names, <code>my-channel$1</code>, <code>my-channel$2</code>, etc. but all registered with the name <code>my-channel</code> to the <code>ChannelRegistry</code>.</p> <p>Kafka connector <code>partitions</code></p> <p>This is essentially very similar to the Kafka connector <code>partitions</code> configuration, but addresses some its limitations. Using <code>partitions</code> config in Kafka connector, channels are merged into the downstream message processor (method annotated with <code>@Incoming</code> or an injected channel) which is therefore called sequentially. This prevents concurrently processing messages from multiple partitions.</p> <p>The <code>concurrency</code> mechanism effectively allows polling Kafka partitions from separate clients and concurrently processing records while preserving the in-partition order.</p> <p>Copy channels inherit all configuration attributes of the main channel config. Per-copy channel attributes can be configured separately using the <code>$</code> separated channel names: <code>mp.messaging.incoming.my-channel$1.attribute</code>.</p> <p>For example, the following AMQP 1.0 channel defines 3 channels each with a different selector:</p> <pre><code>mp.messaging.incoming.data.connector=smallrye-amqp\nmp.messaging.incoming.data.address=address\nmp.messaging.incoming.data.durable=false\nmp.messaging.incoming.data.concurrency=3\nmp.messaging.incoming.data$1.selector=x='foo'\nmp.messaging.incoming.data$2.selector=x='bar'\nmp.messaging.incoming.data$3.selector=x='baz'\n</code></pre> <p>While the <code>concurrency</code> attribute is applicable to channels of any connector type, the channel implementation may need to take this configuration into account and adjust the threading accordingly. Connectors based on Vert.x event loop create a new event loop context per copy-channel to dispatch messages on distinct contexts.</p> <p>Non-blocking processing</p> <p>Note that while this allows concurrent processing, messages are still dispatched on Vert.x event loop threads, and should not be blocked.</p> <p>Otherwise, connectors treat copy channels as independent channels. For example, health check reports are registered separately for each copy-channel.</p>"},{"location":"concepts/incoming-metadata-injection/","title":"Incoming Metadata Injection","text":"<p>Experimental</p> <p>Metadata injection is an experimental feature.</p> <p>When using reactive messaging, <code>Message</code> flow in your system. Each message has a payload but can also contain metadata, as explained in Messages, Payload, Metadata.</p> <p>You can inject the incoming message metadata as a parameter depending on the method signature. Only methods receiving the payload as a parameter can also receive the metadata. For example, <code>@Incoming(\"in\") void consume(String s)</code> can also receive the metadata using the following signature: <code>@Incoming(\"in\") void consume(String s, MyMetadata m)</code>. Note that the payload must be the first parameter.</p> <p>The metadata can be injected using the class of the metadata to inject. In this case, the method is invoked with <code>null</code> if the metadata is missing. You can also inject the metadata using <code>Optional&lt;MetadataClass&gt;,</code> which would be empty if the incoming message does not contain metadata of type <code>MetadataClass.</code></p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic String process(String payload, MyMetadata metadata) {\n    // ...\n    return payload.toUpperCase();\n}\n\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Uni&lt;String&gt; process2(String payload, MyMetadata metadata) {\n    // ...\n    return Uni.createFrom().item(payload.toUpperCase());\n}\n\n// ...\n@Incoming(\"in\")\npublic void consume(String payload, Optional&lt;MyMetadata&gt; metadata, Period metadata2) {\n    // ...\n}\n\n@Incoming(\"in\")\npublic CompletionStage&lt;Void&gt; consume2(String payload, Optional&lt;MyMetadata&gt; metadata, Period metadata2) {\n    // ...\n    return CompletableFuture.completedFuture(null);\n}\n</code></pre>"},{"location":"concepts/incomings/","title":"Multiple Incoming Channels","text":"<p>Experimental</p> <p>Multiple <code>@Incomings</code> is an experimental feature.</p> <p>The <code>@Incoming</code> annotation is repeatable. It means that the method receives the messages transiting on every listed channels, in no specific order:</p> <pre><code>@Incoming(\"channel-1\")\n@Incoming(\"channel-2\")\npublic String process(String s) {\n    // get messages from channel-1 and channel-2\n    return s.toUpperCase();\n}\n</code></pre>"},{"location":"concepts/keyed-multi/","title":"Using <code>KeyedMulti</code>","text":"<p>Experimental</p> <p><code>KeyedMulti</code> is an experimental feature.</p> <p>When implementing a data streaming application, it's common to handle messages partitioned using a key. In this case, your stream manipulation often has to 1) group by key and 2) do the manipulation. Reactive Messaging can do the first step for you and reduce the code complexity. To do this, it injects <code>io.smallrye.reactive.messaging.keyed.KeyedMulti</code> in your method instead of a bare <code>Multi</code>.</p> <p>For example, imagine the following stream, represented as <code>key:value</code>: <code>\"a:1\", \"b:1\", \"a:2\", \"b:2\", \"b:3\"...</code> Then, let's consider the following method:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Multi&lt;String&gt; reshape(KeyedMulti&lt;String, String&gt; multi) {\n    // Called once per key and receive the stream of value for that specific key\n    String key = multi.key();\n    return multi.onItem().scan(AtomicInteger::new, (i, s) -&gt; {\n        i.incrementAndGet();\n        return i;\n    })\n            .map(i -&gt; Integer.toString(i.get()));\n}\n</code></pre> <p>Reactive Messaging automatically extracts the key and value from the incoming stream and invokes the method for each key. The received <code>KeyedMulti</code> represent the stream for each key. The <code>key()</code> method returns the extracted key.</p> <p>The key and value can be extracted from the payload but also (and often) from the message's metadata.</p> <p>When using Kafka, it automatically extracts the key/value from the Kafka records. In the other cases, or if you need custom extraction, you can implement your own <code>io.smallrye.reactive.messaging.keyed.KeyValueExtractor</code>. Implementations are exposed <code>ApplicationScoped</code> beans, and are used to extract the key and value. The following implementation extracts the key and value from payloads structured as \"key:value\":</p> <pre><code>import io.smallrye.reactive.messaging.keyed.KeyValueExtractor;\n\n@ApplicationScoped\npublic class KeyValueExtractorFromPayload implements KeyValueExtractor {\n\n    @Override\n    public boolean canExtract(Message&lt;?&gt; msg, Type keyType, Type valueType) {\n        // Called for the first message of the stream to select the extractor.\n        // Here we only check for the type, but the logic can be more complex\n        return keyType.equals(String.class) &amp;&amp; valueType.equals(String.class);\n    }\n\n    @Override\n    public String extractKey(Message&lt;?&gt; message, Type keyType) {\n        String string = message.getPayload().toString();\n        return string.substring(0, string.indexOf(\":\"));\n    }\n\n    @Override\n    public String extractValue(Message&lt;?&gt; message, Type valueType) {\n        String string = message.getPayload().toString();\n        return string.substring(string.indexOf(\":\") + 1);\n    }\n\n}\n</code></pre> <p>The extractor selection uses the <code>canExtract</code> method. When multiple extractors are available, you can implement the <code>getPriority()</code> method to give a lower priority. Default extractors have the priority 100. So, if you have a custom extractor with the priority 99, it will be used (if it replies <code>true</code> to the <code>canExtract</code> call). In addition, you can use the <code>io.smallrye.reactive.messaging.keyed.Keyed</code> annotation to indicate the class of the extractor to use. The extractor must still be a CDI bean, but the <code>canExtract</code> method is not called, and priority does not matter:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Multi&lt;String&gt; reshape(\n        @Keyed(KeyValueExtractorFromPayload.class) KeyedMulti&lt;String, String&gt; multi) {\n    return multi.onItem().scan(AtomicInteger::new, (i, s) -&gt; {\n        i.incrementAndGet();\n        return i;\n    }).map(i -&gt; Integer.toString(i.get()));\n}\n</code></pre>"},{"location":"concepts/logging/","title":"Logging","text":"<p>SmallRye Reactive Messaging uses JBoss Logging as logging API. This section explains how to configure the loggers for various logging backends.</p> <p>Tip</p> <p>If you are developing SmallRye Reactive Messaging and wonder about how the logs are managed, it uses JBoss Logging Tools.</p>"},{"location":"concepts/logging/#logging-backends","title":"Logging Backends","text":"<p>SmallRye Reactive Messaging uses the JBoss Logging library to write messages to a log file. This library is a logging bridge that integrates different log frameworks. You can decide which of the following frameworks you want to use for your application:</p> <ul> <li>JBoss LogManager (<code>jboss</code>)</li> <li>Log4j 2 (<code>log4j2</code>)</li> <li>Log4j 1 (<code>log4j</code>)</li> <li>Slf4j (<code>slf4j</code>)</li> <li>JDK logging (<code>jul</code>)</li> </ul> <p>You only need to add the chosen framework to the classpath, and the JBoss Logging library will pick it up. If there are multiple frameworks available on the classpath, it picks the first found (in the order from the list). Alternatively, you can set the <code>org.jboss.logging.provider</code> system property is one of the values given above.</p> <p>The concepts and log categories are the same for all frameworks. However, the format of the configuration file and the names of the log levels differ. Check the documentation of your logging library to find out which dependencies are required, the exact name of the log levels, and where the configuration should be written.</p>"},{"location":"concepts/logging/#log-categories","title":"Log Categories","text":"<p>As all applications and frameworks, SmallRye Reactive Messaging writes log messages in different categories and log levels. The categories group messages from specific connectors, classes or components. The following table shows the essential log categories used by SmallRye Reactive Messaging:</p> Category Description <code>io.smallrye.reactive.messaging</code> This category contains all the messages written by SmallRye Reactive Messaging. <code>io.smallrye.reactive.messaging.provider</code> This category contains all the messages generated by the core (provider). <code>io.smallrye.reactive.messaging.kafka</code> This category contains all the messages generated by the Kafka Connector. <code>io.smallrye.reactive.messaging.amqp</code> This category contains all the messages generated by the AMQP Connector. <code>io.smallrye.reactive.messaging.jms</code> This category contains all the messages generated by the JMS Connector. <code>io.smallrye.reactive.messaging.camel</code> This category contains all the messages generated by the Camel Connector. <code>io.smallrye.reactive.messaging.mqtt</code> This category contains all the messages generated by the MQTT (Client) Connector. <code>io.smallrye.reactive.messaging.mqtt-server</code> This category contains all the messages generated by the MQTT (Server) Connector. <p>The names of the log levels are defined by your logging framework and determine the amount and granularity of the log messages. You can assign a log level to each category. If you do not specify a specific category\u2019s log level, it will inherit the level from its parent category. Thus, setting the log level of <code>io.smallrye.reactive.messaging</code> influences every loggers from SmallRye Reactive Messaging.</p>"},{"location":"concepts/logging/#message-code","title":"Message Code","text":"<p>Each message has an identifier code. They are all prefixed with <code>SRMSG</code>, followed with the numeric code.</p> <p>In the following output, the code is <code>SRMSG00229</code>:</p> <pre><code>[2020-06-15 13:35:07] [INFO   ] SRMSG00229: Channel manager initializing...\n</code></pre>"},{"location":"concepts/logging/#recommended-logging-configurations","title":"Recommended logging configurations","text":""},{"location":"concepts/logging/#development","title":"Development","text":""},{"location":"concepts/logging/#log4j-1","title":"Log4J 1","text":"<p>log4j.properties</p> <pre><code>log4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.Target=System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p [%c] - %m%n\n\nlog4j.rootLogger=info, stdout\nlog4j.logger.io.smallrye.reactive.messaging=info\nlog4j.logger.org.jboss.weld=warn\n</code></pre>"},{"location":"concepts/logging/#log4j-2","title":"Log4J 2","text":"<p>log4j2.xml</p> <pre><code>&lt;Configuration monitorInterval=\"60\"&gt;\n  &lt;Properties&gt;\n    &lt;Property name=\"log-path\"&gt;PropertiesConfiguration&lt;/Property&gt;\n  &lt;/Properties&gt;\n  &lt;Appenders&gt;\n    &lt;Console name=\"Console-Appender\" target=\"SYSTEM*OUT\"&gt;\n      &lt;PatternLayout&gt;\n        &lt;pattern&gt;\n          [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n\n        &lt;/pattern&gt;&gt;\n      &lt;/PatternLayout&gt;\n    &lt;/Console&gt;\n  &lt;/Appenders&gt;\n  &lt;Loggers&gt;\n    &lt;Logger name=\"io.smallrye.reactive.messaging\" level=\"info\" additivity=\"false\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Logger&gt;\n    &lt;Logger name=\"org.jboss.weld\" level=\"warn\" additivity=\"false\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Logger&gt;\n    &lt;Root level=\"info\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Root&gt;\n  &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre>"},{"location":"concepts/logging/#jdk-jul","title":"JDK (JUL)","text":"<p>logging.properties</p> <pre><code>handlers=java.util.logging.ConsoleHandler\n\njava.util.logging.ConsoleHandler.level=FINEST\njava.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter\njava.util.logging.SimpleFormatter.format=[%1$tF %1$tT] [%4$-7s] %5$s %n\n\n.level=INFO\nio.smallrye.reactive.messaging.level=INFO\norg.jboss.weld.level=WARNING\n</code></pre>"},{"location":"concepts/logging/#logback-via-slf4j","title":"LogBack via SLF4J*","text":"<p>logback.xml</p> <pre><code>&lt;configuration&gt;\n  &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n    &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt;\n      &lt;Pattern&gt;\n        %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n\n      &lt;/Pattern&gt;\n    &lt;/encoder&gt;\n  &lt;/appender&gt;\n  &lt;logger name=\"io.smallrye.reactive.messaging\" level=\"info\" additivity=\"false\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/logger&gt;\n  &lt;logger name=\"org.jboss.weld\" level=\"warn\" additivity=\"false\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/logger&gt;\n  &lt;root level=\"info\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"concepts/logging/#production","title":"Production","text":""},{"location":"concepts/logging/#log4j-1_1","title":"Log4J 1","text":"<p>log4j.properties</p> <pre><code>log4j.appender.stdout=org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.Target=System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p [%c] - %m%n\n\nlog4j.rootLogger=info, stdout\nlog4j.logger.io.smallrye.reactive.messaging=warn\nlog4j.logger.org.jboss.weld=error\n</code></pre>"},{"location":"concepts/logging/#log4j-2_1","title":"Log4J 2","text":"<p>log4j2.xml</p> <pre><code>&lt;Configuration monitorInterval=\"60\"&gt;\n  &lt;Properties&gt;\n    &lt;Property name=\"log-path\"&gt;PropertiesConfiguration&lt;/Property&gt;\n  &lt;/Properties&gt;\n  &lt;Appenders&gt;\n    &lt;Console name=\"Console-Appender\" target=\"SYSTEM*OUT\"&gt;\n      &lt;PatternLayout&gt;\n        &lt;pattern&gt;\n          [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n\n        &lt;/pattern&gt;&gt;\n      &lt;/PatternLayout&gt;\n    &lt;/Console&gt;\n  &lt;/Appenders&gt;\n  &lt;Loggers&gt;\n    &lt;Logger name=\"io.smallrye.reactive.messaging\" level=\"warn\" additivity=\"false\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Logger&gt;\n    &lt;Logger name=\"org.jboss.weld\" level=\"error\" additivity=\"false\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Logger&gt;\n    &lt;Root level=\"info\"&gt;\n      &lt;AppenderRef ref=\"Console-Appender\"/&gt;\n    &lt;/Root&gt;\n  &lt;/Loggers&gt;\n&lt;/Configuration&gt;\n</code></pre>"},{"location":"concepts/logging/#jdk-jul_1","title":"JDK (JUL)","text":"<p>logging.properties</p> <pre><code>handlers=java.util.logging.ConsoleHandler\n\njava.util.logging.ConsoleHandler.level=INFO\njava.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter\njava.util.logging.SimpleFormatter.format=[%1$tF %1$tT] [%4$-7s] %5$s %n\n\n.level=INFO\nio.smallrye.reactive.messaging.level=WARNING\norg.jboss.weld.level=SEVERE\n</code></pre> <p>logback.xml</p> <pre><code>&lt;configuration&gt;\n  &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n    &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt;\n      &lt;Pattern&gt;\n        %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n\n      &lt;/Pattern&gt;\n    &lt;/encoder&gt;\n  &lt;/appender&gt;\n  &lt;logger name=\"io.smallrye.reactive.messaging\" level=\"warn\" additivity=\"false\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/logger&gt;\n  &lt;logger name=\"org.jboss.weld\" level=\"error\" additivity=\"false\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/logger&gt;\n  &lt;root level=\"info\"&gt;\n    &lt;appender-ref ref=\"STDOUT\"/&gt;\n  &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"concepts/merge/","title":"Merge channels","text":"<p>Experimental</p> <p><code>@Merge</code> is an experimental feature.</p> <p>By default, messages transiting in a channel can arise from a single producer. Having multiple producers is considered erroneous and is reported at deployment time.</p> <p>The Merge annotation changes this behavior and indicates that a channel can have multiple producers. <code>@Merge</code> must be used with the <code>@Incoming</code> annotation:</p> <pre><code>@Incoming(\"in1\")\n@Outgoing(\"out\")\npublic int increment(int i) {\n    return i + 1;\n}\n\n@Incoming(\"in2\")\n@Outgoing(\"out\")\npublic int multiply(int i) {\n    return i * 2;\n}\n\n@Incoming(\"out\")\n@Merge\npublic void getAll(int i) {\n    //...\n}\n</code></pre> <p>In the previous example, the consumer gets all the messages (from both producers).</p> <p>The <code>@Merge</code> annotation allows configuring how the incoming messages (from the different producers) are merged into the channel. The <code>mode</code> attribute allows configuring this behavior:</p> <ul> <li> <p><code>ONE</code> picks a single producer, discarding the other producer;</p> </li> <li> <p><code>MERGE</code> (default) gets all the messages as they come, without any     defined order. Messages from different producers may be interleaved.</p> </li> <li> <p><code>CONCAT</code> concatenates the producers. The messages from one producer     are received until the messages from other producers are received.</p> </li> </ul> <p>Note</p> <p>Outbound connectors also support a <code>merge</code> attribute that allows consuming the messages to multiple upstreams. It will dispatch all the received messages.</p>"},{"location":"concepts/message-context/","title":"Message Contexts","text":"<p>Message context provides a way to propagate data along the processing of a message. It can be used to propagate message specific objects in an implicit manner and be able to retrieve them later, such as the user, session or transaction.</p> <p>Important</p> <p>Message contexts are only support by Kafka, AMQP, RabbitMQ and MQTT connectors.</p> <p>Note</p> <p>Message context support is an experimental and SmallRye only feature.</p>"},{"location":"concepts/message-context/#whats-a-message-context","title":"What's a message context","text":"<p>A message context is execution context on which a message is processed. Each stage of the processing is going to use the same execution context. Thus, it allows storing data which can later be restored. For example, you can imagine storing some authentication (<code>User</code> in the following example) data in one part of your processing and restore it in a later stage.</p> <p><pre><code>@Incoming(\"data\")\n@Outgoing(\"process\")\npublic Message&lt;String&gt; process(Message&lt;String&gt; input) {\n    // Extract some data from the message and store it in the context\n    User user = ...;\n    // Store the extracted data into the message context.\n    ContextLocals.put(\"user\", user);\n    record input;\n}\n\n@Incoming(\"process\")\n@Outgoing(\"after-process\")\npublic String handle(String payload) {\n   // You can retrieve the store data using\n   User user = ContextLocals.get(\"user\", null);\n\n   // ...\n   return payload;\n}\n</code></pre> The Message context is also available when using blocking or asynchronous stages (stage returning <code>Uni</code> or <code>CompletionStage</code>)</p>"},{"location":"concepts/message-context/#the-difference-with-metadata","title":"The difference with metadata","text":"<p>Message metadata can be used to provide a similar feature. However, it requires using <code>Messages</code> which can be inconvenient (need to handle the acknowledgement manually). Message Contexts provide a simpler API, closer to a Message CDI scope: you can save data, and restore it later. The implicit propagation avoid having to deal with <code>Messages</code>.</p>"},{"location":"concepts/message-context/#supported-signatures","title":"Supported signatures","text":"<p>Message context works with:</p> <ul> <li>methods consuming or producing <code>Messages</code>, <code>Uni&lt;Message&lt;T&gt;&gt;</code> and <code>CompletionStage&lt;Message&lt;T&gt;&gt;</code></li> <li>methods consuming or producing payloads, <code>Uni&lt;Payload&gt;</code> and <code>CompletionStage&lt;Payload&gt;</code>.</li> <li>blocking and non-blocking methods</li> </ul> <p>However, message context are NOT enforced when using methods consuming or producing:</p> <ul> <li><code>Multi</code>, <code>Flow.Publisher</code>, <code>Publisher</code> and <code>PublisherBuilder</code></li> <li><code>Subscriber</code>, <code>Flow.Subscriber</code>, and <code>SubscriberBuilder</code></li> <li><code>Processor</code>, <code>Flow.Processor</code>, and <code>ProcessorBuilder</code></li> </ul>"},{"location":"concepts/message-context/#under-the-hood","title":"Under the hood","text":"<p>Under the hood, the message context feature uses Vert.x duplicated contexts. A duplicated context is a view of the \"root\" (event loop) context, which is restored at each stage of the message processing.</p> <p>Each time that a compatible connector receives a message from a broker, it creates a new duplicated context and attaches it to the message. So the context is stored in the metadata of the message.</p> <p>When the message is processed, SmallRye Reactive Messaging makes sure that this processing is executed on the stored duplicated context.</p>"},{"location":"concepts/model/","title":"Development Model","text":"<p>Reactive Messaging proposes a CDI-based programming model to implement event-driven applications. Following the CDI principles, beans are forming the main building block of your application. Reactive Messaging provides a set of annotations and types to implement beans that generate, consume or process messages.</p>"},{"location":"concepts/model/#incoming-and-outgoing","title":"@Incoming and @Outgoing","text":"<p>Reactive Messaging provides two main annotations:</p> <ul> <li>org.eclipse.microprofile.reactive.messaging.Incoming - indicates the consumed channel</li> <li>org.eclipse.microprofile.reactive.messaging.Outgoing - indicates the populated channel</li> </ul> <p>These annotations are used on methods:</p> <pre><code>package beans;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n@ApplicationScoped\npublic class MessageProcessingBean {\n\n    @Incoming(\"consumed-channel\")\n    @Outgoing(\"populated-channel\")\n    public Message&lt;String&gt; process(Message&lt;String&gt; in) {\n        // Process the payload\n        String payload = in.getPayload().toUpperCase();\n        // Create a new message from `in` and just update the payload\n        return in.withPayload(payload);\n    }\n}\n</code></pre> <p>Note</p> <p>Reactive Messaging beans can either be in the application scope (<code>@ApplicationScoped</code>) or dependent scope (<code>@Dependent</code>).</p> <p>Manipulating messages can be cumbersome. When you are only interested in the payload, you can use the following syntax: The following code is equivalent to the snippet from above:</p> <pre><code>package beans;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n@ApplicationScoped\npublic class PayloadProcessingBean {\n\n    @Incoming(\"consumed-channel\")\n    @Outgoing(\"populated-channel\")\n    public String process(String in) {\n        return in.toUpperCase();\n    }\n}\n</code></pre> <p>Important</p> <p>You should not call methods annotated with <code>@Incoming</code> and/or <code>@Outgoing</code> directly from your code. They are invoked by the framework. Having user code invoking them would not have the expected outcome.</p> <p>SmallRye Reactive Messaging automatically binds matching <code>@Outgoing</code> to <code>@Incoming</code> to form a chain:</p> A chain of components <p>If we consider the following code:</p> <pre><code>@Outgoing(\"source\")\npublic Multi&lt;String&gt; generate() {\n    return Multi.createFrom().items(\"Hello\", \"from\", \"reactive\", \"messaging\");\n}\n\n@Incoming(\"source\")\n@Outgoing(\"sink\")\npublic String process(String in) {\n    return in.toUpperCase();\n}\n\n@Incoming(\"sink\")\npublic void consume(String processed) {\n    System.out.println(processed);\n}\n</code></pre> <p>It would generate the following chain:</p> <pre><code>generate --&gt; [ source ] --&gt; process --&gt; [ sink ] --&gt; consume\n</code></pre> <p>Methods annotated with <code>@Incoming</code> or <code>@Outgoing</code> don\u2019t have to be in the same bean (class). You can distribute them among a set of beans. Remote interactions are also possible when using connectors.</p> <p>Methods annotated with:</p> <ul> <li>only <code>@Outgoing</code> are used to generate messages or payloads</li> <li>only <code>@Incoming</code> are used to consume messages or payloads</li> <li>both <code>@Incoming</code> and <code>@Outgoing</code> are used to process messages or payloads; or transform the stream</li> </ul>"},{"location":"concepts/model/#creating-messages","title":"Creating messages","text":"<p>Messages are envelopes around payload. They are the vehicle. While manipulating payload is convenient, messages let you add metadata, handle acknowledgement...</p> <p>Creating <code>Messages</code> is done using the Message interface directly:</p> <pre><code>// Create a simple message wrapping a payload\nMessage&lt;Price&gt; m1 = Message.of(price);\n\n// Create a message with metadata\nMessage&lt;Price&gt; m2 = Message.of(price, Metadata.of(new PriceMetadata()));\n\n// Create a message with several metadata\nMessage&lt;Price&gt; m3 = Message.of(price,\n        Metadata.of(new PriceMetadata(), new MyMetadata()));\n\n// Create a message with an acknowledgement callback\nMessage&lt;Price&gt; m4 = Message.of(price, () -&gt; {\n    // Called when the message is acknowledged by the next consumer.\n    return CompletableFuture.completedFuture(null);\n});\n\n// Create a message with both metadata and acknowledgement callback\nMessage&lt;Price&gt; m5 = Message.of(price,\n        Metadata.of(new PriceMetadata()),\n        () -&gt; {\n            // Called when the message is acknowledged by the next consumer.\n            return CompletableFuture.completedFuture(null);\n        });\n</code></pre> <p>Messages accept <code>null</code> as payload. Channels connected to outbound connectors interpret messages with <code>null</code> payload differently depending on the technology. <pre><code>// Create a message with null payload\nMessage&lt;Price&gt; m6 = Message.of(null, Metadata.of(new PriceMetadata()));\n</code></pre></p> <p>You can also create new instance of <code>Message</code> from an existing one:</p> <pre><code>// Create a new message with a new payload but with the same metadata\nMessage&lt;Price&gt; m1 = message.withPayload(new Price(12.4));\n\n// Create a new message with a new payload and add another metadata\nMessage&lt;Price&gt; m2 = message\n        .withPayload(new Price(15.0))\n        .withMetadata(Metadata.of(new PriceMetadata()));\n\n// Create a new message with a new payload and a custom acknowledgement\nMessage&lt;Price&gt; m3 = message\n        .withPayload(new Price(15.0))\n        .withAck(() -&gt;\n        // acknowledge the incoming message\n        message.ack()\n                .thenAccept(x -&gt; {\n                    // do something\n                }));\n</code></pre> <p>Acknowledgement?</p> <p>Acknowledgement is an important part of messaging systems. This will be covered in the acknowledgement section.</p> <p>Connector Metadata</p> <p>Most connectors are providing metadata to let you extract technical details about the message, but also customize the outbound dispatching.</p>"},{"location":"concepts/model/#messages-vs-payloads","title":"Messages vs. Payloads","text":"<p>Reactive messaging offers flexibility when it comes to handling messages and their acknowledgements. The application developer can choose to finely handle acknowledgements per-message basis, by handling the <code>Message</code>-based signatures. Otherwise, when handling payloads, acknowledgements (and negative-acknowledgements) are handled by the framework. The following sections in this documentation detail both development models.</p> <p>While being the easier development model, in the past using payload-based signatures did not allow associating connector-specific metadata, making the <code>Message</code>-based signatures the de-facto choice even for the most common scenarios. This lead to using the connector custom-message implementation types, such as <code>IncomingKafkaRecord</code>, <code>KafkaRecord</code> or <code>IncomingRabbitMQMessage</code>, as a convenience for accessing connector-specific metadata.</p> <p>Not only this forces to handle acknowledgements manually, it also doesn't allow for incoming or outgoing <code>Messages</code> to be intercepted or observed.</p> <p>Custom <code>Message</code> types &amp; Message interception</p> <p>Custom <code>Message</code> types such as <code>IncomingKafkaRecord</code>, <code>KafkaRecord</code> or <code>IncomingRabbitMQMessage</code> are not compatible with features intercepting messages. Therefore, it is no longer recommended to use custom <code>Message</code> implementations in consumptions methods. Instead, you can either use the generic <code>Message</code> type and access specific metadata, or use the payload with metadata injection</p> <p>Since incoming metadata injection and generic payloads features added to SmallRye Reactive Messaging, it is easier to use payload signatures and benefit from acknowledgement handling and still access connector-specific metadata.</p>"},{"location":"concepts/model/#generating-messages","title":"Generating Messages","text":"<p>To produce messages to a channel, you need to use the <code>@Outgoing</code> annotation. This annotation takes a single parameter: the name of the populated channel.</p>"},{"location":"concepts/model/#generating-messages-synchronously","title":"Generating messages synchronously","text":"<p>You can generate messages synchronously. In this case, the method is called for every request from the downstream:</p> <pre><code>@Outgoing(\"my-channel\")\npublic Message&lt;Integer&gt; generateMessagesSynchronously() {\n    return Message.of(counter.getAndIncrement());\n}\n</code></pre> <p>Requests?</p> <p>Reactive Messaging connects components to build a reactive stream. In a reactive stream, the emissions are controlled by the consumer (downstream) indicating to the publisher (upstream) how many items it can consume. With this protocol, the consumers are never flooded.</p>"},{"location":"concepts/model/#generating-messages-using-completionstage","title":"Generating messages using CompletionStage","text":"<p>You can also return a <code>CompletionStage</code> / <code>CompletableFuture</code>. In this  case, Reactive Messaging waits until the <code>CompletionStage</code> gets  completed before calling it again.</p> <p>For instance, this signature is useful to poll messages from a source  using an asynchronous client:</p> <pre><code>@Outgoing(\"my-channel\")\npublic CompletionStage&lt;Message&lt;Price&gt;&gt; generateMessagesAsCompletionStage() {\n    return asyncClient.poll()\n            .thenApply(Message::of);\n}\n</code></pre>"},{"location":"concepts/model/#generating-messages-using-uni","title":"Generating messages using Uni","text":"<p>You can also return a Uni instance. In  this case, Reactive Messaging waits until the <code>Uni</code> emits its item  before calling it again.</p> <p>This signature is useful when integrating asynchronous clients providing  a Mutiny API.</p> <pre><code>@Outgoing(\"my-channel\")\npublic Uni&lt;Message&lt;Integer&gt;&gt; generateMessagesAsync() {\n    return Uni.createFrom().item(() -&gt; Message.of(counter.getAndIncrement()));\n}\n</code></pre>"},{"location":"concepts/model/#generating-reactive-streams-of-messages","title":"Generating Reactive Streams of messages","text":"<p>Instead of producing the message one by one, you can return the stream directly. If you have a data source producing Reactive Streams <code>Publisher</code> (or sub-types, such as <code>Multi</code>), this is the signature you are looking for:</p> <pre><code>public Flow.Publisher&lt;Message&lt;String&gt;&gt; generateMessageStream() {\n    Multi&lt;String&gt; multi = reactiveClient.getStream();\n    return multi.map(Message::of);\n}\n</code></pre> <p>In this case, the method is called once to retrieve the <code>Publisher</code>.</p>"},{"location":"concepts/model/#generating-payloads","title":"Generating Payloads","text":"<p>Instead of <code>Message</code>, you can produce payloads. In this case, Reactive Messaging produces a simple message from the payload using <code>Message.of</code>.</p>"},{"location":"concepts/model/#generating-payload-synchronously","title":"Generating payload synchronously","text":"<p>You can produce payloads synchronously. The framework calls the method upon request and create <code>Messages</code> around the produced payloads.</p> <pre><code>@Outgoing(\"my-channel\")\npublic Integer generatePayloadsSynchronously() {\n    return counter.getAndIncrement();\n}\n</code></pre>"},{"location":"concepts/model/#generating-payload-using-completionstage","title":"Generating payload using CompletionStage","text":"<p>You can also return <code>CompletionStage</code> or <code>CompletableFuture</code>. For example, if you have an asynchronous client returning <code>CompletionStage</code>, you can use it as follows, to poll the data one by one:</p> <pre><code>@Outgoing(\"my-channel\")\npublic CompletionStage&lt;Price&gt; generatePayloadsAsCompletionStage() {\n    return asyncClient.poll();\n}\n</code></pre>"},{"location":"concepts/model/#generating-payload-by-producing-unis","title":"Generating payload by producing Unis","text":"<p>You can also return a <code>Uni</code> if you have a client using Mutiny types:</p> <pre><code>@Outgoing(\"my-channel\")\npublic Uni&lt;Integer&gt; generatePayloadsAsync() {\n    return Uni.createFrom().item(() -&gt; counter.getAndIncrement());\n}\n</code></pre>"},{"location":"concepts/model/#generating-reactive-streams-of-payloads","title":"Generating Reactive Streams of payloads","text":"<p>Finally, you can return a <code>Publisher</code> (or a sub-type such as a <code>Multi</code>):</p> <pre><code>@Outgoing(\"my-channel\")\npublic Multi&lt;String&gt; generatePayloadsStream() {\n    Multi&lt;String&gt; multi = reactiveClient.getStream();\n    return multi;\n}\n</code></pre> <p>In this case, Reactive Messaging calls the method only once to retrieve the <code>Publisher</code>.</p>"},{"location":"concepts/model/#consuming-messages","title":"Consuming Messages","text":"<p>To consume messages from a channel, you need to use the <code>@Incoming</code> annotation. This annotation takes a single parameter: the name of the consumed channel.</p> <p>Because <code>Messages</code> must be acknowledged, consuming messages requires returning asynchronous results that would complete when the incoming message get acknowledged.</p> <p>For example, you can receive the <code>Message</code>, process it and return the acknowledgement as result:</p> <pre><code>@Incoming(\"my-channel\")\npublic CompletionStage&lt;Void&gt; consumeMessage(Message&lt;Price&gt; message) {\n    handle(message.getPayload());\n    return message.ack();\n}\n'\n</code></pre> <p>You can also return a <code>Uni</code> if you need to implement more complicated processing:</p> <pre><code>@Incoming(\"my-channel\")\npublic Uni&lt;Void&gt; consumeMessageUni(Message&lt;Price&gt; message) {\n    return Uni.createFrom().item(message)\n            .onItem().invoke(m -&gt; handle(m.getPayload()))\n            .onItem().transformToUni(x -&gt; Uni.createFrom().completionStage(message.ack()));\n}\n</code></pre>"},{"location":"concepts/model/#consuming-payloads","title":"Consuming Payloads","text":"<p>Unlike consuming messages, consuming payloads support both synchronous and asynchronous consumption.</p> <p>For example, you can consume a payload as follows:</p> <pre><code>@Incoming(\"my-channel\")\npublic void consumePayload(Price payload) {\n    // do something\n}\n</code></pre> <p>In this case, you don\u2019t need to deal with the acknowledgement yourself. The framework acknowledges the incoming message (that wrapped the payload) once your method returns successfully.</p> <p>If you need to achieve asynchronous actions, you can return a <code>CompletionStage</code> or a <code>Uni</code>:</p> <pre><code>@Incoming(\"my-channel\")\npublic CompletionStage&lt;Void&gt; consumePayloadCS(Price payload) {\n    CompletionStage&lt;Void&gt; cs = handleAsync(payload);\n    return cs;\n}\n</code></pre> <pre><code>@Incoming(\"my-channel\")\npublic Uni&lt;Void&gt; consumePayloadUni(Price payload) {\n    return Uni.createFrom().item(payload)\n            .onItem().invoke(this::handle)\n            .onItem().ignore().andContinueWithNull();\n}\n</code></pre> <p>In these 2 cases, the framework acknowledges the incoming message when the returned construct gets completed.</p>"},{"location":"concepts/model/#processing-messages","title":"Processing Messages","text":"<p>You can process <code>Message</code> both synchronously or asynchronously. This  later case is useful when you need to execute an asynchronous action during your processing such as invoking a remote service.</p> <p>Do process <code>Messages</code> synchronously uses:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Message&lt;String&gt; processMessage(Message&lt;Integer&gt; in) {\n    return in.withPayload(Integer.toString(in.getPayload()));\n}\n</code></pre> <p>This method transforms the <code>int</code> payload to a <code>String</code>, and wraps it into a <code>Message</code>.</p> <p>'''important \"Using <code>Message.withX</code> methods\" You may be surprised by the usage of <code>Message.withX</code> methods. It allows metadata propagation as the metadata would be copied from the incoming message and so dispatched to the next method.</p> <p>You can also process <code>Messages</code> asynchronously:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic CompletionStage&lt;Message&lt;String&gt;&gt; processMessageCS(Message&lt;Integer&gt; in) {\n    CompletionStage&lt;String&gt; cs = invokeService(in.getPayload());\n    return cs.thenApply(in::withPayload);\n}\n</code></pre> <p>Or using Mutiny:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Uni&lt;Message&lt;String&gt;&gt; processMessageUni(Message&lt;String&gt; in) {\n    return invokeService(in.getPayload())\n            .map(in::withPayload);\n}\n</code></pre> <p>In general, you want to create the new <code>Message</code> from the incoming one. It enables metadata propagation and post-acknowledgement. For this, use the <code>withX</code> method from the <code>Message</code> class returning a new <code>Message</code> instance but copy the content (metadata, ack/nack...).</p>"},{"location":"concepts/model/#processing-payloads","title":"Processing payloads","text":"<p>If you don\u2019t need to manipulate the envelope, you can process payload directly either synchronously or asynchronously:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic String processPayload(int in) {\n    return Integer.toString(in);\n}\n</code></pre> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic CompletionStage&lt;String&gt; processPayloadCS(int in) {\n    return invokeService(in);\n}\n</code></pre> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Uni&lt;String&gt; processPayload(String in) {\n    return invokeService(in);\n}\n</code></pre> <p>What about metadata?</p> <p>With these methods, the metadata are automatically propagated.</p>"},{"location":"concepts/model/#processing-streams","title":"Processing streams","text":"<p>The previous processing method were taking single <code>Message</code> or payload. Sometimes you need more advanced manipulation. For this, SmallRye Reactive Messaging lets you process the stream of <code>Message</code> or the stream of payloads directly:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Multi&lt;Message&lt;String&gt;&gt; processMessageStream(Multi&lt;Message&lt;Integer&gt;&gt; stream) {\n    return stream\n            .onItem().transformToUni(message -&gt; invokeService(message.getPayload())\n                    .onFailure().recoverWithItem(\"fallback\")\n                    .onItem().transform(message::withPayload))\n            .concatenate();\n\n}\n</code></pre> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Multi&lt;String&gt; processPayloadStream(Multi&lt;Integer&gt; stream) {\n    return stream\n            .onItem().transformToUni(payload -&gt; invokeService(payload)\n                    .onFailure().recoverWithItem(\"fallback\"))\n            .concatenate();\n\n}\n</code></pre> <p>You can receive either a (Reactive Streams) <code>Publisher</code>, a <code>PublisherBuilder</code> or (Mutiny) <code>Multi</code>. You can return any subclass of <code>Publisher</code> or a <code>Publisher</code> directly.</p> <p>Important</p> <p>These signatures do not support metadata propagation. In the case of a stream of <code>Message</code>, you need to propagate the metadata manually. In the case of a stream of payload, propagation is not supported, and incoming metadata are lost.</p>"},{"location":"concepts/observability/","title":"Observability API","text":"<p>Important</p> <p>Observability API is experimental and SmallRye only feature.</p> <p>Smallrye Reactive Messaging proposes an observability API that allows to observe messages received and send through inbound and outbound channels.</p> <p>For any observation to happen, you need to provide an implementation of the <code>MessageObservationCollector</code>, discovered as a CDI-managed bean.</p> <p>At wiring time the discovered <code>MessageObservationCollector</code> implementation <code>initObservation</code> method is called once per channel to initialize the <code>ObservationContext</code>. The default <code>initObservation</code> implementation returns a default <code>ObservationContext</code> object, but the collector implementation can provide a custom per-channel <code>ObservationContext</code> object that'll hold information necessary for the observation. The <code>ObservationContext#complete</code> method is called each time a message observation is completed \u2013 message being acked or nacked. The collector implementation can decide at initialization time to disable the observation per channel by returning a <code>null</code> observation context.</p> <p>For each new message, the collector is on <code>onNewMessage</code> method with the channel name, the <code>Message</code> and the <code>ObservationContext</code> object initialized beforehand. This method can react to the creation of a new message but also is responsible for instantiating and returning a <code>MessageObservation</code>. While custom implementations can augment the observability capability, SmallRye Reactive Messaging provides a default implementation <code>DefaultMessageObservation</code>.</p> <p>So a simple observability collector can be implemented as such:</p> <pre><code>package observability;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.reactive.messaging.observation.DefaultMessageObservation;\nimport io.smallrye.reactive.messaging.observation.MessageObservation;\nimport io.smallrye.reactive.messaging.observation.MessageObservationCollector;\nimport io.smallrye.reactive.messaging.observation.ObservationContext;\n\n@ApplicationScoped\npublic class SimpleMessageObservationCollector implements MessageObservationCollector&lt;ObservationContext&gt; {\n\n    @Override\n    public MessageObservation onNewMessage(String channel, Message&lt;?&gt; message, ObservationContext ctx) {\n        // Called after message has been created\n        return new DefaultMessageObservation(channel);\n    }\n\n}\n</code></pre> <p>A collector with a custom <code>ObservationContext</code> can be implemented as such :</p> <pre><code>package observability;\n\nimport java.time.Duration;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.reactive.messaging.observation.DefaultMessageObservation;\nimport io.smallrye.reactive.messaging.observation.MessageObservation;\nimport io.smallrye.reactive.messaging.observation.MessageObservationCollector;\nimport io.smallrye.reactive.messaging.observation.ObservationContext;\n\n@ApplicationScoped\npublic class ContextMessageObservationCollector\n        implements MessageObservationCollector&lt;ContextMessageObservationCollector.MyContext&gt; {\n\n    @Override\n    public MyContext initObservation(String channel, boolean incoming, boolean emitter) {\n        // Called on observation setup, per channel\n        // if returned null the observation for that channel is disabled\n        return new MyContext(channel, incoming, emitter);\n    }\n\n    @Override\n    public MessageObservation onNewMessage(String channel, Message&lt;?&gt; message, MyContext ctx) {\n        // Called after message has been created\n        return new DefaultMessageObservation(channel);\n    }\n\n    public static class MyContext implements ObservationContext {\n\n        private final String channel;\n        private final boolean incoming;\n        private final boolean emitter;\n\n        public MyContext(String channel, boolean incoming, boolean emitter) {\n            this.channel = channel;\n            this.incoming = incoming;\n            this.emitter = emitter;\n        }\n\n        @Override\n        public void complete(MessageObservation observation) {\n            // called after message processing has completed and observation is done\n            // register duration\n            Duration duration = observation.getCompletionDuration();\n            Throwable reason = observation.getReason();\n            if (reason != null) {\n                // message was nacked\n            } else {\n                // message was acked successfully\n            }\n        }\n    }\n\n}\n</code></pre>"},{"location":"concepts/outgoings/","title":"Multiple Outgoing Channels","text":"<p>Experimental</p> <p>Multiple <code>@Outgoings</code> is an experimental feature.</p> <p>The <code>@Outgoing</code> annotation is repeatable. It means that the method dispatches outgoing messages to multiple listed channels:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out1\")\n@Outgoing(\"out2\")\npublic String process(String s) {\n    // send messages from channel-in to both channel-out1 and channel-out2\n    return s.toUpperCase();\n}\n</code></pre> <p>The default behaviour is same as the @Broadcast annotation, meaning that outbound messages are dispatched to all listed outgoing channels.</p> <p>However, different dispatching mechanism can be employed:</p>"},{"location":"concepts/outgoings/#selectively-dispatching-messages-using-targeted-messages","title":"Selectively dispatching messages using <code>Targeted</code> messages","text":"<p>You can selectively dispatch messages to multiple outgoings by returning Targeted :</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out1\")\n@Outgoing(\"out2\")\n@Outgoing(\"out3\")\npublic Targeted process(double price) {\n    // send messages from channel-in to both channel-out1 and channel-out2\n    Targeted targeted = Targeted.of(\"out1\", \"Price: \" + price,\n            \"out2\", \"Quote: \" + price);\n    if (price &gt; 90.0) {\n        return targeted.with(\"out3\", price);\n    }\n    return targeted;\n}\n</code></pre> <p>In this example, three outgoing channels are declared on the <code>process</code> method but in some condition channel <code>out3</code> does not receive any messages.</p> <p>Coordinated acknowledgements</p> <p><code>Targeted</code> return types coordinate acknowledgements between outgoing messages and the incoming message, therefore the incoming message will be ack'ed only when all outgoing messages are ack'ed.</p> <p>In cases where you need to consume <code>Message</code> and handle metadata propagation more finely you can use TargetedMessages which is a <code>Message</code> type:</p> <pre><code>@Incoming(\"channel-in\")\n@Outgoing(\"channel-out1\")\n@Outgoing(\"channel-out2\")\n@Outgoing(\"channel-out3\")\npublic TargetedMessages processMessage(Message&lt;String&gt; msg) {\n    // send messages from channel-in to both channel-out1 and channel-out2\n    return Messages.chain(msg)\n            .with(Map.of(\"channel-out1\", msg.withPayload(msg.getPayload().toUpperCase()),\n                    \"channel-out2\", msg.withPayload(msg.getPayload().toLowerCase())));\n}\n</code></pre> <p>Note that in this case coordinated acknowledgements is handled explicitly using Messages utility.</p>"},{"location":"concepts/outgoings/#branching-outgoing-channels-with-multisplitter","title":"Branching outgoing channels with <code>MultiSplitter</code>","text":"<p>In stream transformer processors it can be useful to branch out an incoming stream into different sub-streams, based on some conditions.</p> <p>When consuming a <code>Multi</code>, you can use the <code>Multi.split</code> (see Mutiny documentation) operation to define multiple branches. The stream transformer method with multiple outgoings must return a MultiSplitter.</p> <pre><code>enum Caps {\n    ALL_CAPS,\n    ALL_LOW,\n    MIXED\n}\n\n@Incoming(\"in\")\n@Outgoing(\"sink1\")\n@Outgoing(\"sink2\")\n@Outgoing(\"sink3\")\npublic MultiSplitter&lt;String, Caps&gt; reshape(Multi&lt;String&gt; in) {\n    return in.split(Caps.class, s -&gt; {\n        if (Objects.equals(s, s.toLowerCase())) {\n            return Caps.ALL_LOW;\n        } else if (Objects.equals(s, s.toUpperCase())) {\n            return Caps.ALL_CAPS;\n        } else {\n            return Caps.MIXED;\n        }\n    });\n}\n</code></pre> <p>In this case the number of outgoing channels must match the number of branches given to <code>split</code> operation. Outgoing channels will be tried to be matched to branch identifier enum <code>toString</code> ignoring case. If not all branches are matched, it will fall back to one-by-one matching depending on the order of outgoing channel declarations and enum ordinals.</p>"},{"location":"concepts/pausable-channels/","title":"Pausable Channels","text":"<p>Based on reactive streams, Smallrye Reactive Messaging ensures that channels are back-pressured. This means that the flow of messages is controlled by the downstream consumer, whether it is a processing method or outgoing channel. Sometimes you may want to pause the flow of messages, for example, when the consumer is not ready to process them.</p> <p>Injected <code>@Channel</code> streams are not subscribed to by default, so the flow of messages is controlled by the application. But for <code>@Incoming</code> methods, the flow of messages is controlled by the runtime.</p> <p>Pausable channels are useful when you want to control the flow of messages within your application.</p>"},{"location":"concepts/pausable-channels/#creating-a-pausable-channel","title":"Creating a Pausable Channel","text":"<p>To use pausable channels, you need to activate it with the configuration property <code>pausable</code> set to <code>true</code>.</p> <pre><code>mp.messaging.incoming.my-channel.pausable=true\n</code></pre>"},{"location":"concepts/pausable-channels/#configuration-options","title":"Configuration Options","text":"<p>Pausable channels support the following configuration options:</p> <ul> <li><code>pausable.initially-paused</code> - Whether the channel starts in a paused state (default: <code>false</code>)</li> <li><code>pausable.late-subscription</code> - Whether to subscribe to the upstream after pausing (default: <code>false</code>)</li> <li><code>pausable.buffer-size</code> - Maximum buffer size for items received while the channel is paused (optional)</li> <li><code>pausable.buffer-strategy</code> - Strategy for handling already requested items that arrive while paused. Possible values are:<ul> <li><code>BUFFER</code> - Buffer items received while paused and deliver them when resumed (default)</li> <li><code>DROP</code> - Drop already requested items received while paused</li> <li><code>IGNORE</code> - Let already requested items flow through even while paused</li> </ul> </li> </ul> <p>Example configuration:</p> <pre><code>mp.messaging.incoming.my-channel.pausable=true\nmp.messaging.incoming.my-channel.pausable.initially-paused=true\nmp.messaging.incoming.my-channel.pausable.buffer-size=100\nmp.messaging.incoming.my-channel.pausable.buffer-strategy=BUFFER\nmp.messaging.incoming.my-channel.pausable.late-subscription=false\n</code></pre>"},{"location":"concepts/pausable-channels/#controlling-the-flow-of-messages","title":"Controlling the flow of messages","text":"<p>If a channel is configured to be pausable, you can inject the <code>PausableChannel</code> qualified with the <code>@Channel(\"channel-name\")</code> annotation to control the flow of messages. You can also get the <code>PausableChannel</code> by channel name from the <code>ChannelRegistry</code> programmatically, and pause or resume the channel as needed:</p> <pre><code>package pausable;\n\nimport jakarta.annotation.PostConstruct;\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\nimport io.smallrye.reactive.messaging.ChannelRegistry;\nimport io.smallrye.reactive.messaging.PausableChannel;\n\n@ApplicationScoped\npublic class PausableController {\n\n    // Option 1: Inject ChannelRegistry and retrieve the pausable channel\n    @Inject\n    ChannelRegistry registry;\n\n    // Option 2: Directly inject the pausable channel\n    @Inject\n    @Channel(\"my-channel\")\n    PausableChannel myChannel;\n\n    @PostConstruct\n    public void resume() {\n        // Wait for the application to be ready\n        // Option 1: Retrieve the pausable channel from the registry\n        PausableChannel pausable = registry.getPausable(\"my-channel\");\n        // Resume the processing of the messages\n        pausable.resume();\n    }\n\n    public void pause() {\n        // Option 2: Use the injected pausable channel\n\n        // Pause the processing of the messages\n        myChannel.pause();\n    }\n\n    @Incoming(\"my-channel\")\n    void process(String message) {\n        // Process the message\n    }\n\n}\n</code></pre> <p>Warning</p> <p>Pausable channels only work with back-pressure aware subscribers, with bounded downstream requests.</p>"},{"location":"concepts/signatures/","title":"Supported signatures","text":"<p>The following tables list the supported method signatures and indicate the various supported features. For instance, they indicate the default and available acknowledgement strategies (when applicable).</p>"},{"location":"concepts/signatures/#method-signatures-to-generate-data","title":"Method signatures to generate data","text":"Signature Invocation time <code>@Outgoing Publisher&lt;Message&lt;O&gt;&gt; method()</code> ` Called once at assembly time <code>@Outgoing Publisher&lt;O&gt; method()</code> ` Called once at assembly time <code>@Outgoing Multi&lt;Message&lt;O&gt;&gt; method()</code> ` Called once at assembly time <code>@Outgoing Multi&lt;O&gt; method()</code> ` Called once at assembly time <code>@Outgoing Flow.Publisher&lt;Message&lt;O&gt;&gt; method()</code> ` Called once at assembly time <code>@Outgoing Flow.Publisher&lt;O&gt; method()</code> ` Called once at assembly time <code>@Outgoing PublisherBuilder&lt;Message&lt;O&gt;&gt; method()</code> ` Called once at assembly time <code>@Outgoing PublisherBuilder&lt;O&gt; method()</code> ` Called once at assembly time <code>@Outgoing Message&lt;O&gt; method()</code> ` Called for every downstream request, sequentially <code>@Outgoing O method()</code> ` Called for every downstream request, sequentially <code>@Outgoing CompletionStage&lt;Message&lt;O&gt;&gt; method()</code> ` Called for every downstream request, sequentially (After the completion of the last returned CompletionStage) <code>@Outgoing CompletionStage&lt;O&gt; method()</code> ` Called for every downstream request, , sequentially (After the completion of the last returned CompletionStage) <code>@Outgoing Uni&lt;Message&lt;O&gt;&gt; method()</code> ` Called for every downstream request, sequentially (After the completion of the last returned Uni) <code>@Outgoing Uni&lt;O&gt; method()</code> ` Called for every downstream request, , sequentially (After the completion of the last returned Uni)"},{"location":"concepts/signatures/#method-signatures-to-consume-data","title":"Method signatures to consume data","text":"Signature Invocation time Supported Acknowledgement Strategies <code>@Incoming void method(I p)</code> Called for every incoming payload (sequentially) POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming CompletionStage&lt;?&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, NONE, PRE_PROCESSING <code>@Incoming CompletionStage&lt;?&gt; method(I p)</code> Called for every incoming payload (sequentially) POST_PROCESSING, PRE_PROCESSING, NONE <code>@Incoming Uni&lt;?&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, NONE, PRE_PROCESSING <code>@Incoming Uni&lt;?&gt; method(I p)</code> Called for every incoming payload (sequentially) POST_PROCESSING, PRE_PROCESSING, NONE <code>@Incoming Subscriber&lt;Message&lt;I&gt;&gt; method()</code> Called once at assembly time MANUAL, POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming Subscriber&lt;I&gt; method()</code> Called once at assembly time POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming Flow.Subscriber&lt;Message&lt;I&gt;&gt; method()</code> Called once at assembly time MANUAL, POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming Flow.Subscriber&lt;I&gt; method()</code> Called once at assembly time POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming SubscriberBuilder&lt;Message&lt;I&gt;, ?&gt; method()</code> Called once at assembly time MANUAL, POST_PROCESSING, NONE, PRE_PROCESSING <code>@Incoming SubscriberBuilder&lt;I, ?&gt; method()</code> Called once at assembly time MANUAL, POST_PROCESSING, NONE, PRE_PROCESSING"},{"location":"concepts/signatures/#method-signatures-to-process-data","title":"Method signatures to process data","text":"Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation <code>@Outgoing @Incoming Message&lt;O&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) POST_PROCESSING (Smallrye only), MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming Message&lt;O&gt; method(I payload)</code> Called for every incoming message (sequentially) POST_PROCESSING (Smallrye only), NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming O method(I payload)</code> Called for every incoming payload (sequentially) POST_PROCESSING, NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming CompletionStage&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming CompletionStage&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) POST_PROCESSING, NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming CompletionStage&lt;Message&lt;O&gt;&gt; method(I payload)</code> Called for every incoming payload (sequentially) POST_PROCESSING (Smallrye only), NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming Uni&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming Uni&lt;Message&lt;O&gt;&gt; method(I payload)</code> Called for every incoming payload (sequentially) POST_PROCESSING (Smallrye only), NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming Uni&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) POST_PROCESSING, NONE, PRE_PROCESSING automatic <code>@Outgoing @Incoming Processor&lt;Message&lt;I&gt;, Message&lt;O&gt;&gt; method()</code> Called once at assembly time MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming Processor&lt;I, O&gt; method()</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming Flow.Processor&lt;Message&lt;I&gt;, Message&lt;O&gt;&gt; method()</code> Called once at assembly time MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming Flow.Processor&lt;I, O&gt; method()</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming ProcessorBuilder&lt;Message&lt;I&gt;, Message&lt;O&gt;&gt; method()</code> Called once at assembly time MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming ProcessorBuilder&lt;I, O&gt; method()</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming Publisher&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming Publisher&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) PRE_PROCESSING, POST_PROCESSING, NONE automatic <code>@Outgoing @Incoming Multi&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming Multi&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) PRE_PROCESSING, POST_PROCESSING, NONE automatic <code>@Outgoing @Incoming Flow.Publisher&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming Flow.Publisher&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) PRE_PROCESSING, POST_PROCESSING, NONE automatic <code>@Outgoing @Incoming PublisherBuilder&lt;Message&lt;O&gt;&gt; method(Message&lt;I&gt; msg)</code> Called for every incoming message (sequentially) MANUAL, PRE_PROCESSING, NONE manual <code>@Outgoing @Incoming PublisherBuilder&lt;O&gt; method(I payload)</code> Called for every incoming payload (sequentially) PRE_PROCESSING, POST_PROCESSING, NONE automatic <p>Note that in additional to the MicroProfile Reactive Messaging specification, SmallRye Reactive Messaging supports the post-processing acknowledgment handling with automatic metadata propagation for the following signatures:</p> <ul> <li><code>@Outgoing @Incoming Message&lt;O&gt; method(I payload)</code></li> <li><code>@Outgoing @Incoming CompletionStage&lt;Message&lt;O&gt;&gt; method(I payload)</code></li> <li><code>@Outgoing @Incoming Uni&lt;Message&lt;O&gt;&gt; method(I payload)</code></li> <li><code>@Outgoing @Incoming Message&lt;O&gt; method(Message&lt;I&gt; payload)</code> : For this signature, the post-processing acknowledgment handling is limited.   It covers cases for nacking incoming messages on caught exceptions at the method body, acking incoming messages when outgoing message is skipped by returning <code>null</code>, and chaining acknowlegment from outgoing message to the incoming.   However, if the incoming message has already been (n)acked, you will experience duplicate (n)acks.</li> </ul>"},{"location":"concepts/signatures/#method-signatures-to-manipulate-streams","title":"Method signatures to manipulate streams","text":"Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation <code>@Outgoing @Incoming Publisher&lt;Message&lt;O&gt;&gt; method(Publisher&lt;Message&lt;I&gt;&gt; pub)</code> Called once at assembly time MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming Publisher&lt;O&gt; method(Publisher&lt;I&gt; pub)</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming Multi&lt;Message&lt;O&gt;&gt; method(Multi&lt;Message&lt;I&gt;&gt; pub)</code> Called once at assembly time MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming Multi&lt;O&gt; method(Multi&lt;I&gt; pub)</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming Flow.Publisher&lt;Message&lt;O&gt;&gt; method(Flow.Publisher&lt;Message&lt;I&gt;&gt; pub)</code> Called once at assembly time MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming Flow.Publisher&lt;O&gt; method(Flow.Publisher&lt;I&gt; pub)</code> Called once at assembly time PRE_PROCESSING, NONE not supported <code>@Outgoing @Incoming PublisherBuilder&lt;Message&lt;O&gt;&gt; method(PublisherBuilder&lt;Message&lt;I&gt;&gt; pub)</code> Called once at assembly time MANUAL, NONE, PRE_PROCESSING manual <code>@Outgoing @Incoming PublisherBuilder&lt;O&gt; method(PublisherBuilder&lt;I&gt; pub)</code> Called once at assembly time NONE, PRE_PROCESSING not supported <p>Important</p> <p>When processing <code>Message</code>, it is often required to chain the incoming <code>Message</code> to enable post-processing acknowledgement and metadata propagation. Use the <code>with</code> (like <code>withPayload</code>) methods from the incoming message, so it copies the metadata and ack/nack methods. It returns a new <code>Message</code> with the right content.</p>"},{"location":"concepts/skipping/","title":"Skipping messages","text":"<p>Sometimes you receive a message and don\u2019t want to produce an output message. To handle this, you have several choices:</p> <ol> <li>for method processing single message or payload, producing <code>null</code>     would produce an ignored message (not forwarded)</li> <li>for method processing streams, you can generate an empty stream.</li> </ol>"},{"location":"concepts/skipping/#skipping-a-single-item","title":"Skipping a single item","text":"<p>To skip a single message or payload, return <code>null</code>:</p> <pre><code>// Skip when processing payload synchronously - returning `null`\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic String processPayload(String s) {\n    if (s.equalsIgnoreCase(\"skip\")) {\n        return null;\n    }\n    return s.toUpperCase();\n}\n\n// Skip when processing message synchronously - returning `null`\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Message&lt;String&gt; processMessage(Message&lt;String&gt; m) {\n    String s = m.getPayload();\n    if (s.equalsIgnoreCase(\"skip\")) {\n        m.ack();\n        return null;\n    }\n    return m.withPayload(s.toUpperCase());\n}\n\n// Skip when processing payload asynchronously - returning a `Uni` with a `null` value\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Uni&lt;String&gt; processPayloadAsync(String s) {\n    if (s.equalsIgnoreCase(\"skip\")) {\n        // Important, you must not return `null`, but a `null` content\n        return Uni.createFrom().nullItem();\n    }\n    return Uni.createFrom().item(s.toUpperCase());\n}\n\n// Skip when processing message asynchronously - returning a `Uni` with a `null` value\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Uni&lt;Message&lt;String&gt;&gt; processMessageAsync(Message&lt;String&gt; m) {\n    String s = m.getPayload();\n    if (s.equalsIgnoreCase(\"skip\")) {\n        m.ack();\n        return Uni.createFrom().nullItem();\n    }\n    return Uni.createFrom().item(m.withPayload(s.toUpperCase()));\n}\n</code></pre>"},{"location":"concepts/skipping/#skipping-in-a-stream","title":"Skipping in a stream","text":"<p>To skip a message or payload when manipulating a stream, emit an empty <code>Multi</code> (or <code>Publisher</code>):</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out-1\")\npublic Multi&lt;String&gt; processPayload(String s) {\n    if (s.equalsIgnoreCase(\"skip\")) {\n        return Multi.createFrom().empty();\n    }\n    return Multi.createFrom().item(s.toUpperCase());\n}\n\n@Incoming(\"in\")\n@Outgoing(\"out-2\")\npublic Multi&lt;Message&lt;String&gt;&gt; processMessage(Message&lt;String&gt; m) {\n    String s = m.getPayload();\n    if (s.equalsIgnoreCase(\"skip\")) {\n        return Multi.createFrom().empty();\n    }\n    return Multi.createFrom().item(m.withPayload(s.toUpperCase()));\n}\n\n@Incoming(\"in\")\n@Outgoing(\"out-3\")\npublic Multi&lt;String&gt; processPayloadStream(Multi&lt;String&gt; stream) {\n    return stream\n            .select().where(s -&gt; !s.equalsIgnoreCase(\"skip\"))\n            .onItem().transform(String::toUpperCase);\n}\n\n@Incoming(\"in\")\n@Outgoing(\"out-4\")\npublic Multi&lt;Message&lt;String&gt;&gt; processMessageStream(Multi&lt;Message&lt;String&gt;&gt; stream) {\n    return stream\n            .select().where(m -&gt; !m.getPayload().equalsIgnoreCase(\"skip\"))\n            .onItem().transform(m -&gt; m.withPayload(m.getPayload().toUpperCase()));\n}\n</code></pre>"},{"location":"concepts/testing/","title":"Testing your application","text":"<p>It\u2019s not rare to have to test your application but deploying the infrastructure can be cumbersome. While Docker or Test Containers have improved the testing experience, you may want to mock this infrastructure.</p> <p>SmallRye Reactive Messaging proposes an in-memory connector for this exact purpose. It allows switching the connector used for a channel with an in-memory connector. This in-memory connector provides a way to send messages to incoming channels, or check the received messages for outgoing channels.</p> <p>To use the in-memory connector, you need to add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-in-memory&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n  &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then, in a test, you can do something like:</p> <pre><code>package testing;\n\nimport jakarta.enterprise.inject.Any;\nimport jakarta.inject.Inject;\n\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\nimport io.smallrye.reactive.messaging.memory.InMemoryConnector;\nimport io.smallrye.reactive.messaging.memory.InMemorySink;\nimport io.smallrye.reactive.messaging.memory.InMemorySource;\n\n// @io.quarkus.test.junit.QuarkusTest or the Junit 5 extension that allows injection in tests\npublic class MyTest {\n\n    // 1. Switch the channels to the in-memory connector:\n    @BeforeAll\n    public static void switchMyChannels() {\n        InMemoryConnector.switchIncomingChannelsToInMemory(\"prices\");\n        InMemoryConnector.switchOutgoingChannelsToInMemory(\"processed-prices\");\n    }\n\n    // 2. Don't forget to reset the channel after the tests:\n    @AfterAll\n    public static void revertMyChannels() {\n        InMemoryConnector.clear();\n    }\n\n    // 3. Inject the in-memory connector in your test,\n    // or use the bean manager to retrieve the instance\n    @Inject\n    @Any\n    InMemoryConnector connector;\n\n    @Test\n    void test() {\n        // 4. Retrieves the in-memory source to send message\n        InMemorySource&lt;Integer&gt; prices = connector.source(\"prices\");\n        // 5. Retrieves the in-memory sink to check what is received\n        InMemorySink&lt;Integer&gt; results = connector.sink(\"processed-prices\");\n\n        // 6. Send fake messages:\n        prices.send(1);\n        prices.send(2);\n        prices.send(3);\n\n        // 7. Check you have received the expected messages\n        Assertions.assertEquals(3, results.received().size());\n    }\n}\n</code></pre> <p>When switching a channel to the in-memory connector, all the configuration properties are ignored.</p> <p>Warning</p> <p>This connector has been designed for testing purpose only. Switching the channel to in-memory connector means that the original connector is not invoked at all during tests. Therefore, if your code depends on a specific connector behaviour or a custom metadata you need to simulate those in your tests.</p> <p>The switch methods return <code>Map&lt;String, String&gt;</code> instances containing the set properties. While these system properties are already set, you can retrieve them and pass them around, for example if you need to start an external process with these properties:</p> <pre><code>public Map&lt;String, String&gt; start() {\n    Map&lt;String, String&gt; env = new HashMap&lt;&gt;();\n    env.putAll(InMemoryConnector.switchIncomingChannelsToInMemory(\"prices\"));\n    env.putAll(InMemoryConnector.switchOutgoingChannelsToInMemory(\"my-data-stream\"));\n    return env;\n}\n\npublic void stop() {\n    InMemoryConnector.clear();\n}\n</code></pre> <p>Note</p> <p>The in-memory connector support the <code>broadcast</code> and <code>merge</code> attributes. So, if your connector is configured with <code>broadcast: true</code>, the connector broadcasts the messages to all the channel consumers. If your connector is configured with <code>merge:true</code>, the connector receives all the messages sent to the mapped channel even when coming from multiple producers.</p>"},{"location":"concepts/testing/#vertx-context-with-in-memory-connector","title":"Vert.x Context with In-memory Connector","text":"<p>For the sake of simplicity, In-memory connector channels dispatch messages on the caller thread of <code>InMemorySource#send</code> method. However, most of the other connectors handle context propagation dispatching messages on separate duplicated Vert.x contexts.</p> <p>If this causes a change of behaviour in your tests, you can configure the in-memory connector channels with <code>run-on-vertx-context</code> attribute to dispatch events, including messages and acknowledgements, on a Vert.x context. Alternatively you can switch this behaviour using the <code>InMemorySource#runOnVertxContext</code> method.</p>"},{"location":"jms/advanced-jms/","title":"Advanced configuration","text":""},{"location":"jms/advanced-jms/#underlying-thread-pool","title":"Underlying thread pool","text":"<p>Lots of JMS operations are blocking and so not cannot be done on the caller thread. For this reason, these blocking operations are executed on a worker thread.</p> <p>You can configure the thread pool providing these worker threads using the following MicroProfile Config properties:</p> <ul> <li> <p><code>smallrye.jms.threads.max-pool-size</code> - the max number of threads     (Defaults to 10)</p> </li> <li> <p><code>smallrye.jms.threads.ttl</code> - the ttl of the created threads     (Defaults to 60 seconds)</p> </li> </ul>"},{"location":"jms/advanced-jms/#selecting-the-connectionfactory","title":"Selecting the ConnectionFactory","text":"<p>The JMS Connector requires a <code>jakarta.jms.ConnectionFactory</code> to be exposed as a CDI bean. The connector looks for a <code>jakarta.jms.ConnectionFactory</code> and delegate the interaction with the JMS server to this factory.</p> <p>In case you have several connection factories, you can use the <code>@Identifier</code> qualifier on your factory to specify the name. Then, in the channel configuration, configure the name as follows:</p> <pre><code># Configure the connector globally\nmp.messaging.connector.smallrye-jms.connection-factory-name=my-factory-name\n# Configure a specific incoming channel\nmp.messaging.incoming.my-channel.connection-factory-name=my-factory-name\n# Configure a specific outgoing channel\nmp.messaging.outgoing.my-channel.connection-factory-name=my-factory-name\n</code></pre>"},{"location":"jms/jms/","title":"The JMS connector","text":"<p>The JMS connector adds support for Jakarta Messaging to Reactive Messaging. It is designed to integrate with JakartaEE applications that are sending or receiving Jakarta Messaging Messages.</p> <p>Jakarta Messaging is a Java Message Oriented Middleware API for sending messages between two or more clients. It is a programming model to handle the producer-consumer messaging problem. It is a messaging standard that allows application components based on Jakarta EE to create, send, receive, and read messages. It allows the communication between different components of a distributed application to be loosely coupled, reliable, and asynchronous.</p>"},{"location":"jms/jms/#using-the-jms-connector","title":"Using the JMS connector","text":"<p>To you the JMS Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-jms&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-jms</code>.</p> <p>So, to indicate that a channel is managed by this connector you need:</p> <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-jms\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-jms\n</code></pre> <p>The JMS Connector requires a <code>jakarta.jms.ConnectionFactory</code> to be exposed (as CDI bean). The connector looks for a <code>jakarta.jms.ConnectionFactory</code> and delegate the interaction with the JMS server to this factory. In other words, it creates the JMS connection and context using this factory.</p> <p>So, in order to use this connector you would need to expose a <code>jakarta.jms.ConnectionFactory</code>:</p> <pre><code>import jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.enterprise.inject.Produces;\nimport jakarta.jms.ConnectionFactory;\n\nimport org.apache.activemq.artemis.jms.client.ActiveMQJMSConnectionFactory;\n\n@ApplicationScoped\npublic class ConnectionFactoryBean {\n\n    @Produces\n    ConnectionFactory factory() {\n        return new ActiveMQJMSConnectionFactory(\n                \"tcp://localhost:61616\",\n                null, null);\n    }\n\n}\n</code></pre> <p>The factory class may depend on your JMS connector/server.</p>"},{"location":"jms/receiving-jms-messages/","title":"Receiving messages from JMS","text":"<p>The JMS Connector retrieves JMS Message and maps each of them into Reactive Messaging <code>Messages</code>.</p>"},{"location":"jms/receiving-jms-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have a <code>jakarta.jms.ConnectionFactory</code> bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector.</p> <p>Configure your application to receive JMS messages on the <code>prices</code> channel as follows:</p> <pre><code>mp.messaging.incoming.prices.connector=smallrye-jms\n</code></pre> <p>Note</p> <p>You don\u2019t need to set the destination. By default, it uses the channel name (<code>prices</code>). You can configure the <code>destination</code> attribute to override it.</p> <p>Note</p> <p>By default the connector uses a <code>queue</code>. You can configure it to use a <code>topic</code> by setting <code>destination-type=topic</code>.</p> <p>Then, your application receives <code>Message&lt;Double&gt;</code>. You can consume the payload directly:</p> <pre><code>package jms.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class JmsPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(double price) {\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;Double&gt;</code>:</p> <pre><code>package jms.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class JmsPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;Double&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message\n        return price.ack();\n    }\n\n}\n</code></pre>"},{"location":"jms/receiving-jms-messages/#deserialization","title":"Deserialization","text":"<p>The content of the incoming JMS message is mapped to a Java object.</p> <p>By default it extracts the JMS Message body as a <code>java.lang.Object</code>. This can be changed by setting, in the incoming JMS Message:</p> <ol> <li> <p>The <code>_classname</code> property</p> </li> <li> <p>the <code>JMSType</code></p> </li> </ol> <p>The value must be a fully qualified class name. The connector then load the associated class.</p> <p>Note</p> <p>The connector loads the associated <code>Class</code> using the <code>TCCL</code> and if not found, the classloader used to load the connector.</p> <p>If the target type is a primitive type ort <code>String</code>, the resulting message contains the mapped payload.</p> <p>If the target type is a class, the object is built using included JSON deserializer (JSON-B and Jackson provided OOB from the <code>JMSType</code>. If not, the default behavior is used (Java deserialization).</p>"},{"location":"jms/receiving-jms-messages/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from JMS contains an instance of io.smallrye.reactive.messaging.jms.IncomingJmsMessageMetadata  in the metadata.</p> <pre><code>Optional&lt;IncomingJmsMessageMetadata&gt; metadata = incoming.getMetadata(IncomingJmsMessageMetadata.class);\nmetadata.ifPresent(meta -&gt; {\n    long expiration = meta.getExpiration();\n    Destination destination = meta.getDestination();\n    String value = meta.getStringProperty(\"my-property\");\n});\n</code></pre>"},{"location":"jms/receiving-jms-messages/#acknowledgement","title":"Acknowledgement","text":"<p>When the Reactive Messaging <code>Message</code> gets acknowledged, the associated JMS Message is acknowledged. As JMS acknowledgement is blocking, this acknowledgement is delegated to a worker thread.</p>"},{"location":"jms/receiving-jms-messages/#failure-handling","title":"Failure Handling","text":"<p>If a message produced from a JMS message is nacked, a failure strategy is applied. The JMS connector supports 3 strategies:</p> <ul> <li> <p><code>fail</code> - (default) fail the application, no more messages will be processed. The failing message is not acknowledged and may be redelivered by the JMS broker. The application is marked as unhealthy (impacting liveness checks).</p> </li> <li> <p><code>ignore</code> - the failure is logged, but the processing continues. The failing message is acknowledged and will not be redelivered.</p> </li> <li> <p><code>dead-letter-queue</code> - the failing message is acknowledged and sent to a JMS dead letter queue destination. The processing continues with the next message.</p> <p>The dead letter queue destination can be configured using the <code>dead-letter-queue.destination</code> attribute. If not specified, it defaults to <code>dead-letter-queue-$channel</code>. Messages sent to the dead letter queue preserve the original message body and properties. In addition, the following properties are added:</p> <ul> <li><code>dead_letter_exception_class_name</code> - the fully qualified class name of the exception</li> <li><code>dead_letter_reason</code> - the exception message</li> <li><code>dead_letter_cause_class_name</code> - the fully qualified class name of the root cause (if available)</li> <li><code>dead_letter_cause</code> - the root cause exception message (if available)</li> </ul> </li> </ul>"},{"location":"jms/receiving-jms-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default broadcast Whether or not the JMS message should be dispatched to multiple consumers boolean false <code>false</code> client-id The client id String false connection-factory-name The name of the JMS connection factory  (<code>jakarta.jms.ConnectionFactory</code>) to be used. If not set, it uses any exposed JMS connection factory String false dead-letter-queue.destination When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates on which queue the message is sent. Defaults is <code>dead-letter-topic-$channel</code> string false dead-letter-queue.producer-client-id When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates what client id the generated producer should use. Defaults is <code>jms-dead-letter-topic-producer-$client-id</code> string false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either <code>queue</code> or <code>topic</code> string false <code>queue</code> durable Set to <code>true</code> to use a durable subscription boolean false <code>false</code> failure-strategy Specify the failure strategy to apply when a message produced from a record is acknowledged negatively (nack). Values can be <code>fail</code> (default), <code>ignore</code>, or <code>dead-letter-queue</code> string false <code>fail</code> no-local Enable or disable local delivery boolean false <code>false</code> password The password to connect to to the JMS server String false retry Whether to retry on terminal stream errors. boolean false <code>true</code> retry.initial-delay The initial delay for the retry. string false <code>PT1S</code> retry.jitter How much the delay jitters as a multiplier between 0 and 1. The formula is current delay * jitter. For example, with a current delay of 2H, a jitter of 0.5 will result in an actual delay somewhere between 1H and 3H. double false <code>0.5</code> retry.max-delay The maximum delay string false <code>PT10S</code> retry.max-retries Maximum number of retries for terminal stream errors. int false <code>3</code> reuse-jms-context Whether to reuse JMS contexts by creating child contexts from a parent context. When enabled, child contexts will be created using JMSContext.createContext() instead of ConnectionFactory.createContext() boolean false <code>false</code> selector The JMS selector String false session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false <code>AUTO_ACKNOWLEDGE</code> tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> username The username to connect to to the JMS server String false"},{"location":"jms/sending-jms-messages/","title":"Sending messages to JMS","text":"<p>The JMS Connector can send Reactive Messaging <code>Messages</code> as JMS Message.</p>"},{"location":"jms/sending-jms-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have a <code>jakarta.jms.ConnectionFactory</code> bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector.</p> <p>Configure your application to write the messages from the <code>prices</code> channel into a JMS Message as follows:</p> <pre><code>mp.messaging.outgoing.prices.connector=smallrye-jms\n</code></pre> <p>Note</p> <p>You don\u2019t need to set the destination. By default, it uses the channel name (<code>prices</code>). You can configure the <code>destination</code> attribute to override it.</p> <p>Note</p> <p>By default the connector uses a <code>queue</code>. You can configure it to use a <code>topic</code> by setting <code>destination-type=topic</code>.</p> <p>Then, your application must send <code>Message&lt;Double&gt;</code> to the <code>prices</code> channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package jms.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class JmsPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package jms.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class JmsPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble()));\n    }\n\n}\n</code></pre>"},{"location":"jms/sending-jms-messages/#serialization","title":"Serialization","text":"<p>The connector serializes the incoming message payload into the body of the outgoing JMS Message.</p> <p>If the payload is a <code>String</code> or a primitive type, the payload is encoded as <code>String</code> and the <code>JMSType</code> is set to the target class. The <code>_classname</code> property is also set. The JMS Message is a <code>TextMessage</code>.</p> <p>If the payload is a <code>byte[]</code>, it\u2019s passed as <code>byte[]</code> in a JMS <code>BytesMessage</code>.</p> <p>Otherwise, the payload is encoded using included JSON serializer (JSON-B and Jackson provided OOB. The <code>JMSType</code> is set to the target class. The <code>_classname</code> property is also set. The JMS Message is a <code>TextMessage</code>.</p> <p>For example, the following code serialize the produced <code>Person</code> using JSON-B.</p> <pre><code>@Incoming(\"...\")\n@Outgoing(\"my-channel\")\npublic Person sendToJms(...) {\n  // ...\n  return new Person(\"bob\", 42);\n}\n</code></pre> <p>It requires that the <code>Person</code> class can be serialized to JSON. The classname is passed in the <code>JMSType</code> property and <code>_classname</code> property.</p>"},{"location":"jms/sending-jms-messages/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of OutgoingJmsMessageMetadata to influence how the message is going to be written to JMS.</p> <pre><code>OutgoingJmsMessageMetadata metadata = OutgoingJmsMessageMetadata.builder()\n        .withProperties(JmsProperties.builder().with(\"some-property\", \"some-value\").build())\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre> <p>The metadata allow adding properties but also override the destination.</p>"},{"location":"jms/sending-jms-messages/#acknowledgement","title":"Acknowledgement","text":"<p>Once the JMS message is sent to the JMS server, the message is acknowledged. Sending a JMS message is a blocking operation. So, sending is done on a worker thread.</p>"},{"location":"jms/sending-jms-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default client-id The client id String false connection-factory-name The name of the JMS connection factory  (<code>jakarta.jms.ConnectionFactory</code>) to be used. If not set, it uses any exposed JMS connection factory String false correlation-id The JMS Message correlation id string false delivery-delay The delivery delay long false delivery-mode The delivery mode. Either <code>persistent</code> or <code>non_persistent</code> string false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either <code>queue</code> or <code>topic</code> string false <code>queue</code> disable-message-id Omit the message id in the outbound JMS message boolean false disable-message-timestamp Omit the message timestamp in the outbound JMS message boolean false merge Whether the connector should allow multiple upstreams boolean false <code>false</code> password The password to connect to to the JMS server String false priority The JMS Message priority int false reply-to The reply to destination if any string false reply-to-destination-type The type of destination for the response. It can be either <code>queue</code> or <code>topic</code> string false <code>queue</code> retry Whether to retry on terminal stream errors. boolean false <code>true</code> retry.initial-delay The initial delay for the retry. string false <code>PT1S</code> retry.jitter How much the delay jitters as a multiplier between 0 and 1. The formula is current delay * jitter. For example, with a current delay of 2H, a jitter of 0.5 will result in an actual delay somewhere between 1H and 3H. double false <code>0.5</code> retry.max-delay The maximum delay string false <code>PT10S</code> retry.max-retries Maximum number of retries for terminal stream errors. int false <code>3</code> reuse-jms-context Whether to reuse JMS contexts by creating child contexts from a parent context. When enabled, child contexts will be created using JMSContext.createContext() instead of ConnectionFactory.createContext() boolean false <code>false</code> session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false <code>AUTO_ACKNOWLEDGE</code> tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> ttl The JMS Message time-to-live long false username The username to connect to to the JMS server String false"},{"location":"kafka/avro-configuration/","title":"Using Apache Avro serializer/deserializer","text":"<p>If you are using Apache Avro serializer/deserializer, please note the following configuration properties.</p>"},{"location":"kafka/avro-configuration/#for-confluent-schema-registry","title":"For Confluent Schema Registry","text":"<p>Confluent Avro library is <code>io.confluent:kafka-avro-serializer</code>. Note that this library is not available in Maven Central, you need to use the Confluent Maven repository.</p>"},{"location":"kafka/avro-configuration/#consumer","title":"Consumer","text":"Property Recommended value value.deserializer io.confluent.kafka.serializers.KafkaAvroDeserializer schema.registry.url http://{your_host}:{your_port}/ specific.avro.reader true <p>Example:</p> <pre><code>mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer\nmp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/\nmp.messaging.incoming.[channel].specific.avro.reader=true\n</code></pre>"},{"location":"kafka/avro-configuration/#producer","title":"Producer","text":"Property Recommended value value.serializer io.confluent.kafka.serializers.KafkaAvroSerializer schema.registry.url http://{your_host}:{your_port}/ <p>Example:</p> <pre><code>mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer\nmp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/\n</code></pre>"},{"location":"kafka/avro-configuration/#for-apicurio-registry-1x","title":"For Apicurio Registry 1.x","text":"<p>Apicurio Registry 1.x Avro library is <code>io.apicurio:apicurio-registry-utils-serde</code>.</p> <p>The configuration properties listed here are meant to be used with the Apicurio Registry 1.x client library and Apicurio Registry 1.x server.</p>"},{"location":"kafka/avro-configuration/#consumer_1","title":"Consumer","text":"Property Recommended value value.deserializer io.apicurio.registry.utils.serde.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/api apicurio.registry.avro-datum-provider io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider apicurio.registry.use-specific-avro-reader true <p>Example:</p> <pre><code>mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.utils.serde.AvroKafkaDeserializer\nmp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api\nmp.messaging.incoming.[channel].apicurio.registry.avro-datum-provider=io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider\nmp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true\n</code></pre>"},{"location":"kafka/avro-configuration/#producer_1","title":"Producer","text":"Property Recommended value value.serializer io.apicurio.registry.utils.serde.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/api <p>To automatically register schemas with the registry, add:</p> Property Value apicurio.registry.global-id io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy <p>Example:</p> <pre><code>mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.utils.serde.AvroKafkaSerializer\nmp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api\nmp.messaging.outgoing.[channel].apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy\n</code></pre>"},{"location":"kafka/avro-configuration/#for-apicurio-registry-2x","title":"For Apicurio Registry 2.x","text":"<p>Apicurio Registry 2.x Avro library is <code>io.apicurio:apicurio-registry-serdes-avro-serde</code>.</p> <p>The configuration properties listed here are meant to be used with the Apicurio Registry 2.x client library and Apicurio Registry 2.x server.</p>"},{"location":"kafka/avro-configuration/#consumer_2","title":"Consumer","text":"Property Recommended value value.deserializer io.apicurio.registry.serde.avro.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 apicurio.registry.use-specific-avro-reader true <p>Example:</p> <pre><code>mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.serde.avro.AvroKafkaDeserializer\nmp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2\nmp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true\n</code></pre>"},{"location":"kafka/avro-configuration/#producer_2","title":"Producer","text":"Property Recommended value value.serializer io.apicurio.registry.serde.avro.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 <p>To automatically register schemas with the registry, add:</p> Property Value apicurio.registry.auto-register true <p>Example:</p> <pre><code>mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.serde.avro.AvroKafkaSerializer\nmp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2\nmp.messaging.outgoing.[channel].apicurio.registry.auto-register=true\n</code></pre>"},{"location":"kafka/client-service/","title":"KafkaClientService","text":"<p>For advanced use cases, SmallRye Reactive Messaging provides a bean of type <code>KafkaClientService</code> that you can inject:</p> <pre><code>@Inject\nKafkaClientService kafka;\n</code></pre> <p>From there, you can obtain an <code>io.smallrye.reactive.messaging.kafka.KafkaProducer</code> and an <code>io.smallrye.reactive.messaging.kafka.KafkaConsumer</code>.</p> <p><code>KafkaProducer</code> and <code>KafkaConsumer</code> expose a non-blocking API on top of the Kafka client API. They also mediate access to the threads that SmallRye Reactive Messaging uses to run all Kafka operations: the polling thread, used for consuming records from Kafka topics, and the sending thread, used for producing records to Kafka topics. (Just to be clear: each channel has its own polling thread and sending thread.)</p> <p>The reason why SmallRye Reactive Messaging uses a special thread to run the poll loop should be obvious: the <code>Consumer</code> API is blocking. The <code>Producer</code> API, on the other hand, is documented to be non-blocking. However, in present versions, Kafka doesn\u2019t guarantee that in all cases; see KAFKA-3539 for more details. That is why SmallRye Reactive Messaging uses a dedicated thread to run the send operations as well.</p> <p>Sometimes, SmallRye Reactive Messaging provides direct access to the Kafka <code>Producer</code> or <code>Consumer</code>. For example, a <code>KafkaConsumerRebalanceListener</code> methods are always invoked on the polling thread, so they give you direct access to <code>Consumer</code>. In such case, you should use the <code>Producer</code>/<code>Consumer</code> API directly, instead of the <code>KafkaProducer</code>/<code>KafkaConsumer</code> API.</p>"},{"location":"kafka/consumer-rebalance-listener/","title":"Consumer Rebalance Listener","text":"<p>To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> interface, make the implementing class a bean, and add the <code>@Identifier</code> qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset.</p> <p>The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the <code>partitionsAssigned</code> callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the <code>partitionsRevoked</code> and <code>partitionsAssigned</code> callbacks again, so you can implement custom logic.</p> <p>Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier.</p> <p>When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes.</p> <p>If the rebalance listener handles offset commit on behalf of the user (using the <code>ignore</code> commit strategy), the rebalance listener must commit the offset synchronously in the <code>partitionsRevoked</code> callback. We also recommend applying the same logic when the application stops.</p> <p>Unlike the <code>ConsumerRebalanceListener</code> from Apache Kafka, the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> methods pass the Kafka <code>Consumer</code> and the set of topics/partitions.</p>"},{"location":"kafka/consumer-rebalance-listener/#example","title":"Example","text":"<p>In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> interface and is annotated with <code>@Identifier</code>. We then must configure our inbound connector to use this named bean.</p> <pre><code>package kafka.inbound;\n\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.logging.Logger;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.clients.consumer.Consumer;\nimport org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener;\n\n@ApplicationScoped\n@Identifier(\"rebalanced-example.rebalancer\")\npublic class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener {\n\n    private static final Logger LOGGER = Logger.getLogger(KafkaRebalancedConsumerRebalanceListener.class.getName());\n\n    /**\n     * When receiving a list of partitions will search for the earliest offset within 10 minutes\n     * and seek the consumer to it.\n     *\n     * @param consumer underlying consumer\n     * @param partitions set of assigned topic partitions\n     */\n    @Override\n    public void onPartitionsAssigned(Consumer&lt;?, ?&gt; consumer,\n            Collection&lt;org.apache.kafka.common.TopicPartition&gt; partitions) {\n        long now = System.currentTimeMillis();\n        long shouldStartAt = now - 600_000L; //10 minute ago\n\n        Map&lt;org.apache.kafka.common.TopicPartition, Long&gt; request = new HashMap&lt;&gt;();\n        for (org.apache.kafka.common.TopicPartition partition : partitions) {\n            LOGGER.info(\"Assigned \" + partition);\n            request.put(partition, shouldStartAt);\n        }\n        Map&lt;org.apache.kafka.common.TopicPartition, OffsetAndTimestamp&gt; offsets = consumer\n                .offsetsForTimes(request);\n        for (Map.Entry&lt;org.apache.kafka.common.TopicPartition, OffsetAndTimestamp&gt; position : offsets.entrySet()) {\n            long target = position.getValue() == null ? 0L : position.getValue().offset();\n            LOGGER.info(\"Seeking position \" + target + \" for \" + position.getKey());\n            consumer.seek(position.getKey(), target);\n        }\n    }\n\n}\n</code></pre> <pre><code>package kafka.inbound;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Acknowledgment;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class KafkaRebalancedConsumer {\n\n    @Incoming(\"rebalanced-example\")\n    @Acknowledgment(Acknowledgment.Strategy.NONE)\n    public CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; message) {\n        // We don't need to ACK messages because in this example we set offset during consumer re-balance\n        return CompletableFuture.completedFuture(null);\n    }\n\n}\n</code></pre> <p>To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name:</p> <ul> <li><code>mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer</code></li> </ul> <p>Or have the listener\u2019s name be the same as the group id:</p> <ul> <li><code>mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer</code></li> </ul> <p>Setting the consumer rebalance listener\u2019s name takes precedence over using the group id.</p>"},{"location":"kafka/default-configuration/","title":"Retrieving Kafka default configuration","text":"<p>If your application/runtime exposes as a CDI bean of type <code>Map&lt;String, Object</code> with the identifier <code>default-kafka-broker</code>, this configuration is used to establish the connection with the Kafka broker.</p> <p>For example, you can imagine exposing this map as follows:</p> <pre><code>@Produces\n@ApplicationScoped\n@Identifier(\"default-kafka-broker\")\npublic Map&lt;String, Object&gt; createKafkaRuntimeConfig() {\n    Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;();\n\n    StreamSupport\n        .stream(config.getPropertyNames().spliterator(), false)\n        .map(String::toLowerCase)\n        .filter(name -&gt; name.startsWith(\"kafka\"))\n        .distinct()\n        .sorted()\n        .forEach(name -&gt; {\n            final String key = name.substring(\"kafka\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\", \".\");\n            final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n            properties.put(key, value);\n        });\n\n    return properties;\n}\n</code></pre> <p>This previous example would extract all the configuration keys from MicroProfile Config starting with <code>kafka</code>.</p> <p>Quarkus</p> <p>Starting with Quarkus 1.5, a map corresponding to the previous example is automatically provided.</p> <p>In addition to this default configuration, you can configure the name of the <code>Map</code> producer using the <code>kafka-configuration</code> attribute:</p> <pre><code>mp.messaging.incoming.my-channel.connector=smallrye-kafka\nmp.messaging.incoming.my-channel.kafka-configuration=my-configuration\n</code></pre> <p>In this case, the connector looks for the <code>Map</code> associated with the <code>my-configuration</code> name. If <code>kafka-configuration</code> is not set, an optional lookup for a <code>Map</code> exposed with the channel name (<code>my-channel</code> in the previous example) is done.</p> <p>Important</p> <p>If <code>kafka-configuration</code> is set and no <code>Map</code> can be found, the deployment fails.</p> <p>Attribute values are resolved as follows:</p> <ol> <li> <p>if the attribute is set directly on the channel configuration     (<code>mp.messaging.incoming.my-channel.attribute=value</code>), this value is     used</p> </li> <li> <p>if the attribute is not set on the channel, the connector looks for     a <code>Map</code> with the channel name or the configured     <code>kafka-configuration</code> (if set) and the value is retrieved from that     <code>Map</code></p> </li> <li> <p>If the resolved <code>Map</code> does not contain the value the default <code>Map</code>     is used (exposed with the <code>default-kafka-broker</code> name)</p> </li> </ol>"},{"location":"kafka/health/","title":"Health reporting","text":"<p>The Kafka connector reports the startup, readiness and liveness of each channel managed by the connector.</p> <p>Note</p> <p>To disable health reporting, set the <code>health-enabled</code> attribute for the channel to <code>false</code>.</p>"},{"location":"kafka/health/#startup-readiness","title":"Startup &amp; Readiness","text":""},{"location":"kafka/health/#metrics-based-strategy","title":"Metrics-based strategy","text":"<p>By default, both inbound and outbound channels use underlying Kafka client metrics to check if at least one active connection exists with a broker. This strategy is lightweight and does not require additional remote interactions with the broker.</p>"},{"location":"kafka/health/#client-based-strategy","title":"Client-based strategy","text":"<p>You can also enable another strategy by setting the <code>health-topic-verification-enabled</code> attribute to <code>true</code>. With this second strategy, the health checks use a Kafka Admin Client to access the broker and retrieve the list of existing topics. Retrieving this list can be a lengthy and expensive operation. You can configure a timeout using the <code>health-topic-verification-timeout</code> attribute. The default timeout is set to 2 seconds. Note that if the timeout is reached, the health check fails.</p> <p>Deprecated</p> <p><code>health-readiness-topic-verification</code> and <code>health-readiness-timeout</code> attributes are deprecated and replaced by <code>health-topic-verification-enabled</code> and <code>health-topic-verification-timeout</code>.</p> <p>For startup checks both inbound and outbound side verify that the Kafka topic is created and its partitions are available in the broker. If multiple topics are consumed using the <code>topics</code> attribute, the readiness check verifies that all the consumed topics are available. If you use a pattern (using the <code>pattern</code> attribute), the readiness check verifies that at least one existing topic matches the pattern.</p> <p>For readiness checks inbound channels verify that the underlying consumer is assigned at least a partition to consume. On the outbound side (writing records to Kafka) verify that the broker is still accessible.</p> <p>Note</p> <p>If <code>health-topic-verification-enabled</code> is enabled, both for startup and readiness checks use this strategy. They can be disabled explicitly using <code>health-topic-verification-startup-disabled</code> and <code>health-topic-verification-readiness-disabled</code> flags.</p> <p>To summarize for startup and readiness health checks:</p> <p>Startup</p> Inbound Outbound Metrics-based <code>connection-count</code> metric &gt; 0 or no subscribers to the stream <code>connection-count</code> metric &gt; 0 Client-based Subscribed topic(s) exist in the broker and are available Produced topic exist in the broker and is available <p>Readiness</p> Inbound Outbound Metrics-based <code>connection-count</code> metric &gt; 0 or no subscribers to the stream <code>connection-count</code> metric &gt; 0 Client-based Consumer has at least one partition assignment no subscribers to the stream Cluster is acessible using the Kafka admin API"},{"location":"kafka/health/#liveness","title":"Liveness","text":"<p>On the inbound side (receiving records from Kafka), the liveness check verifies that:</p> <ul> <li> <p>no failures have been caught</p> </li> <li> <p>the client is connected to the broker</p> </li> </ul> <p>On the outbound side (writing records to Kafka), the liveness check verifies that:</p> <ul> <li>no failures have been caught</li> </ul> <p>Note that a message processing failures nacks the message which is then handled by the failure-strategy. It the responsibility of the failure-strategy to report the failure and influence the outcome of the liveness checks. The <code>fail</code> failure strategy reports the failure and so the liveness check will report the failure.</p>"},{"location":"kafka/kafka/","title":"Apache Kafka Connector","text":"<p>The Kafka connector adds support for Kafka to Reactive Messaging. With it you can receive Kafka Records as well as write <code>message</code> into Kafka.</p> <p>Apache Kafka is a popular distributed streaming platform. It lets you:</p> <ul> <li> <p>Publish and subscribe to streams of records, similar to a message     queue or enterprise messaging system.</p> </li> <li> <p>Store streams of records in a fault-tolerant durable way.</p> </li> <li> <p>Process streams of records as they occur.</p> </li> </ul> <p>The Kafka cluster stores streams of records in categories called topics. Each record consists of a key, a value, and a timestamp.</p> <p>For more details about Kafka, check the documentation.</p>"},{"location":"kafka/kafka/#using-the-kafka-connector","title":"Using the Kafka Connector","text":"<p>To use the Kafka Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-kafka&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-kafka</code>.</p> <p>So, to indicate that a channel is managed by this connector you need:</p> <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-kafka\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-kafka\n</code></pre>"},{"location":"kafka/kerberos/","title":"Kerberos authentication","text":"<p>When using Kerberos authentication, you need to configure the connector with:</p> <ul> <li> <p>the security protocol set to <code>SASL_PLAINTEXT</code></p> </li> <li> <p>the SASL mechanism set to <code>GSSAPI</code></p> </li> <li> <p>the Jaas config configured with <code>Krb5LoginModule</code></p> </li> <li> <p>the Kerberos service name</p> </li> </ul> <p>The following snippet provides an example:</p> <pre><code>kafka.bootstrap.servers=ip-192-168-0-207.us-east-2.compute.internal:9094\nkafka.sasl.mechanism=GSSAPI\nkafka.security.protocol=SASL_PLAINTEXT\nkafka.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true refreshKrb5Config=true useKeyTab=true storeKey=true keyTab=\"file:/opt/kafka/krb5/kafka-producer.keytab\" principal=\"kafka-producer/ip-192-168-0-207.us-east-2.compute.internal@INTERNAL\";\nkafka.sasl.kerberos.service.name=kafka\n</code></pre>"},{"location":"kafka/protobuf-configuration/","title":"Using Google Protobuf serializer/deserializer","text":"<p>If you are using Protocol Buffers serializer/deserializer, please note the following configuration properties.</p>"},{"location":"kafka/protobuf-configuration/#for-confluent-schema-registry","title":"For Confluent Schema Registry","text":"<p>Confluent protobuf library is <code>io.confluent:kafka-protobuf-serializer</code>. Note that this library is not available in Maven Central, you need to use the Confluent Maven repository.</p>"},{"location":"kafka/protobuf-configuration/#consumer","title":"Consumer","text":"Property Recommended value value.deserializer io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer schema.registry.url http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.protobuf.value.type your.package.DomainObjectKey$Key mp.messaging.incoming.[channel].specific.protobuf.key.type your.package.DomainObjectValue$Value <p>Example:</p> <pre><code>mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer\nmp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/\nmp.messaging.incoming.[channel].specific.protobuf.value.type=your.package.DomainObjectKey$Key\nmp.messaging.incoming.[channel].specific.protobuf.key.type=your.package.DomainObjectValue$Value\n</code></pre>"},{"location":"kafka/protobuf-configuration/#producer","title":"Producer","text":"Property Recommended value value.serializer io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer schema.registry.url http://{your_host}:{your_port}/ <p>Example:</p> <pre><code>mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer\nmp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/\n</code></pre>"},{"location":"kafka/receiving-kafka-records/","title":"Receiving Kafka Records","text":"<p>The Kafka Connector retrieves Kafka Records from Kafka Brokers and maps each of them to Reactive Messaging <code>Messages</code>.</p>"},{"location":"kafka/receiving-kafka-records/#example","title":"Example","text":"<p>Let\u2019s imagine you have a Kafka broker running, and accessible using the <code>kafka:9092</code> address (by default it would use <code>localhost:9092</code>). Configure your application to receive Kafka records from a Kafka topic on the <code>prices</code> channel as follows:</p> <pre><code>kafka.bootstrap.servers=kafka:9092 # &lt;1&gt;\n\nmp.messaging.incoming.prices.connector=smallrye-kafka # &lt;2&gt;\nmp.messaging.incoming.prices.value.deserializer=org.apache.kafka.common.serialization.DoubleDeserializer # &lt;3&gt;\nmp.messaging.incoming.prices.broadcast=true # &lt;4&gt;\n</code></pre> <ol> <li> <p>Configure the broker location. You can configure it globally or per     channel</p> </li> <li> <p>Configure the connector to manage the <code>prices</code> channel</p> </li> <li> <p>Sets the (Kafka) deserializer to read the record\u2019s value</p> </li> <li> <p>Make sure that we can receive from more than one consumer (see     <code>KafkaPriceConsumer</code> and <code>KafkaPriceMessageConsumer</code> below)</p> </li> </ol> <p>Note</p> <p>You don\u2019t need to set the Kafka topic. By default, it uses the channel name (<code>prices</code>). You can configure the <code>topic</code> attribute to override it.</p> <p>Then, your application receives <code>Message&lt;Double&gt;</code>. You can consume the payload directly:</p> <pre><code>package kafka.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class KafkaPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(double price) {\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;Double&gt;</code>:</p> <pre><code>package kafka.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class KafkaPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;Double&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message (commit the offset)\n        return price.ack();\n    }\n\n}\n</code></pre>"},{"location":"kafka/receiving-kafka-records/#deserialization","title":"Deserialization","text":"<p>The deserialization is handled by the underlying Kafka Client. You need to configure the:</p> <ul> <li> <p><code>mp.messaging.incoming.[channel-name].value.deserializer</code> to     configure the value deserializer (mandatory)</p> </li> <li> <p><code>mp.messaging.incoming.[channel-name].key.deserializer</code> to configure     the key deserializer (optional, default to <code>String</code>)</p> </li> </ul> <p>If you want to use a custom deserializer, add it to your <code>CLASSPATH</code> and configure the associate attribute.</p> <p>In addition, the Kafka Connector also provides a set of message converters. So you can receive payloads representing records from Kafka using:</p> <ul> <li>Record - a pair key/value</li> <li>ConsumerRecord -     a structure representing the record with all its metadata <pre><code>@Incoming(\"topic-a\")\npublic void consume(Record&lt;String, String&gt; record) {\n    String key = record.key(); // Can be `null` if the incoming record has no key\n    String value = record.value(); // Can be `null` if the incoming record has no value\n}\n\n@Incoming(\"topic-b\")\npublic void consume(ConsumerRecord&lt;String, String&gt; record) {\n    String key = record.key(); // Can be `null` if the incoming record has no key\n    String value = record.value(); // Can be `null` if the incoming record has no value\n    String topic = record.topic();\n    int partition = record.partition();\n    // ...\n}\n</code></pre>"},{"location":"kafka/receiving-kafka-records/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from Kafka contains an instance of IncomingKafkaRecordMetadata in the metadata. It provides the key, topic, partitions, headers and so on:</p> <pre><code>IncomingKafkaRecordMetadata&lt;String, Double&gt; metadata = incoming.getMetadata(IncomingKafkaRecordMetadata.class)\n        .orElse(null);\nif (metadata != null) {\n    // The topic\n    String topic = metadata.getTopic();\n\n    // The key\n    String key = metadata.getKey();\n\n    // The timestamp\n    Instant timestamp = metadata.getTimestamp();\n\n    // The underlying record\n    ConsumerRecord&lt;String, Double&gt; record = metadata.getRecord();\n\n    // ...\n}\n</code></pre>"},{"location":"kafka/receiving-kafka-records/#acknowledgement","title":"Acknowledgement","text":"<p>When a message produced from a Kafka record is acknowledged, the connector invokes a commit strategy. These strategies decide when the consumer offset for a specific topic/partition is committed. Committing an offset indicates that all previous records have been processed. It is also the position where the application would restart the processing after a crash recovery or a restart.</p> <p>Committing every offset has performance penalties as Kafka offset management can be slow. However, not committing the offset often enough may lead to message duplication if the application crashes between two commits.</p> <p>The Kafka connector supports three strategies:</p> <ul> <li> <p><code>throttled</code> keeps track of received messages and commit to the next     offset after the latest acked message in sequence. This strategy     guarantees at-least-once delivery even if the channel performs     asynchronous processing. The connector tracks the received records     and periodically (period specified by <code>auto.commit.interval.ms</code>     (default: 5000)) commits the highest consecutive offset. The     connector will be marked as unhealthy if a message associated with a     record is not acknowledged in     <code>throttled.unprocessed-record-max-age.ms</code> (default: 60000). Indeed,     this strategy cannot commit the offset as soon as a single record     processing fails (see failure-strategy to configure what happens on     failing processing). If <code>throttled.unprocessed-record-max-age.ms</code> is     set to less than or equal to 0, it does not perform any health check     verification. Such a setting might lead to running out of memory if     there are poison pill messages. This strategy is the default if     <code>enable.auto.commit</code> is not explicitly set to <code>true</code>.</p> <p>The <code>throttled</code> strategy supports concurrent processing with ordering guarantees using the <code>throttled.ordered</code> configuration. See Concurrent Processing with Ordering Guarantees for more details.</p> </li> <li> <p><code>checkpoint</code> allows persisting consumer offsets on a \"state store\",     instead of committing them back to the Kafka broker. Using the     <code>CheckpointMetadata</code> API, consumer code can persist a processing     state with the offset to mark the progress of a consumer.     When the processing continues from a previously persisted offset,     it seeks the Kafka consumer to that offset and also restores the     persisted state, continuing the stateful processing from where it     left off. The <code>checkpoint</code> strategy holds locally the processing     state associated with the latest offset, and persists it     periodically to the state store (period specified by     <code>auto.commit.interval.ms</code> (default: 5000)). The connector will be     marked as unhealthy if no processing state is persisted to the state     store in <code>checkpoint.unsynced-state-max-age.ms</code> (default: 10000).     Using the <code>CheckpointMetadata</code> API the user code can force to persist     the state on message ack. If <code>checkpoint.unsynced-state-max-age.ms</code>     is set to less than or equal to 0, it does not perform any health     check verification. For more information, see     Stateful processing with Checkpointing</p> </li> <li> <p><code>latest</code> commits the record offset received by the Kafka consumer as     soon as the associated message is acknowledged (if the offset is     higher than the previously committed offset). This strategy provides     at-least-once delivery if the channel processes the message     without performing any asynchronous processing. This strategy should     not be used on high-load as offset commit is expensive. However, it     reduces the risk of duplicates.</p> </li> <li> <p><code>ignore</code> performs no commit. This strategy is the default strategy     when the consumer is explicitly configured with <code>enable.auto.commit</code>     to <code>true</code>. It delegates the offset commit to the Kafka client. When     <code>enable.auto.commit</code> is <code>true</code> this strategy DOES NOT guarantee     at-least-once delivery. However, if the processing failed between     two commits, messages received after the commit and before the     failure will be re-processed.</p> </li> </ul> <p>Important</p> <p>The Kafka connector disables the Kafka auto commit if not explicitly enabled. This behavior differs from the traditional Kafka consumer.</p> <p>If high-throughout is important for you, and not limited by the downstream, we recommend to either:</p> <ul> <li>Use the <code>throttled</code> policy</li> <li>or set <code>enable.auto.commit</code> to <code>true</code> and annotate the consuming     method with <code>@Acknowledgment(Acknowledgment.Strategy.NONE)</code></li> </ul>"},{"location":"kafka/receiving-kafka-records/#concurrent-processing-with-ordering-guarantees","title":"Concurrent Processing with Ordering Guarantees","text":"<p>Experimental</p> <p>Ordered concurrent processing is experimental, and APIs and features are subject to change.</p> <p>The <code>throttled</code> commit strategy keeps track of out-of-order message acknowledgements, allowing concurrent processing while ensuring correct offset commits. You can configure processing order guarantees using the <code>throttled.ordered</code> attribute to control which messages can be processed concurrently using the <code>@Blocking</code> annotation.</p>"},{"location":"kafka/receiving-kafka-records/#processing-order-modes","title":"Processing Order Modes","text":"<ul> <li> <p><code>unordered</code> (default): Messages are processed without any ordering guarantees,   allowing maximum concurrency across all partitions and keys.</p> </li> <li> <p><code>key</code>: Messages with the same key from the same topic-partition are   processed sequentially, while messages with different keys or from different   partitions can be processed concurrently. This ensures per-key ordering within   each partition.</p> </li> <li> <p><code>partition</code>: Messages from the same topic-partition are processed   sequentially, but messages from different partitions can be processed concurrently.   This ensures partition-level ordering while still allowing parallelism across   partitions.</p> </li> </ul>"},{"location":"kafka/receiving-kafka-records/#examples","title":"Examples","text":"<p>When using ordered processing modes, configure the <code>throttled.ordered</code> property and control concurrency with the <code>@Blocking(ordered = false)</code> annotation's worker pool:</p> <pre><code>mp.messaging.incoming.orders.connector=smallrye-kafka\nmp.messaging.incoming.orders.topic=orders\nmp.messaging.incoming.orders.commit-strategy=throttled\nmp.messaging.incoming.orders.throttled.ordered=key\n\n# Configure worker pool concurrency for thread pool size\nsmallrye.messaging.worker.order-pool.max-concurrency=10\n\n# Configure merge concurrency (how many groups can process concurrently)\n# Should typically match or be less than worker pool max-concurrency\nmp.messaging.incoming.orders.throttled.ordered.max-concurrency=10\n</code></pre> <pre><code>@Incoming(\"orders\")\n@Blocking(ordered = false, value = \"order-pool\")\npublic void processOrder(Order order) {\n    // Orders with the same key are processed sequentially\n    // Orders with different keys can be processed concurrently\n}\n</code></pre> <p>In order to allow creating more groups (of keys or partitions) that can be processed concurrently, each group buffers ordered messages to process. The <code>max-queue-size-factor</code> (default: 2) attribute controls the buffer size per group, calculated as <code>max.poll.records * max-queue-size-factor</code>. The <code>throttled.ordered.max-concurrency</code> attribute controls how many groups can be processed concurrently. If not set, it defaults to <code>max.poll.records</code>. When buffers of all groups are full, backpressure is applied to pause consumption.</p>"},{"location":"kafka/receiving-kafka-records/#failure-management","title":"Failure Management","text":"<p>If a message produced from a Kafka record is nacked, a failure strategy is applied. The Kafka connector supports 3 strategies:</p> <ul> <li> <p><code>fail</code> - fail the application, no more records will be processed.     (default) The offset of the record that has not been processed     correctly is not committed.</p> </li> <li> <p><code>ignore</code> - the failure is logged, but the processing continue. The     offset of the record that has not been processed correctly is     committed.</p> </li> <li> <p><code>dead-letter-queue</code> - the offset of the record that has not been     processed correctly is committed, but the record is written to a     (Kafka) dead letter queue topic.</p> </li> <li> <p><code>delayed-retry-topic</code> - the offset of the record that has not been     processed correctly is still committed, but the record is written to     a series of Kafka topics for retrying the processing with some delay.     This allows retrying failed records by reconsuming them later without     blocking the processing of the latest records.</p> </li> </ul> <p>The strategy is selected using the <code>failure-strategy</code> attribute.</p>"},{"location":"kafka/receiving-kafka-records/#dead-letter-queue","title":"Dead Letter Queue","text":"<p>In the case of <code>dead-letter-queue</code>, you can configure the following attributes:</p> <ul> <li><code>dead-letter-queue.topic</code>: the topic to use to write the records not     processed correctly, default is <code>dead-letter-topic-$channel</code>, with     <code>$channel</code> being the name of the channel.</li> <li> <p><code>dead-letter-queue.producer-client-id</code>: the client id used by the kafka producer when sending records to dead letter queue topic. If not specified it will default to <code>kafka-dead-letter-topic-producer-$client-id</code>, with $client-id being the value obtained from consumer client id.</p> </li> <li> <p><code>dead-letter-queue.key.serializer</code>: the serializer used to write the     record key on the dead letter queue. By default, it deduces the     serializer from the key deserializer.</p> </li> <li> <p><code>dead-letter-queue.value.serializer</code>: the serializer used to write     the record value on the dead letter queue. By default, it deduces     the serializer from the value deserializer.</p> </li> </ul> <p>The record written on the dead letter topic contains the original record\u2019s headers, as well as a set of additional headers about the original record:</p> <ul> <li> <p><code>dead-letter-reason</code> - the reason of the failure (the <code>Throwable</code>     passed to <code>nack()</code>)</p> </li> <li> <p><code>dead-letter-cause</code> - the cause of the failure (the <code>getCause()</code> of     the <code>Throwable</code> passed to <code>nack()</code>), if any</p> </li> <li> <p><code>dead-letter-topic</code> - the original topic of the record</p> </li> <li> <p><code>dead-letter-partition</code> - the original partition of the record     (integer mapped to String)</p> </li> <li> <p><code>dead-letter-offset</code> - the original offset of the record (long     mapped to String)</p> </li> </ul> <p>When using <code>dead-letter-queue</code>, it is also possible to change some metadata of the record that is sent to the dead letter topic. To do that, use the <code>Message.nack(Throwable, Metadata)</code> method:</p> <pre><code>@Incoming(\"in\")\npublic CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; message) {\n    return message.nack(new Exception(\"Failed!\"), Metadata.of(\n            OutgoingKafkaRecordMetadata.builder()\n                    .withKey(\"failed-record\")\n                    .withHeaders(new RecordHeaders()\n                            .add(\"my-header\", \"my-header-value\".getBytes(StandardCharsets.UTF_8)))\n                    .build()));\n}\n</code></pre> <p>The <code>Metadata</code> may contain an instance of <code>OutgoingKafkaRecordMetadata</code>. If the instance is present, the following properties will be used:</p> <ul> <li> <p>key; if not present, the original record\u2019s key will be used</p> </li> <li> <p>topic; if not present, the configured dead letter topic will be used</p> </li> <li> <p>partition; if not present, partition will be assigned automatically</p> </li> <li> <p>headers; combined with the original record\u2019s headers, as well as the     <code>dead-letter-*</code> headers described above</p> </li> </ul>"},{"location":"kafka/receiving-kafka-records/#delayed-retry-topic","title":"Delayed Retry Topic","text":"<p>Experimental</p> <p>Delayed retry topic feature is experimental.</p> <p>The delayed retry topic strategy allows failed records to be automatically retried by forwarding them to a series of retry topics. Each retry topic is associated with a specific delay time, which is expressed in milliseconds. When a record processing fails, it is forwarded to the first retry topic. The failure strategy then consumes these records and dispatches them to be retried again once the delay time of the topic has elapsed.</p> <p>If the processing of a record fails again, the message is forwarded to the next topic in the list, with possibly a longer delay time. If the processing of a record keeps failing, it will eventually be abandoned. Alternatively, if the <code>dead-letter-queue.topic</code> property is configured, the record will be sent to the dead letter queue.</p> <p>The Kafka producer client used when forwarding records to retry topics can be configured using the dead-letter-queue properties namely, <code>dead-letter-queue.producer-client-id</code>, <code>dead-letter-queue.key.serializer</code> and <code>dead-letter-queue.value.serializer</code>.</p> <p>Delayed retry topics and delays can be configured with following attributes:</p> <ul> <li> <p><code>delayed-retry-topic.topics</code> : The comma-separated list of retry topics, each one suffixed with <code>_[DELAY_IN_MILLISECONDS]</code> for indicating the delay time. For example, <code>my_retry_topic_2000,my_retry_topic_4000,my_retry_topic_10000</code> will use three topics my_retry_topic_2000, my_retry_topic_4000 and my_retry_topic_10000, with 2000ms 4000ms and 10000ms respectively.</p> <p>If not configured the source channel name is used, with 10, 20 and 50 seconds of delay, ex. for a channel named <code>source</code>, retry topics will be <code>source_retry_10000</code>, <code>source_retry_20000</code>, <code>source_retry_50000</code>.</p> </li> <li> <p><code>delayed-retry-topic.max-retries</code> : The maximum number of retries before abandoning the retries. If configured higher than the number of retry topics the last topic is used until maximum number of retries is reached. This can be configured to use a single retry topic with a fixed delay and multiple retries.</p> <p>For example, <code>delayed-retry-topic.topics=source_retry_10000</code> and <code>delayed-retry-topic.max-retries=4</code> will forward failed records to the topic source_retry_10000 with maximum of 4 retries.</p> </li> <li> <p><code>delayed-retry-topic.timeout</code> : The global timeout in milliseconds for a retried record. The timeout is calculated from the first failure for a record. If the next retry will reach the timeout, instead of forwarding to the retry topic the retry is abandoned and, if configured, the record is forwarded to the dead letter queue.</p> <p>The default is 120 seconds.</p> </li> </ul> <p>Important</p> <p>While you can use Smallrye Fault Tolerance to retry processing, it will block the processing of further messages until the retried record is processed successfully, or abandoned.</p> <p>Delayed retry topic failure strategy allows effectively implementing non-blocking retries. But it will not preserve the order of messages inside a topic-partition.</p> <p>The record written on the delayed retry topics will preserve the key and partition of the original record. It also contains the original record\u2019s headers, as well as a set of additional headers about the original record:</p> <ul> <li><code>delayed-retry-count</code> the current number of retries</li> <li><code>delayed-retry-original-timestamp</code> the original timestamp of the record</li> <li><code>delayed-retry-first-processing-timestamp</code> the first processing timestamp of the record</li> <li><code>delayed-retry-reason</code> the reason of the failure (the <code>Throwable</code> passed to <code>nack()</code>)</li> <li><code>delayed-retry-cause</code> the cause of the failure (the <code>getCause()</code> of the <code>Throwable</code> passed to <code>nack()</code>), if any</li> <li><code>delayed-retry-topic</code> the original topic of the record</li> <li><code>delayed-retry-partition</code> the original partition of the record</li> <li><code>delayed-retry-offset</code> the original offset of the record</li> <li><code>delayed-retry-exception-class-name</code> the class name of the throwable passed to <code>nack()</code></li> <li><code>delayed-retry-cause-class-name</code> the class name of the the <code>getCause()</code> of the <code>Throwable</code> passed to <code>nack()</code>, if any</li> </ul> <p>As for the dead letter queue it is possible to change forwarded values by providing a <code>OutgoingKafkaRecordMetadata</code> when the message is nacked using <code>Message.nack(Throwable, Metadata)</code>.</p> <p>Multiple partitions</p> <p>The delayed retry topic strategy does not create retry topics automatically. If the source topic has multiple partitions, delayed retry and dead letter queue topics would need to be setup with the same number of partitions.</p> <p>It is possible to scale consumer application instances according to the number of partitions. But it is not guaranteed that the retry topics consumer will be assigned the same partition(s) as the main topic consumer. Therefore, retry processing of a record can happen in an other instance.</p>"},{"location":"kafka/receiving-kafka-records/#custom-commit-and-failure-strategies","title":"Custom commit and failure strategies","text":"<p>In addition to provided strategies, it is possible to implement custom commit and failure strategies and configure Kafka channels with them.</p> <p>For example, for a custom commit strategy, implement the KafkaCommitHandler interface, and provide a managed bean implementing the <code>KafkaCommitHandler.Factory</code> interface, identified using <code>@Identifier</code> qualifier.</p> <pre><code>package kafka.inbound;\n\nimport java.util.Collection;\nimport java.util.function.BiConsumer;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.common.TopicPartition;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord;\nimport io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration;\nimport io.smallrye.reactive.messaging.kafka.KafkaConsumer;\nimport io.smallrye.reactive.messaging.kafka.commit.KafkaCommitHandler;\nimport io.vertx.mutiny.core.Vertx;\n\npublic class KafkaCustomCommit implements KafkaCommitHandler {\n\n    @Override\n    public &lt;K, V&gt; Uni&lt;Void&gt; handle(IncomingKafkaRecord&lt;K, V&gt; record) {\n        // called on message ack\n        return Uni.createFrom().voidItem();\n    }\n\n    @Override\n    public &lt;K, V&gt; Uni&lt;IncomingKafkaRecord&lt;K, V&gt;&gt; received(IncomingKafkaRecord&lt;K, V&gt; record) {\n        // called before message processing\n        return Uni.createFrom().item(record);\n    }\n\n    @Override\n    public void terminate(boolean graceful) {\n        // called on channel shutdown\n    }\n\n    @Override\n    public void partitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {\n        // called on partitions assignment\n    }\n\n    @Override\n    public void partitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {\n        // called on partitions revoked\n    }\n\n    @ApplicationScoped\n    @Identifier(\"custom\")\n    public static class Factory implements KafkaCommitHandler.Factory {\n\n        @Override\n        public KafkaCommitHandler create(KafkaConnectorIncomingConfiguration config,\n                Vertx vertx,\n                KafkaConsumer&lt;?, ?&gt; consumer,\n                BiConsumer&lt;Throwable, Boolean&gt; reportFailure) {\n            return new KafkaCustomCommit(/* ... */);\n        }\n    }\n\n}\n</code></pre> <p>Finally, to use the custom commit strategy, set the <code>commit-strategy</code> attribute to the identifier of the commit handler factory: <code>mp.messaging.incoming.$channel.commit-strategy=custom</code>. Similarly, custom failure strategies can be configured using <code>failure-strategy</code> attribute.</p> <p>Note</p> <p>If the custom strategy implementation inherits ContextHolder class it can access the Vert.x event-loop context created for the Kafka consumer</p>"},{"location":"kafka/receiving-kafka-records/#retrying-processing","title":"Retrying processing","text":"<p>You can combine Reactive Messaging with SmallRye Fault Tolerance, and retry processing when it fails:</p> <pre><code>@Incoming(\"kafka\")\n@Outgoing(\"processed\")\n@Retry(delay = 10, maxRetries = 5)\npublic String process(String v) {\n   // ... retry if this method throws an exception\n}\n</code></pre> <p>You can configure the delay, the number of retries, the jitter...</p> <p>If your method returns a <code>Uni</code>, you need to add the <code>@NonBlocking</code> annotation:</p> <pre><code>@Incoming(\"kafka\")\n@Outgoing(\"processed\")\n@Retry(delay = 10, maxRetries = 5)\n@NonBlocking\npublic Uni&lt;String&gt; process(String v) {\n   // ... retry if this method throws an exception or the returned Uni produce a failure\n}\n</code></pre> <p>The incoming messages are acknowledged only once the processing completes successfully. So, it commits the offset after the successful processing. If after the retries the processing still failed, the message is nacked and the failure strategy is applied.</p> <p>You can also use <code>@Retry</code> on methods only consuming incoming messages:</p> <pre><code>@Incoming(\"kafka\")\n@Retry(delay = 10, maxRetries = 5)\npublic void consume(String v) {\n   // ... retry if this method throws an exception\n}\n</code></pre>"},{"location":"kafka/receiving-kafka-records/#handling-deserialization-failures","title":"Handling deserialization failures","text":"<p>Because deserialization happens before creating a <code>Message</code>, the failure strategy presented above cannot be applied. However, when a deserialization failure occurs, you can intercept it and provide a fallback value. To achieve this, create a CDI bean implementing the DeserializationFailureHandler interface:</p> <pre><code>@ApplicationScoped\n@Identifier(\"failure-retry\") // Set the name of the failure handler\npublic class MyDeserializationFailureHandler\n    implements DeserializationFailureHandler&lt;JsonObject&gt; { // Specify the expected type\n\n    @Override\n    public JsonObject decorateDeserialization(Uni&lt;JsonObject&gt; deserialization,\n            String topic, boolean isKey, String deserializer, byte[] data,\n            Headers headers) {\n        return deserialization\n                    .onFailure().retry().atMost(3)\n                    .await().atMost(Duration.ofMillis(200));\n    }\n}\n</code></pre> <p>The bean must be exposed with the <code>@Identifier</code> qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute:</p> <ul> <li> <p><code>mp.messaging.incoming.$channel.key-deserialization-failure-handler</code>:     name of the bean handling deserialization failures happening for the     record\u2019s key</p> </li> <li> <p><code>mp.messaging.incoming.$channel.value-deserialization-failure-handler</code>:     name of the bean handling deserialization failures happening for the     record\u2019s value,</p> </li> </ul> <p>The handler is called with the deserialization action as a <code>Uni&lt;T&gt;</code>, the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers augmented with headers describing the failure (which ease the write to a dead letter). On the deserialization <code>Uni</code> failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the deserialized object. Alternatively, the handler can only implement <code>handleDeserializationFailure</code> method and provide a fallback value, which may be <code>null</code>.</p> <p>If you don\u2019t configure a deserialization failure handlers and a deserialization failure happens, the application is marked unhealthy. You can also ignore the failure, which will log the exception and produce a <code>null</code> value. To enable this behavior, set the <code>mp.messaging.incoming.$channel.fail-on-deserialization-failure</code> attribute to <code>false</code>.</p> <p>If the <code>fail-on-deserialization-failure</code> attribute is set to <code>false</code> and the <code>failure-strategy</code> attribute is <code>dead-letter-queue</code> the failed record will be sent to the corresponding dead letter queue topic. The forwarded record will have the original key and value, and the following headers set:</p> <ul> <li><code>deserialization-failure-reason</code>: The deserialization failure message</li> <li><code>deserialization-failure-cause</code>: The deserialization failure cause if any</li> <li><code>deserialization-failure-key</code>: Whether the deserialization failure happened on a key</li> <li><code>deserialization-failure-topic</code>: The topic of the incoming message when a deserialization failure happen</li> <li><code>deserialization-failure-deserializer</code>: The class name of the underlying deserializer</li> <li><code>deserialization-failure-key-data</code>: If applicable the key data that was not able to be deserialized</li> <li><code>deserialization-failure-value-data</code>: If applicable the value data that was not able to be deserialized</li> </ul>"},{"location":"kafka/receiving-kafka-records/#receiving-cloud-events","title":"Receiving Cloud Events","text":"<p>The Kafka connector supports Cloud Events. When the connector detects a structured or binary Cloud Events, it adds a IncomingKafkaCloudEventMetadata in the metadata of the Message. <code>IncomingKafkaCloudEventMetadata</code> contains the various (mandatory and optional) Cloud Event attributes.</p> <p>If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.</p>"},{"location":"kafka/receiving-kafka-records/#binary-cloud-events","title":"Binary Cloud Events","text":"<p>For <code>binary</code> Cloud Events, all mandatory Cloud Event attributes must be set in the record header, prefixed by <code>ce_</code> (as mandated by the protocol binding). The connector considers headers starting with the <code>ce_</code> prefix but not listed in the specification as extensions. You can access them using the <code>getExtension</code> method from <code>IncomingKafkaCloudEventMetadata</code>. You can retrieve them as <code>String</code>.</p> <p>The <code>datacontenttype</code> attribute is mapped to the <code>content-type</code> header of the record. The <code>partitionkey</code> attribute is mapped to the record\u2019s key, if any.</p> <p>Note that all headers are read as UTF-8.</p> <p>With binary Cloud Events, the record\u2019s key and value can use any deserializer.</p>"},{"location":"kafka/receiving-kafka-records/#structured-cloud-events","title":"Structured Cloud Events","text":"<p>For <code>structured</code> Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value.</p> <p>Structured Cloud Event must set the <code>content-type</code> header of the record to <code>application/cloudevents</code> or prefix the value with <code>application/cloudevents</code> such as: <code>application/cloudevents+json; charset=UTF-8</code>.</p> <p>To receive structured Cloud Events, your value deserializer must be:</p> <ul> <li> <p><code>org.apache.kafka.common.serialization.StringDeserializer</code></p> </li> <li> <p><code>org.apache.kafka.common.serialization.ByteArrayDeserializer</code></p> </li> <li> <p><code>io.vertx.kafka.client.serialization.JsonObjectDeserializer</code></p> </li> </ul> <p>As mentioned previously, the value must be a valid JSON object containing at least all the mandatory Cloud Events attributes.</p> <p>If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event <code>data</code>.</p> <p>The <code>partitionkey</code> attribute is mapped to the record\u2019s key if any.</p>"},{"location":"kafka/receiving-kafka-records/#consumer-rebalance-listener","title":"Consumer Rebalance Listener","text":"<p>To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> interface, make the implementing class a bean, and add the <code>@Identifier</code> qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset.</p> <p>The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the <code>partitionsAssigned</code> callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the <code>partitionsRevoked</code> and <code>partitionsAssigned</code> callbacks again, so you can implement custom logic.</p> <p>Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier.</p> <p>When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes.</p> <p>If the rebalance listener handles offset commit on behalf of the user (using the <code>ignore</code> commit strategy), the rebalance listener must commit the offset synchronously in the <code>partitionsRevoked</code> callback. We also recommend applying the same logic when the application stops.</p> <p>Unlike the <code>ConsumerRebalanceListener</code> from Apache Kafka, the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> methods pass the Kafka <code>Consumer</code> and the set of topics/partitions.</p>"},{"location":"kafka/receiving-kafka-records/#example_1","title":"Example","text":"<p>In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code> interface and is annotated with <code>@Identifier</code>. We then must configure our inbound connector to use this named bean.</p> <pre><code>package kafka.inbound;\n\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.logging.Logger;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.clients.consumer.Consumer;\nimport org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener;\n\n@ApplicationScoped\n@Identifier(\"rebalanced-example.rebalancer\")\npublic class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener {\n\n    private static final Logger LOGGER = Logger.getLogger(KafkaRebalancedConsumerRebalanceListener.class.getName());\n\n    /**\n     * When receiving a list of partitions will search for the earliest offset within 10 minutes\n     * and seek the consumer to it.\n     *\n     * @param consumer underlying consumer\n     * @param partitions set of assigned topic partitions\n     */\n    @Override\n    public void onPartitionsAssigned(Consumer&lt;?, ?&gt; consumer,\n            Collection&lt;org.apache.kafka.common.TopicPartition&gt; partitions) {\n        long now = System.currentTimeMillis();\n        long shouldStartAt = now - 600_000L; //10 minute ago\n\n        Map&lt;org.apache.kafka.common.TopicPartition, Long&gt; request = new HashMap&lt;&gt;();\n        for (org.apache.kafka.common.TopicPartition partition : partitions) {\n            LOGGER.info(\"Assigned \" + partition);\n            request.put(partition, shouldStartAt);\n        }\n        Map&lt;org.apache.kafka.common.TopicPartition, OffsetAndTimestamp&gt; offsets = consumer\n                .offsetsForTimes(request);\n        for (Map.Entry&lt;org.apache.kafka.common.TopicPartition, OffsetAndTimestamp&gt; position : offsets.entrySet()) {\n            long target = position.getValue() == null ? 0L : position.getValue().offset();\n            LOGGER.info(\"Seeking position \" + target + \" for \" + position.getKey());\n            consumer.seek(position.getKey(), target);\n        }\n    }\n\n}\n</code></pre> <pre><code>package kafka.inbound;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Acknowledgment;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class KafkaRebalancedConsumer {\n\n    @Incoming(\"rebalanced-example\")\n    @Acknowledgment(Acknowledgment.Strategy.NONE)\n    public CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; message) {\n        // We don't need to ACK messages because in this example we set offset during consumer re-balance\n        return CompletableFuture.completedFuture(null);\n    }\n\n}\n</code></pre> <p>To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name:</p> <ul> <li><code>mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer</code></li> </ul> <p>Or have the listener\u2019s name be the same as the group id:</p> <ul> <li><code>mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer</code></li> </ul> <p>Setting the consumer rebalance listener\u2019s name takes precedence over using the group id.</p>"},{"location":"kafka/receiving-kafka-records/#receiving-kafka-records-in-batches","title":"Receiving Kafka Records in Batches","text":"<p>By default, incoming methods receive each Kafka record individually. Under the hood, Kafka consumer clients poll the broker constantly and receive records in batches, presented inside the <code>ConsumerRecords</code> container.</p> <p>In batch mode, your application can receive all the records returned by the consumer poll in one go.</p> <p>To achieve this you need to set <code>mp.messaging.incoming.$channel.batch=true</code> and specify a compatible container type to receive all the data:</p> <pre><code>@Incoming(\"prices\")\npublic void consume(List&lt;Double&gt; prices) {\n    for (double price : prices) {\n        // process price\n    }\n}\n</code></pre> <p>The incoming method can also receive <code>Message&lt;List&lt;Payload&gt;</code> or <code>ConsumerRecords&lt;Key, Payload&gt;</code> types, They give access to record details such as offset or timestamp :</p> <pre><code>@Incoming(\"prices\")\npublic CompletionStage&lt;Void&gt; consumeMessage(Message&lt;List&lt;Double&gt;&gt; msg) {\n    IncomingKafkaRecordBatchMetadata&lt;String, Double&gt; batchMetadata = msg.getMetadata(IncomingKafkaRecordBatchMetadata.class)\n            .get();\n    for (ConsumerRecord&lt;String, Double&gt; record : batchMetadata.getRecords()) {\n        int partition = record.partition();\n        long offset = record.offset();\n        long timestamp = record.timestamp();\n    }\n    // ack will commit the latest offsets (per partition) of the batch.\n    return msg.ack();\n}\n\n@Incoming(\"prices\")\npublic void consumeRecords(ConsumerRecords&lt;String, Double&gt; records) {\n    for (TopicPartition partition : records.partitions()) {\n        for (ConsumerRecord&lt;String, Double&gt; record : records.records(partition)) {\n            //... process messages\n        }\n    }\n}\n</code></pre> <p>Note that the successful processing of the incoming record batch will commit the latest offsets for each partition received inside the batch. The configured commit strategy will be applied for these records only.</p> <p>Conversely, if the processing throws an exception, all messages are nacked, applying the failure strategy for all the records inside the batch.</p>"},{"location":"kafka/receiving-kafka-records/#accessing-metadata-of-batch-records","title":"Accessing metadata of batch records","text":"<p>When receiving records in batch mode, the metadata of each record is accessible through the <code>IncomingKafkaRecordBatchMetadata</code> :</p> <pre><code>@Incoming(\"prices\")\npublic void consumeRecords(ConsumerRecords&lt;String, Double&gt; records,\n        IncomingKafkaRecordBatchMetadata&lt;String, Double&gt; metadata) {\n    for (TopicPartition partition : records.partitions()) {\n        for (ConsumerRecord&lt;String, Double&gt; record : records.records(partition)) {\n            TracingMetadata tracing = metadata.getMetadataForRecord(record, TracingMetadata.class);\n            if (tracing != null) {\n                tracing.getCurrentContext().makeCurrent();\n            }\n            //... process messages\n        }\n    }\n}\n</code></pre> <p>Like in this example, this can be useful to propagate the tracing information of each record.</p>"},{"location":"kafka/receiving-kafka-records/#manual-topic-partition-assignment","title":"Manual topic-partition assignment","text":"<p>The default behavior of Kafka incoming channels is to subscribe to one or more topics in order to receive records from the Kafka broker. Channel attributes <code>topic</code> and <code>topics</code> allow specifying topics to subscribe to, or <code>pattern</code> attribute allows to subscribe to all topics matching a regular expression. Subscribing to topics allows partitioning consumption of topics by dynamically assigning (rebalancing) partitions between members of a consumer group.</p> <p>The <code>assign-seek</code> configuration attribute allows manually assigning topic-partitions to a Kafka incoming channel, and optionally seek to a specified offset in the partition to start consuming records. If <code>assign-seek</code> is used, the consumer will not be dynamically subscribed to topics, but instead will statically assign the described partitions. In manual topic-partition rebalancing doesn't happen and therefore rebalance listeners are never called.</p> <p>The attribute takes a list of triplets separated by commas: <code>&lt;topic&gt;:&lt;partition&gt;:&lt;offset&gt;</code>.</p> <p>For example, the following configuration</p> <pre><code>mp.messaging.incoming.data.assign-seek=topic1:0:10, topic2:1:20\n</code></pre> <p>assigns the consumer to: - Partition 0 of topic 'topic1', setting the initial position at offset 10. - Partition 1 of topic 'topic2', setting the initial position at offset 20.</p> <p>The topic, partition, and offset in each triplet can have the following variations: - If the topic is omitted, the configured <code>topic</code> will be used. - If the offset is omitted, partitions are assigned to the consumer but won't be seeked to offset. - If offset is 0, it seeks to the beginning of the topic-partition. - If offset is -1, it seeks to the end of the topic-partition.</p>"},{"location":"kafka/receiving-kafka-records/#stateful-processing-with-checkpointing","title":"Stateful processing with Checkpointing","text":"<p>Experimental</p> <p>Checkpointing is experimental, and APIs and features are subject to change in the future.</p> <p>The <code>checkpoint</code> commit strategy allows for a Kafka incoming channel to manage topic-partition offsets, not by committing on the Kafka broker, but by persisting consumers' advancement on a state store.</p> <p>In addition to that, if the consumer builds an internal state as a result of consumed records, the topic-partition offset persisted to the state store can be associated with a processing state, saving the local state to the persistent store. When a consumer restarts or consumer group instances scale, i.e. when new partitions get assigned to the consumer, the checkpointing works by resuming the processing from the latest offset and its saved state.</p> <p>The <code>@Incoming</code> channel consumer code can manipulate the processing state through the <code>CheckpointMetadata</code> API:</p> <pre><code>@Incoming(\"prices\")\npublic CompletionStage&lt;Void&gt; consume(Message&lt;Double&gt; record) {\n    // Get the `CheckpointMetadata` from the incoming message\n    CheckpointMetadata&lt;Double&gt; checkpoint = CheckpointMetadata.fromMessage(record);\n\n    // `CheckpointMetadata` allows transforming the processing state\n    // Applies the given function, starting from the value `0.0` when no previous state exists\n    checkpoint.transform(0.0, current -&gt; current + record.getPayload(), /* persistOnAck */ true);\n\n    // `persistOnAck` flag set to true, ack will persist the processing state\n    // associated with the latest offset (per partition).\n    return record.ack();\n}\n</code></pre> <p>The <code>transform</code> method allows applying a transformation function to the current state, producing a changed state and registering it locally for checkpointing. By default, the local state is synced (persisted) to the state store periodically, period specified by <code>auto.commit.interval.ms</code>, (default: 5000). If <code>persistOnAck</code> flag is given, the latest state is persisted to the state store eagerly on message acknowledgment. The <code>setNext</code> method works similarly directly setting the latest state.</p> <p>The <code>checkpoint</code> commit strategy tracks when a processing state is last persisted for each topic-partition. If an outstanding state change can not be persisted for <code>checkpoint.unsynced-state-max-age.ms</code> (default: 10000), the channel is marked unhealthy.</p> <p>Where and how processing states are persisted is decided by the state store implementation. This can be configured on the incoming channel using <code>checkpoint.state-store</code> configuration property, using the state store factory identifier name. The serialization of state objects depends on the state store implementation. In order to instruct state stores for serialization can require configuring the type name of state objects using <code>checkpoint.state-type</code> property.</p> <p>In order to keep Smallrye Reactive Messaging free of persistence-related dependencies, this library includes only a default state store named <code>file</code>. It is based on Vert.x Filesystem API and stores the processing state in Json formatted files, in a local directory configured by the <code>checkpoint.file.state-dir</code> property. State files follow the naming scheme <code>[consumer-group-id]:[topic]:[partition]</code>.</p>"},{"location":"kafka/receiving-kafka-records/#implementing-state-stores","title":"Implementing State Stores","text":"<p>State store implementations are required to implement <code>CheckpointStateStore</code> interface, and provide a managed bean implementing <code>CheckpointStateStore.Factory</code>, identified with <code>@Identifier</code> bean qualifier indicating the name of the state-store. The factory bean identifier indicates the name to configure on <code>checkpoint.state-store</code> config property. The factory is discovered as a CDI managed bean and state store is created once per channel:</p> <pre><code>package kafka.inbound;\n\nimport java.util.Collection;\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.TopicPartition;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration;\nimport io.smallrye.reactive.messaging.kafka.KafkaConsumer;\nimport io.smallrye.reactive.messaging.kafka.commit.CheckpointStateStore;\nimport io.smallrye.reactive.messaging.kafka.commit.ProcessingState;\nimport io.vertx.mutiny.core.Vertx;\n\npublic class MyCheckpointStateStore implements CheckpointStateStore {\n\n    private final String consumerGroupId;\n    private final Class&lt;?&gt; stateType;\n\n    public MyCheckpointStateStore(String consumerGroupId, Class&lt;?&gt; stateType) {\n        this.consumerGroupId = consumerGroupId;\n        this.stateType = stateType;\n    }\n\n    /**\n     * Can be used with\n     * {@code mp.reactive.messaging.incoming.my-channel.commit-strategy=checkpoint}\n     * {@code mp.reactive.messaging.incoming.my-channel.checkpoint.state-store=my-store}\n     */\n    @ApplicationScoped\n    @Identifier(\"my-store\")\n    public static class Factory implements CheckpointStateStore.Factory {\n\n        @Override\n        public CheckpointStateStore create(KafkaConnectorIncomingConfiguration config,\n                Vertx vertx,\n                KafkaConsumer&lt;?, ?&gt; consumer,\n                Class&lt;?&gt; stateType /* if configured, otherwise null */) {\n            String consumerGroupId = (String) consumer.configuration().get(ConsumerConfig.GROUP_ID_CONFIG);\n            return new MyCheckpointStateStore(consumerGroupId, stateType);\n        }\n    }\n\n    @Override\n    public Uni&lt;Map&lt;TopicPartition, ProcessingState&lt;?&gt;&gt;&gt; fetchProcessingState(Collection&lt;TopicPartition&gt; partitions) {\n        // Called on Vert.x event loop\n        // Return a Uni completing with the map of topic-partition to processing state\n        // The Uni will be subscribed also on Vert.x event loop\n        return Uni.createFrom().nullItem();\n    }\n\n    @Override\n    public Uni&lt;Void&gt; persistProcessingState(Map&lt;TopicPartition, ProcessingState&lt;?&gt;&gt; state) {\n        // Called on Vert.x event loop\n        // Return a Uni completing with void when the given states are persisted\n        // The Uni will be subscribed also on Vert.x event loop\n        return Uni.createFrom().voidItem();\n    }\n\n    @Override\n    public void close() {\n        /* Called when channel is closing, no-op by default */\n    }\n}\n</code></pre> <p>The checkpoint commit strategy calls the state store in following events:</p> <ul> <li><code>fetchProcessingState</code> : on partitions assigned, to seek the consumer to the latest offset.</li> <li><code>persistProcessingState</code> on partitions revoked, to persist the state of last processed record.</li> <li><code>persistProcessingState</code> on message acknowledgement, if a new state is set during the processing and <code>persistOnAck</code> flag is set.</li> <li><code>persistProcessingState</code> on <code>auto.commit.interval.ms</code> intervals, if a new state is set during processing.</li> <li><code>persistProcessingState</code> on channel shutdown.</li> <li><code>close</code> on channel shutdown.</li> </ul>"},{"location":"kafka/receiving-kafka-records/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default assign-seek Assign partitions and optionally seek to offsets, instead of subscribing to topics. A comma-separating list of triplets in form of <code>&lt;topic&gt;:|&lt;partition&gt;|:&lt;offset&gt;</code> to assign statically to the consumer and seek to the given offsets. Offset <code>0</code> seeks to beginning and offset <code>-1</code> seeks to the end of the topic-partition. If the topic is omitted the configured topic will be used. If the offset is omitted partitions are assigned to the consumer but won't be seeked to offset. string false auto.offset.reset What to do when there is no initial offset in Kafka.Accepted values are earliest, latest and none string false <code>latest</code> batch Whether the Kafka records are consumed in batch. The channel injection point must consume a compatible type, such as <code>List&lt;Payload&gt;</code> or <code>KafkaRecordBatch&lt;Payload&gt;</code>. boolean false <code>false</code> bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false <code>localhost:9092</code> broadcast Whether the Kafka records should be dispatched to multiple consumer boolean false <code>false</code> checkpoint.state-store While using the <code>checkpoint</code> commit-strategy, the name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.StateStore.Factory</code> to specify the state store implementation. string false checkpoint.state-type While using the <code>checkpoint</code> commit-strategy, the fully qualified type name of the state object to persist in the state store. When provided, it can be used by the state store implementation to help persisting the processing state object. string false checkpoint.unsynced-state-max-age.ms While using the <code>checkpoint</code> commit-strategy, specify the max age in milliseconds that the processing state must be persisted before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false <code>10000</code> client-id-prefix Prefix for Kafka client <code>client.id</code> attribute. If defined configured or generated <code>client.id</code> will be prefixed with the given value. string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing, the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false <code>true</code> commit-strategy Specify the commit strategy to apply when a message produced from a record is acknowledged. Values can be <code>latest</code>, <code>ignore</code> or <code>throttled</code>. If <code>enable.auto.commit</code> is true then the default is <code>ignore</code> otherwise it is <code>throttled</code> string false consumer-rebalance-listener.name The name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener</code>. If set, this rebalance listener is applied to the consumer. string false dead-letter-queue.key.serializer When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates the key serializer to use. If not set the serializer associated to the key deserializer is used string false dead-letter-queue.producer-client-id When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates what client id the generated producer should use. Defaults is <code>kafka-dead-letter-topic-producer-$client-id</code> string false dead-letter-queue.topic When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates on which topic the record is sent. Defaults is <code>dead-letter-topic-$channel</code> string false dead-letter-queue.value.serializer When the <code>failure-strategy</code> is set to <code>dead-letter-queue</code> indicates the value serializer to use. If not set the serializer associated to the value deserializer is used string false delayed-retry-topic.max-retries When the <code>failure-strategy</code> is set to <code>delayed-retry-topic</code> indicates the maximum number of retries. If higher than the number of delayed retry topics, last topic is used. int false delayed-retry-topic.timeout When the <code>failure-strategy</code> is set to <code>delayed-retry-topic</code> indicates the global timeout per record. int false <code>120000</code> delayed-retry-topic.topics When the <code>failure-strategy</code> is set to <code>delayed-retry-topic</code> indicates topics to use. If not set the source channel name is used, with 10, 20 and 50 seconds delayed topics. string false enable.auto.commit If enabled, consumer's offset will be periodically committed in the background by the underlying Kafka client, ignoring the actual processing outcome of the records. It is recommended to NOT enable this setting and let Reactive Messaging handles the commit. boolean false <code>false</code> fail-on-deserialization-failure When no deserialization failure handler is set and a deserialization failure happens, report the failure and mark the application as unhealthy. If set to <code>false</code> and a deserialization failure happens, a <code>null</code> value is forwarded. boolean false <code>true</code> failure-strategy Specify the failure strategy to apply when a message produced from a record is acknowledged negatively (nack). Values can be <code>fail</code> (default), <code>ignore</code>, or <code>dead-letter-queue</code> string false <code>fail</code> fetch.min.bytes The minimum amount of data the server should return for a fetch request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. int false <code>1</code> graceful-shutdown Whether or not a graceful shutdown should be attempted when the application terminates. boolean false <code>true</code> group.id A unique string that identifies the consumer group the application belongs to. If not set, a unique, generated id is used string false health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false <code>true</code> health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false <code>false</code> health-topic-verification-readiness-disabled Whether the topic verification is disabled for the readiness health check. boolean false <code>false</code> health-topic-verification-startup-disabled Whether the topic verification is disabled for the startup health check. boolean false <code>false</code> health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false <code>2000</code> kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key-deserialization-failure-handler The name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler</code>. If set, deserialization failure happening when deserializing keys are delegated to this handler which may retry or provide a fallback value. string false key.deserializer The deserializer classname used to deserialize the record's key string false <code>org.apache.kafka.common.serialization.StringDeserializer</code> lazy-client Whether Kafka client is created lazily or eagerly. boolean false <code>false</code> max-queue-size-factor Multiplier factor to determine maximum number of records queued for processing, using <code>max.poll.records</code> * <code>max-queue-size-factor</code>. Defaults to 2. In <code>batch</code> mode <code>max.poll.records</code> is considered <code>1</code>. int false <code>2</code> partitions The number of partitions to be consumed concurrently. The connector creates the specified amount of Kafka consumers. It should match the number of partition of the targeted topic int false <code>1</code> pattern Indicate that the <code>topic</code> property is a regular expression. Must be used with the <code>topic</code> property. Cannot be used with the <code>topics</code> property boolean false <code>false</code> pause-if-no-requests Whether the polling must be paused when the application does not request items and resume when it does. This allows implementing back-pressure based on the application capacity. Note that polling is not stopped, but will not retrieve any records when paused. boolean false <code>true</code> poll-timeout The polling timeout in milliseconds. When polling records, the poll will wait at most that duration before returning records. Default is 1000ms int false <code>1000</code> requests When <code>partitions</code> is greater than 1, this attribute allows configuring how many records are requested by each consumers every time. int false <code>128</code> retry Whether or not the connection to the broker is re-attempted in case of failure boolean false <code>true</code> retry-attempts The maximum number of reconnection before failing. -1 means infinite retry int false <code>-1</code> retry-max-wait The max delay (in seconds) between 2 reconnects int false <code>30</code> throttled.ordered While using the <code>throttled</code> commit-strategy, configures ordering guarantees for concurrent processing. Possible values: <code>key</code> for per-key sequential processing within partitions, or <code>partition</code> for per-partition sequential processing. Requires <code>@Blocking(ordered = false)</code> for concurrency. string false throttled.ordered.max-concurrency While using the <code>throttled</code> commit-strategy with ordered processing, specifies the maximum number of groups (keys or partitions) that can be processed concurrently. This should typically match your worker pool's <code>max-concurrency</code> setting. Defaults to <code>max.poll.records</code>. int false throttled.unprocessed-record-max-age.ms While using the <code>throttled</code> commit-strategy, specify the max age in milliseconds that an unprocessed message can be before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false <code>60000</code> topic The consumed / populated Kafka topic. If neither this property nor the <code>topics</code> properties are set, the channel name is used string false topics A comma-separating list of topics to be consumed. Cannot be used with the <code>topic</code> or <code>pattern</code> properties string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> value-deserialization-failure-handler The name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler</code>. If set, deserialization failure happening when deserializing values are delegated to this handler which may retry or provide a fallback value. string false value.deserializer The deserializer classname used to deserialize the record's value string true <p>You can also pass any property supported by the underlying Kafka consumer.</p> <p>For example, to configure the <code>max.poll.records</code> property, use:</p> <pre><code>mp.messaging.incoming.[channel].max.poll.records=1000\n</code></pre> <p>Some consumer client properties are configured to sensible default values:</p> <p>If not set, <code>reconnect.backoff.max.ms</code> is set to <code>10000</code> to avoid high load on disconnection.</p> <p>If not set, <code>key.deserializer</code> is set to <code>org.apache.kafka.common.serialization.StringDeserializer</code>.</p> <p>The consumer <code>client.id</code> is configured according to the number of clients to create using <code>mp.messaging.incoming.[channel].partitions</code> property.</p> <ul> <li> <p>If a <code>client.id</code> is provided, it is used as-is or suffixed with     client index if <code>partitions</code> property is set.</p> </li> <li> <p>If a <code>client.id</code> is not provided, it is generated as     <code>kafka-consumer-[channel][-index]</code>.</p> </li> </ul>"},{"location":"kafka/request-reply/","title":"Kafka Request/Reply","text":"<p>Tech Preview</p> <p>Kafka Request Reply Emitter is a tech preview feature.</p> <p>The Kafka Request-Reply pattern allows you to publish a message to a Kafka topic and then await for a reply message that responds to the initial request.</p> <p>The <code>KafkaRequestReply</code> emitter implements the requestor (or the client) of the request-reply pattern for Kafka outbound channels:</p> <pre><code>package kafka.outbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.reply.KafkaRequestReply;\n\n@ApplicationScoped\npublic class KafkaRequestReplyEmitter {\n\n    @Inject\n    @Channel(\"my-request\")\n    KafkaRequestReply&lt;String, Integer&gt; quoteRequest;\n\n    public Uni&lt;Integer&gt; requestQuote(String request) {\n        return quoteRequest.request(request);\n    }\n}\n</code></pre> <p>The <code>request</code> method publishes the request record to the configured target topic of the outgoing channel, and polls a reply topic (by default, the target topic with <code>-replies</code> suffix) for a reply record. When the reply is received the returned <code>Uni</code> is completed with the record value.</p> <p>The request send operation generates a correlation id and sets a header (by default <code>REPLY_CORRELATION_ID</code>), which it expects to be sent back in the reply record. Two additional headers are set on the request record:</p> <ul> <li>The topic from which the reply is expected, by default <code>REPLY_TOPIC</code> header, which can be configured using the <code>reply.topic</code> channel attribute.</li> <li>Optionally, the partition from which the reply is expected, by default <code>REPLY_PARTITION</code> header. The reply partition header is added only when the Kafka request reply is configured specifically to receive records from a topic-partition , using the <code>reply.partition</code> channel attribute. The reply partition header integer value is encoded in 4 bytes, and helper methods <code>KafkaRequestReply#replyPartitionFromBytes</code> and <code>KafkaRequestReply#replyPartitionToBytes</code> can be used for custom operations.</li> </ul> <p>The replier (or the server) can be implemented using a Reactive Messaging processor:</p> <pre><code>package kafka.outbound;\n\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\n@ApplicationScoped\npublic class KafkaReplier {\n\n    Random rand = new Random();\n\n    @Incoming(\"request\")\n    @Outgoing(\"reply\")\n    int handleRequest(String request) {\n        return rand.nextInt(100);\n    }\n}\n</code></pre> <p>Kafka outgoing channels detect default <code>REPLY_CORRELATION_ID</code>, <code>REPLY_TOPIC</code> and <code>REPLY_PARTITION</code> headers and send the reply record to the expected topic-partition by propagating back the correlation id header.</p> <p>Default headers can be configured, using <code>reply.correlation-id.header</code>, <code>reply.topic.header</code> and <code>reply.partition.header</code> channel attributes. If custom headers are used the reply server needs some more manual work. Given the following request/reply outgoing configuration:</p> <pre><code>mp.messaging.outgoing.reqrep.topic=requests\nmp.messaging.outgoing.reqrep.reply.correlation-id.header=MY_CORRELATION\nmp.messaging.outgoing.reqrep.reply.topic.header=MY_TOPIC\nmp.messaging.outgoing.reqrep.reply.partition.header=MY_PARTITION\nmp.messaging.incoming.request.topic=requests\n</code></pre> <p>The reply server can be implemented as the following:</p> <pre><code>package kafka.outbound;\n\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.header.Header;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.reactive.messaging.kafka.reply.KafkaRequestReply;\n\n@ApplicationScoped\npublic class KafkaCustomHeaderReplier {\n\n    Random rand = new Random();\n\n    @Incoming(\"request\")\n    @Outgoing(\"reply\")\n    ProducerRecord&lt;String, Integer&gt; process(ConsumerRecord&lt;String, String&gt; request) {\n        Header topicHeader = request.headers().lastHeader(\"MY_TOPIC\");\n        if (topicHeader == null) {\n            // Skip\n            return null;\n        }\n        String myTopic = new String(topicHeader.value());\n        int generateValue = rand.nextInt(100);\n        Header partitionHeader = request.headers().lastHeader(\"MY_PARTITION\");\n        if (partitionHeader == null) {\n            // Propagate incoming headers, including the correlation id header\n            return new ProducerRecord&lt;&gt;(myTopic, null, request.key(), generateValue, request.headers());\n        }\n        // Send the replies to extracted myTopic-myPartition\n        int myPartition = KafkaRequestReply.replyPartitionFromBytes(partitionHeader.value());\n        return new ProducerRecord&lt;&gt;(myTopic, myPartition, request.key(), generateValue, request.headers());\n    }\n}\n</code></pre>"},{"location":"kafka/request-reply/#requesting-with-message-types","title":"Requesting with <code>Message</code> types","text":"<p>Like the core Emitter's <code>send</code> methods, <code>request</code> method also can receive a <code>Message</code> type and return a message:</p> <pre><code>package kafka.outbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.api.OutgoingKafkaRecordMetadata;\nimport io.smallrye.reactive.messaging.kafka.reply.KafkaRequestReply;\n\n@ApplicationScoped\npublic class KafkaRequestReplyMessageEmitter {\n\n    @Inject\n    @Channel(\"my-request\")\n    KafkaRequestReply&lt;String, Integer&gt; quoteRequest;\n\n    public Uni&lt;Message&lt;Integer&gt;&gt; requestMessage(String request) {\n        return quoteRequest.request(Message.of(request)\n                .addMetadata(OutgoingKafkaRecordMetadata.builder()\n                        .withKey(request)\n                        .build()));\n    }\n}\n</code></pre> <p>Note</p> <p>The ingested reply type of the <code>KafkaRequestReply</code> is discovered at runtime, in order to configure a <code>MessageConveter</code> to be applied on the incoming message before returning the <code>Uni</code> result.</p>"},{"location":"kafka/request-reply/#requesting-multiple-replies","title":"Requesting multiple replies","text":"<p>You can use the <code>requestMulti</code> method to expect any number of replies represented by the <code>Multi</code> return type.</p> <p>For example this can be used to aggregate multiple replies to a single request.</p> <p><pre><code>package org.acme;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.reactive.messaging.kafka.reply.KafkaRequestReply;\n\n@ApplicationScoped\npublic class KafkaRequestReplyMultiEmitter {\n\n    @Inject\n    @Channel(\"my-request\")\n    KafkaRequestReply&lt;String, Integer&gt; quoteRequest;\n\n    public Multi&lt;Integer&gt; requestQuote(String request) {\n        return quoteRequest.requestMulti(request).select().first(5);\n    }\n}\n</code></pre> Like the other <code>request</code> you can also request <code>Message</code> types.</p> <p>Note</p> <p>The channel attribute <code>reply.timeout</code> will be applied between each message, if reached the returned <code>Multi</code> will fail.</p>"},{"location":"kafka/request-reply/#scaling-requestreply","title":"Scaling Request/Reply","text":"<p>If multiple requestor instances are configured on the same outgoing topic, and the same reply topic, each requestor consumer will generate a unique consumer group.id and therefore all requestor instances will receive replies of all instances. If an observed correlation id doesn't match the id of any pending replies, the reply is simply discarded. With the additional network traffic this allows scaling requestors, (and repliers) dynamically.</p> <p>Alternatively, requestor instances can be configured to consume replies from dedicated topics using <code>reply.topic</code> attribute, or distinct partitions of a single topic, using <code>reply.partition</code> attribute. The later will configure the Kafka consumer to assign statically to the given partition.</p>"},{"location":"kafka/request-reply/#pending-replies-and-reply-timeout","title":"Pending replies and reply timeout","text":"<p>By default, the <code>Uni</code> returned from the <code>request</code> method is configured to fail with timeout exception if no replies is received after 5 seconds. This timeout is configurable with the channel attribute <code>reply.timeout</code>.</p> <p>A snapshot of the list of pending replies is available through the <code>KafkaRequestReply#getPendingReplies</code> method.</p>"},{"location":"kafka/request-reply/#waiting-for-topic-partition-assignment","title":"Waiting for topic-partition assignment","text":"<p>The requestor can be found in a position where a request is sent, and it's reply is already published to the reply topic, before the requestor starts and polls the consumer. In case the reply consumer is configured with <code>auto.offset.reset=latest</code>, which is the default value, this can lead to the requestor missing replies.</p> <p>If <code>auto.offset.reset</code> is <code>latest</code>, at wiring time, before any request can take place, the <code>KafkaRequestReply</code> finds partitions that the consumer needs to subscribe and waits for their assignment to the consumer. The timeout of the initial subscription can be adjusted with <code>reply.initial-assignment-timeout</code> which defaults to the <code>reply.timeout</code>. If this timeout fails, <code>KafkaRequestReply</code> will enter an invalid state which will require it to be restarted. If set to <code>-1</code>, the <code>KafkaRequestReply</code> will not wait for the initial assignment of the reply consumer to sent requests.</p> <p>On other occasions the <code>KafkaRequestReply#waitForAssignments</code> method can be used.</p>"},{"location":"kafka/request-reply/#correlation-ids","title":"Correlation Ids","text":"<p>The Kafka Request/Reply allows configuring the correlation id mechanism completely through a <code>CorrelationIdHandler</code> implementation. The default handler is based on randomly generated UUID strings, written to byte array in Kafka record headers. The correlation id handler implementation can be configured using the <code>reply.correlation-id.handler</code> attribute. As mentioned the default configuration is <code>uuid</code>, and an alternative <code>bytes</code> implementation can be used to generate 12 bytes random correlation ids.</p> <p>Custom handlers can be implemented by proposing a CDI-managed bean with <code>@Identifier</code> qualifier.</p>"},{"location":"kafka/request-reply/#reply-error-handling","title":"Reply Error Handling","text":"<p>If the reply server produces an error and can or would like to propagate the error back to the requestor, failing the returned <code>Uni</code>.</p> <p>If configured using the <code>reply.failure.handler</code> channel attribute, the <code>ReplyFailureHandler</code> implementations are discovered through CDI, matching the <code>@Identifier</code> qualifier.</p> <p>A sample reply error handler can lookup header values and return the error to be thrown by the reply:</p> <pre><code>package kafka.outbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.common.header.Header;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport io.smallrye.reactive.messaging.kafka.reply.ReplyFailureHandler;\n\n@ApplicationScoped\n@Identifier(\"my-reply-error\")\npublic class MyReplyFailureHandler implements ReplyFailureHandler {\n\n    @Override\n    public Throwable handleReply(KafkaRecord&lt;?, ?&gt; replyRecord) {\n        Header header = replyRecord.getHeaders().lastHeader(\"REPLY_ERROR\");\n        if (header != null) {\n            return new IllegalArgumentException(new String(header.value()));\n        }\n        return null;\n    }\n}\n</code></pre> <p><code>null</code> return value indicates that no error has been found in the reply record, and it can be delivered to the application.</p>"},{"location":"kafka/request-reply/#advanced-configuration-for-the-kafka-consumer","title":"Advanced configuration for the Kafka consumer","text":"<p>The underlying Kafka consumer can be configured with the <code>reply</code> property prefix. For example the underlying Kafka consuemer can be configured to batch mode using:</p> <pre><code>mp.messaging.outgoing.reqrep.topic=requests\nmp.messaging.outgoing.reqrep.reply.topic=quote-results\nmp.messaging.outgoing.reqrep.reply.batch=true\nmp.messaging.outgoing.reqrep.reply.commit-strategy=latest\n</code></pre>"},{"location":"kafka/test-companion/","title":"Kafka Companion","text":"<p>Experimental</p> <p>Kafka Companion is experimental and APIs are subject to change in the future.</p> <p>The Kafka Companion is a separate Java library for helping to test Kafka applications. It is not intended to mock Kafka, but to the contrary, connect to a Kafka broker and provide high-level features.</p> <p>It is not limited to the SmallRye Reactive Messaging testing, but intends to improve the testability of applications using Kafka. Some of its features:</p> <ul> <li>Running In-container Kafka broker for tests via strimzi-test-container.</li> <li>Running the Kafka broker behind a toxiproxy for simulating network issues.</li> <li>Running embedded Kafka Kraft broker for tests.</li> <li>Base classes for tests to bootstrap tests.</li> <li>Companion classes for easily creating tasks to produce and consume Kafka records.</li> <li>Writing assertions for produce and consume tasks, state of consumers, topics, offsets etc.</li> </ul>"},{"location":"kafka/test-companion/#getting-started-writing-tests","title":"Getting started writing tests","text":"<p>Easiest way of starting to write Kafka tests is to extend <code>KafkaCompanionTestBase</code>. It starts a single-node Kafka broker for the test suite using testcontainers and creates the <code>KafkaCompanion</code> to connect to this broker:</p> <pre><code>public class KafkaWithBaseTest extends KafkaCompanionTestBase {\n\n    @Test\n    public void testWithBase() {\n        // companion is created by the base class\n\n        // produce 100 records to messages topic\n        ProducerTask producerTask = companion.produceIntegers()\n                .usingGenerator(i -&gt; new ProducerRecord&lt;&gt;(\"messages\", i), 100);\n        long msgCount = producerTask.awaitCompletion().count();\n        Assertions.assertEquals(msgCount, 100);\n\n        // consume 100 records from messages topic\n        ConsumerTask&lt;String, Integer&gt; consumerTask = companion.consumeIntegers()\n                .fromTopics(\"messages\", 100);\n        ConsumerRecord&lt;String, Integer&gt; lastRecord = consumerTask.awaitCompletion().getLastRecord();\n        Assertions.assertEquals(lastRecord.value(), 99);\n    }\n}\n</code></pre> <p><code>KafkaCompanion</code> can be used on its own to connect to a broker:</p> <pre><code>// Create companion with bootstrap servers and API timeout (default is 10 seconds)\nKafkaCompanion companion = new KafkaCompanion(\"localhost:9092\", Duration.ofSeconds(5));\n\n// Create producer and start producer task\nProducerBuilder&lt;String, Integer&gt; producer = companion.produceIntegers()\n        .withClientId(\"my-producer\")\n        .withProp(\"max.block.ms\", \"5000\");\nproducer.usingGenerator(i -&gt; new ProducerRecord&lt;&gt;(\"topic\", i), 100);\n\n// Create consumer and start consumer task\nConsumerBuilder&lt;String, Integer&gt; consumer = companion.consumeIntegers()\n        .withGroupId(\"my-group\")\n        .withCommitAsyncWhen(record -&gt; true);\nConsumerTask&lt;String, Integer&gt; records = consumer.fromTopics(\"topic\", Duration.ofSeconds(10));\n// Await completion and assert consumed record count\nAssertions.assertEquals(records.awaitCompletion().count(), 100);\n</code></pre> <p>There are a couple of things to note on how Kafka companion handles producers, consumers and tasks:</p> <p><code>ProducerBuilder</code> and <code>ConsumerBuilder</code> lazy descriptions of with which configuration to create a producer or a consumer.</p> <p><code>ProducerTask</code> and <code>ConsumerTask</code> run asynchronously on dedicated threads and are started as soon as they are created. A task terminates when it is explicitly stopped, when it's predefined duration or number of records has been produced/consumed, or when it encounters an error. An exterior thread can await on records processed, or simply on termination of the task. At a given time produced or consumed records are accessible through the task.</p> <p>The actual creation of the producer or consumer happens when a task is started. When the task terminates the producer or the consumer is automatically closed.</p> <p>For example, in the previous example:</p> <ol> <li>We described a producer with a client id <code>my-producer</code> and <code>max.block.ms</code> of 5 seconds.</li> <li>Then we created a task to produce 100 records using the generator function, without waiting for its completion.</li> <li>We then described a consumer with group id <code>my-group</code> and which commits offset synchronously on every received record.</li> <li>Finally, we created a task to consume records for 10 seconds and await its completion.</li> </ol>"},{"location":"kafka/test-companion/#producing-records","title":"Producing records","text":""},{"location":"kafka/test-companion/#produce-from-records","title":"Produce from records","text":"<p>Produce given records: <pre><code>companion.produce(byte[].class).fromRecords(\n        new ProducerRecord&lt;&gt;(\"topic1\", \"k1\", \"1\".getBytes()),\n        new ProducerRecord&lt;&gt;(\"topic1\", \"k2\", \"2\".getBytes()),\n        new ProducerRecord&lt;&gt;(\"topic1\", \"k3\", \"3\".getBytes()));\n</code></pre></p>"},{"location":"kafka/test-companion/#produce-from-generator-function","title":"Produce from generator function","text":"<p>Produce 10 records using the generator function: <pre><code>companion.produceIntegers().usingGenerator(i -&gt; new ProducerRecord&lt;&gt;(\"topic\", i), 10);\n</code></pre></p>"},{"location":"kafka/test-companion/#produce-from-csv-file","title":"Produce from CSV file","text":"<p>Given a comma-separated file <code>records.csv</code> with the following content <pre><code>messages,0,a,asdf\nmessages,1,b,asdf\nmessages,3,c,asdf\n</code></pre></p> <p>Produce records from the file: <pre><code>companion.produceStrings().fromCsv(\"records.csv\");\n</code></pre></p>"},{"location":"kafka/test-companion/#consuming-records","title":"Consuming records","text":""},{"location":"kafka/test-companion/#consume-from-topics","title":"Consume from topics","text":"<pre><code>companion.consumeIntegers().fromTopics(\"topic1\", \"topic2\");\n</code></pre>"},{"location":"kafka/test-companion/#consume-from-offsets","title":"Consume from offsets","text":"<pre><code>Map&lt;TopicPartition, Long&gt; offsets = new HashMap&lt;&gt;();\noffsets.put(new TopicPartition(\"topic1\", 0), 100L);\noffsets.put(new TopicPartition(\"topic2\", 0), 100L);\ncompanion.consumeIntegers().fromOffsets(offsets, Duration.ofSeconds(10));\n</code></pre>"},{"location":"kafka/test-companion/#consumer-assignment-and-offsets","title":"Consumer assignment and offsets","text":"<p>During execution of the consumer task, information about the underlying consumer's topic partition assignment, position or committed offsets can be accessed. <pre><code>ConsumerBuilder&lt;String, Integer&gt; consumer = companion.consumeIntegers()\n        .withAutoCommit();\nconsumer.fromTopics(\"topic\");\n// ...\nawait().untilAsserted(consumer::waitForAssignment);\nconsumer.committed(new TopicPartition(\"topic\", 1));\n</code></pre></p>"},{"location":"kafka/test-companion/#registering-custom-serdes","title":"Registering Custom Serdes","text":"<p>KafkaCompanion handles Serializers and Deserializers for default types such as primitives, String, ByteBuffer, UUID.</p> <p>Serdes for custom types can be registered to the companion object, and will be used for producer and consumer tasks:</p> <pre><code>KafkaCompanion companion = new KafkaCompanion(\"localhost:9092\");\n\n// Register serde to the companion\ncompanion.registerSerde(Orders.class, new OrdersSerializer(), new OrdersDeserializer());\n\n// Companion will configure consumer accordingly\nConsumerTask&lt;Integer, Orders&gt; orders = companion.consume(Integer.class, Orders.class)\n        .fromTopics(\"orders\", 1000).awaitCompletion();\n\nfor (ConsumerRecord&lt;Integer, Orders&gt; order : orders) {\n    // ... check consumed records\n}\n</code></pre>"},{"location":"kafka/test-companion/#topics","title":"Topics","text":"<p>Create, list, describe and delete topics: <pre><code>companion.topics().create(\"topic1\", 1);\ncompanion.topics().createAndWait(\"topic2\", 3);\nAssertions.assertEquals(companion.topics().list().size(), 2);\n\ncompanion.topics().delete(\"topic1\");\n</code></pre></p>"},{"location":"kafka/test-companion/#consumer-groups-and-offsets","title":"Consumer Groups and Offsets","text":"<p>List topic partition offsets: <pre><code>TopicPartition topicPartition = KafkaCompanion.tp(\"topic\", 1);\nlong latestOffset = companion.offsets().get(topicPartition, OffsetSpec.latest()).offset();\n</code></pre></p> <p>List, describe, alter consumer groups and their offsets: <pre><code>Collection&lt;ConsumerGroupListing&gt; consumerGroups = companion.consumerGroups().list();\nfor (ConsumerGroupListing consumerGroup : consumerGroups) {\n    // check consumer groups\n}\n\nTopicPartition topicPartition = KafkaCompanion.tp(\"topic\", 1);\ncompanion.consumerGroups().resetOffsets(\"consumer-group\", topicPartition);\n</code></pre></p>"},{"location":"kafka/transactions/","title":"Kafka Transactions and Exactly-Once Processing","text":"<p>Tech Preview</p> <p>Kafka Transactions is a tech preview feature.</p> <p>Kafka transactions enable atomic writes to multiple Kafka topics and partitions. Inside a transaction, a producer writes records to the Kafka topic partitions as it would normally do. If the transaction completes successfully, all the records previously written to the broker inside that transaction will be committed, and will be readable for consumers. If an error during the transaction causes it to be aborted, none will be readable for consumers.</p> <p>There are a couple of fundamental things to consider before using transactions:</p> <ul> <li> <p>Each transactional producer is configured with a unique identifier called the <code>transactional.id</code>. This is used to identify the same producer instance across application restarts. By default, it is not configured, and transactions cannot be used. When it is configured, the producer is limited to idempotent delivery, therefore <code>enable.idempotence=true</code> is implied.</p> </li> <li> <p>For only reading transactional messages, a consumer needs to explicitly configure its <code>isolation.level</code> property to <code>read_committed</code>. This will make sure that the consumer will deliver only records committed inside a transaction, and filter out messages from aborted transactions.</p> </li> <li> <p>It should also be noted that this does not mean all records atomically written inside a transaction will be read atomically by the consumer. Transactional producers can write to multiple topics and partitions inside a transaction, but consumers do not know where the transaction starts or ends. Not only multiple consumers inside a consumer group can share those partitions, all records which were part of a single transaction can be consumed in different batch of records by a consumer.</p> </li> </ul> <p>Kafka connector provides <code>KafkaTransactions</code> custom emitter for writing records inside a transaction. Before using a transaction we need to make sure that <code>transactional.id</code> is configured on the channel properties.</p> <pre><code>mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer\n</code></pre> <p>It can be used as a regular emitter <code>@Channel</code>:</p> <pre><code>package kafka.outbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n\n@ApplicationScoped\npublic class KafkaTransactionalProducer {\n\n    @Inject\n    @Channel(\"tx-out-example\")\n    KafkaTransactions&lt;String&gt; txProducer;\n\n    public Uni&lt;Void&gt; emitInTransaction() {\n        return txProducer.withTransaction(emitter -&gt; {\n            emitter.send(KafkaRecord.of(1, \"a\"));\n            emitter.send(KafkaRecord.of(2, \"b\"));\n            emitter.send(KafkaRecord.of(3, \"c\"));\n            return Uni.createFrom().voidItem();\n        });\n    }\n\n}\n</code></pre> <p>Note</p> <p>When <code>transactional.id</code> is provided <code>KafkaProducer#initTransactions</code> is called when the underlying Kafka producer is created.</p> <p>The function given to the <code>withTransaction</code> method receives a <code>TransactionalEmitter</code> for producing records, and returns a <code>Uni</code> that provides the result of the transaction. If the processing completes successfully, the producer is flushed and the transaction is committed. If the processing throws an exception, returns a failing <code>Uni</code>, or marks the <code>TransactionalEmitter</code> for abort, the transaction is aborted.</p> <p>If this method is called on a Vert.x context, the processing function is also called on that context. Otherwise, it is called on the sending thread of the producer.</p> <p>Important</p> <p>A transaction is considered in progress from the call to the <code>withTransaction</code> until the returned <code>Uni</code> results in success or failure. While a transaction is in progress, subsequent calls to the <code>withTransaction</code>, including nested ones inside the given function, will throw <code>IllegalStateException</code>. Note that in Reactive Messaging, the execution of processing methods is already serialized, unless <code>@Blocking(ordered = false)</code> is used. If <code>withTransaction</code> can be called concurrently, for example from a REST endpoint, it is recommended to limit the concurrency of the execution. This can be done using the <code>@Bulkhead</code> annotation from Microprofile Fault Tolerance.</p>"},{"location":"kafka/transactions/#exactly-once-processing","title":"Exactly-Once Processing","text":"<p>Kafka Transactions API also allows managing consumer offsets inside a transaction, together with produced messages. This in turn enables coupling a consumer with a transactional producer in a consume-transform-produce pattern, also known as exactly-once processing. It means that an application consumes messages from a topic-partition, processes them, publishes the results to a topic-partition, and commits offsets of the consumed messages in a transaction.</p> <p>The <code>KafkaTransactions</code> emitter also provides a way to apply exactly-once processing to an incoming Kafka message inside a transaction.</p> <p>The following example includes a batch of Kafka records inside a transaction.</p> <pre><code>mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer\nmp.messaging.outgoing.in-channel.batch=true\nmp.messaging.outgoing.in-channel.commit-strategy=ignore\nmp.messaging.outgoing.in-channel.failure-strategy=ignore\n</code></pre> <pre><code>package kafka.outbound;\n\nimport java.util.List;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport io.smallrye.reactive.messaging.kafka.Record;\nimport io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n\n@ApplicationScoped\npublic class KafkaExactlyOnceProcessor {\n\n    @Inject\n    @Channel(\"tx-out-example\")\n    KafkaTransactions&lt;Integer&gt; txProducer;\n\n    @Incoming(\"in-channel\")\n    public Uni&lt;Void&gt; emitInTransaction(Message&lt;List&lt;Record&lt;String, Integer&gt;&gt;&gt; batch) {\n        return txProducer.withTransaction(batch, emitter -&gt; {\n            for (Record&lt;String, Integer&gt; record : batch.getPayload()) {\n                emitter.send(KafkaRecord.of(record.key(), record.value() + 1));\n            }\n            return Uni.createFrom().voidItem();\n        });\n    }\n\n}\n</code></pre> <p>If the processing completes successfully, before committing the transaction, the topic partition offsets of the given batch message will be committed to the transaction.</p> <p>If the processing needs to abort, after aborting the transaction, the consumer's position is reset to the last committed offset, effectively resuming the consumption from that offset. If no consumer offset has been committed, the consumer's position is reset to the beginning of the topic, even if the offset reset policy is <code>latest</code>.</p> <p>Important</p> <p>When using exactly-once processing, consumed message offset commits are handled by the transaction and therefore <code>commit-strategy</code> needs to be <code>ignore</code>.</p> <p>Any strategy can be employed for the <code>failure-strategy</code>. Note that the <code>Uni</code> returned from the <code>#withTransaction</code> will yield a failure if the transaction fails and is aborted.</p> <p>The application can choose to handle the error case, but for the message consumption to continue, <code>Uni</code> returned from the <code>@Incoming</code> method must not result in failure. <code>KafkaTransactions#withTransactionAndAck</code> method will ack and nack the message but will not stop the reactive stream. Ignoring the failure simply resets the consumer to the last committed offsets and resumes the processing from there.</p> <p>Note</p> <p>It is recommended to use exactly-once processing along with the batch consumption mode. While it is possible to use it with a single Kafka message, it'll have a significant performance impact.</p>"},{"location":"kafka/writing-kafka-records/","title":"Writing Kafka Records","text":"<p>The Kafka Connector can write Reactive Messaging <code>Messages</code> as Kafka Records.</p>"},{"location":"kafka/writing-kafka-records/#example","title":"Example","text":"<p>Let\u2019s imagine you have a Kafka broker running, and accessible using the <code>kafka:9092</code> address (by default it would use <code>localhost:9092</code>). Configure your application to write the messages from the <code>prices</code> channel into a Kafka topic as follows:</p> <p><pre><code>kafka.bootstrap.servers=kafka:9092 # &lt;1&gt;\n\nmp.messaging.outgoing.prices-out.connector=smallrye-kafka # &lt;2&gt;\nmp.messaging.outgoing.prices-out.value.serializer=org.apache.kafka.common.serialization.DoubleSerializer # &lt;3&gt;\nmp.messaging.outgoing.prices-out.topic=prices # &lt;4&gt;\n</code></pre> 1.  Configure the broker location. You can configure it globally or per     channel 2.  Configure the connector to manage the <code>prices</code> channel 3.  Sets the (Kafka) serializer to encode the message payload into the     record\u2019s value 4.  Make sure the topic name is <code>prices</code> (and not the default     <code>prices-out</code>)</p> <p>Then, your application must send <code>Message&lt;Double&gt;</code> to the <code>prices</code> channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package kafka.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class KafkaPriceProducer {\n\n    private final Random random = new Random();\n\n    @Outgoing(\"prices-out\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package kafka.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class KafkaPriceMessageProducer {\n\n    private final Random random = new Random();\n\n    @Outgoing(\"prices-out\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble()));\n    }\n\n}\n</code></pre>"},{"location":"kafka/writing-kafka-records/#serialization","title":"Serialization","text":"<p>The serialization is handled by the underlying Kafka Client. You need to configure the:</p> <ul> <li> <p><code>mp.messaging.outgoing.[channel-name].value.serializer</code> to configure     the value serializer (mandatory)</p> </li> <li> <p><code>mp.messaging.outgoing.[channel-name].key.serializer</code> to configure     the key serializer (optional, default to <code>String</code>)</p> </li> </ul> <p>If you want to use a custom serializer, add it to your <code>CLASSPATH</code> and configure the associate attribute.</p> <p>By default, the written record contains:</p> <ul> <li> <p>the <code>Message</code> payload as value</p> </li> <li> <p>no key, or the key configured using the <code>key</code> attribute or the key     passed in the metadata attached to the <code>Message</code></p> </li> <li> <p>the timestamp computed for the system clock (<code>now</code>) or the timestamp     passed in the metadata attached to the <code>Message</code></p> </li> </ul>"},{"location":"kafka/writing-kafka-records/#sending-keyvalue-pairs","title":"Sending key/value pairs","text":"<p>In the Kafka world, it\u2019s often necessary to send records, i.e. a key/value pair. The connector provides the io.smallrye.reactive.messaging.kafka.Record class that you can use to send a pair:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic Record&lt;String, String&gt; process(String in) {\n    return Record.of(\"my-key\", in);\n}\n</code></pre> <p>When the connector receives a message with a <code>Record</code> payload, it extracts the key and value from it. The configured serializers for the key and the value must be compatible with the record\u2019s key and value. Note that the <code>key</code> and the <code>value</code> can be <code>null</code>. It is also possible to create a record with a <code>null</code> key AND a <code>null</code> value.</p> <p>If you need more control on the written records, use <code>OutgoingKafkaRecordMetadata</code>.</p>"},{"location":"kafka/writing-kafka-records/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of OutgoingKafkaRecordMetadata to influence how the message is going to be written to Kafka. For example, you can add Kafka headers, configure the record key\u2026</p> <pre><code>// Creates an OutgoingKafkaRecordMetadata\n// The type parameter is the type of the record's key\nOutgoingKafkaRecordMetadata&lt;String&gt; metadata = OutgoingKafkaRecordMetadata.&lt;String&gt; builder()\n        .withKey(\"my-key\")\n        .withHeaders(new RecordHeaders().add(\"my-header\", \"value\".getBytes()))\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre>"},{"location":"kafka/writing-kafka-records/#propagating-record-key","title":"Propagating Record Key","text":"<p>When processing messages, you can propagate incoming record key to the outgoing record.</p> <p>Consider the following example method, which consumes messages from the channel <code>in</code>, transforms the payload, and writes the result to the channel <code>out</code>.</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\npublic double process(int in) {\n    return in * 0.88;\n}\n</code></pre> <p>Enabled with <code>mp.messaging.outgoing.[channel-name].propagate-record-key=true</code> configuration, record key propagation produces the outgoing record with the same key as the incoming record.</p> <p>If the outgoing record already contains a key, it won\u2019t be overridden by the incoming record key. If the incoming record does have a null key, the <code>mp.messaging.outgoing.[channel-name].key</code> property is used.</p>"},{"location":"kafka/writing-kafka-records/#propagating-record-headers","title":"Propagating Record headers","text":"<p>You can also propagate incoming record headers to the outgoing record, by specifying the list of headers to be considered.</p> <p><code>mp.messaging.outgoing.[channel-name].propagate-headers=Authorization,Proxy-Authorization</code></p> <p>If the ougoing record already defines a header with the same key, it won't be overriden by the incoming header.</p>"},{"location":"kafka/writing-kafka-records/#dynamic-topic-names","title":"Dynamic topic names","text":"<p>Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the topic inside your application configuration file, but instead, use the outbound metadata to set the name of the topic.</p> <p>For example, you can route to a dynamic topic based on the incoming message:</p> <pre><code>String topicName = selectTopicFromIncommingMessage(incoming);\nOutgoingKafkaRecordMetadata&lt;String&gt; metadata = OutgoingKafkaRecordMetadata.&lt;String&gt; builder()\n        .withTopic(topicName)\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre>"},{"location":"kafka/writing-kafka-records/#acknowledgement","title":"Acknowledgement","text":"<p>Kafka acknowledgement can take times depending on the configuration. Also, it stores in-memory the records that cannot be written.</p> <p>By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received <code>Message</code>). You can disable this by setting the <code>waitForWriteCompletion</code> attribute to <code>false</code>.</p> <p>Note that the <code>acks</code> attribute has a huge impact on the record acknowledgement.</p> <p>If a record cannot be written, the message is <code>nacked</code>.</p>"},{"location":"kafka/writing-kafka-records/#back-pressure-and-inflight-records","title":"Back-pressure and inflight records","text":"<p>The Kafka outbound connector handles back-pressure monitoring the number of in-flight messages waiting to be written to the Kafka broker. The number of in-flight messages is configured using the <code>max-inflight-messages</code> attribute and defaults to 1024.</p> <p>The connector only sends that amount of messages concurrently. No other messages will be sent until at least one in-flight message gets acknowledged by the broker. Then, the connector writes a new message to Kafka when one of the broker\u2019s in-flight messages get acknowledged. Be sure to configure Kafka\u2019s <code>batch.size</code> and <code>linger.ms</code> accordingly.</p> <p>You can also remove the limit of inflight messages by setting <code>max-inflight-messages</code> to <code>0</code>. However, note that the Kafka Producer may block if the number of requests reaches <code>max.in.flight.requests.per.connection</code>.</p>"},{"location":"kafka/writing-kafka-records/#handling-serialization-failures","title":"Handling serialization failures","text":"<p>For Kafka producer client serialization failures are not recoverable, thus the message dispatch is not retried. However, using schema registries, serialization may intermittently fail to contact the registry. In these cases you may need to apply a failure strategy for the serializer. To achieve this, create a CDI bean implementing the SerializationFailureHandler interface:</p> <pre><code>@ApplicationScoped\n@Identifier(\"failure-fallback\") // Set the name of the failure handler\npublic class MySerializationFailureHandler\n    implements SerializationFailureHandler&lt;JsonObject&gt; { // Specify the expected type\n\n    @Override\n    public byte[] decorateSerialization(Uni&lt;byte[]&gt; serialization, String topic,\n                boolean isKey, String serializer,\n                Object data, Headers headers) {\n        return serialization\n                    .onFailure().retry().atMost(3)\n                    .await().atMost(Duration.ofMillis(200));\n    }\n}\n</code></pre> <p>The bean must be exposed with the <code>@Identifier</code> qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute:</p> <ul> <li> <p><code>mp.messaging.incoming.$channel.key-serialization-failure-handler</code>:     name of the bean handling serialization failures happening for the     record\u2019s key</p> </li> <li> <p><code>mp.messaging.incoming.$channel.value-serialization-failure-handler</code>:     name of the bean handling serialization failures happening for the     record\u2019s value,</p> </li> </ul> <p>The handler is called with the serialization action as a <code>Uni</code>, the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers. Failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the serialized byte array. Alternatively, the handler can implement <code>decorateSerialization</code> method which can return a fallback value.</p>"},{"location":"kafka/writing-kafka-records/#sending-cloud-events","title":"Sending Cloud Events","text":"<p>The Kafka connector supports Cloud Events. The connector sends the outbound record as Cloud Events if:</p> <ul> <li> <p>the message metadata contains an     <code>io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata</code>     instance,</p> </li> <li> <p>the channel configuration defines the <code>cloud-events-type</code> and     <code>cloud-events-source</code> attributes.</p> </li> </ul> <p>You can create <code>io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata</code> instances using:</p> <pre><code>package kafka.outbound;\n\nimport java.net.URI;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata;\n\n@ApplicationScoped\npublic class KafkaCloudEventProcessor {\n\n    @Outgoing(\"cloud-events\")\n    public Message&lt;String&gt; toCloudEvents(Message&lt;String&gt; in) {\n        return in.addMetadata(OutgoingCloudEventMetadata.builder()\n                .withId(\"id-\" + in.getPayload())\n                .withType(\"greetings\")\n                .withSource(URI.create(\"http://example.com\"))\n                .withSubject(\"greeting-message\")\n                .build());\n    }\n\n}\n</code></pre> <p>If the metadata does not contain an id, the connector generates one (random UUID). The <code>type</code> and <code>source</code> can be configured per message or at the channel level using the <code>cloud-events-type</code> and <code>cloud-events-source</code> attributes. Other attributes are also configurable.</p> <p>The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values:</p> <pre><code>package kafka.outbound;\n\nimport java.net.URI;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata;\n\n@ApplicationScoped\npublic class KafkaCloudEventMultipleProcessors {\n\n    @Incoming(\"source\")\n    @Outgoing(\"processed\")\n    public Message&lt;String&gt; process(Message&lt;String&gt; in) {\n        return in.addMetadata(OutgoingCloudEventMetadata.builder()\n                .withId(\"id-\" + in.getPayload())\n                .withType(\"greeting\")\n                .build());\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Incoming(\"processed\")\n    @Outgoing(\"cloud-events\")\n    public Message&lt;String&gt; process2(Message&lt;String&gt; in) {\n        OutgoingCloudEventMetadata&lt;String&gt; metadata = in\n                .getMetadata(OutgoingCloudEventMetadata.class)\n                .orElseGet(() -&gt; OutgoingCloudEventMetadata.builder().build());\n\n        return in.addMetadata(OutgoingCloudEventMetadata.from(metadata)\n                .withSource(URI.create(\"source://me\"))\n                .withSubject(\"test\")\n                .build());\n    }\n\n}\n</code></pre> <p>By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the <code>cloud-events-mode</code> to <code>structured</code>. Only JSON is supported, so the created records had its <code>content-type</code> header set to <code>application/cloudevents+json; charset=UTF-8</code> When using the structured mode, the value serializer must be set to <code>org.apache.kafka.common.serialization.StringSerializer</code>, otherwise the connector reports the error. In addition, in structured, the connector maps the message\u2019s payload to JSON, except for <code>String</code> passed directly.</p> <p>The record\u2019s key can be set in the channel configuration (<code>key</code> attribute), in the <code>OutgoingKafkaRecordMetadata</code> or using the <code>partitionkey</code> Cloud Event attribute.</p> <p>Note</p> <p>you can disable the Cloud Event support by setting the <code>cloud-events</code> attribute to <code>false</code></p>"},{"location":"kafka/writing-kafka-records/#using-producerrecord","title":"Using <code>ProducerRecord</code>","text":"<p>Kafka built-in type ProducerRecord\\ can also be used for producing messages: <pre><code>@Outgoing(\"out\")\npublic ProducerRecord&lt;String, String&gt; generate() {\n    return new ProducerRecord&lt;&gt;(\"my-topic\", \"key\", \"value\");\n}\n</code></pre> <p>Warning</p> <p>This is an advanced feature. The <code>ProducerRecord</code> is sent to Kafka as is. Any possible metadata attached through <code>Message&lt;ProducerRecord&lt;K, V&gt;&gt;</code> are ignored and lost.</p>"},{"location":"kafka/writing-kafka-records/#producer-interceptors","title":"Producer Interceptors","text":"<p>Producer interceptors can be configured for Kafka producer clients using the standard <code>interceptor.classes</code> property. Configured classes will be instantiated by the Kafka producer on client creation.</p> <p>Alternatively, producer clients can be configured with a CDI managed-bean implementing org.apache.kafka.clients.producer.ProducerInterceptor interface:</p> <p>To achieve this, the bean must be exposed with the <code>@Identifier</code> qualifier specifying the name of the bean:</p> <pre><code>package kafka.outbound;\n\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport io.smallrye.common.annotation.Identifier;\n\n@ApplicationScoped\n@Identifier(\"my-producer-interceptor\")\npublic class ProducerInterceptorBeanExample implements ProducerInterceptor&lt;Integer, String&gt; {\n\n    @Override\n    public ProducerRecord&lt;Integer, String&gt; onSend(ProducerRecord&lt;Integer, String&gt; producerRecord) {\n        // called before send\n        return producerRecord;\n    }\n\n    @Override\n    public void onAcknowledgement(RecordMetadata recordMetadata, Exception e) {\n        // called on send acknowledgement callback\n    }\n\n    @Override\n    public void close() {\n        // called on client close\n    }\n\n    @Override\n    public void configure(Map&lt;String, ?&gt; map) {\n        // called on client configuration\n    }\n}\n</code></pre> <p>Then, in the channel configuration, specify the following attribute: <code>mp.messaging.incoming.$channel.interceptor-bean=my-producer-interceptor</code>.</p> <p>Warning</p> <p>The <code>onSend</code> method will be called on the producer sending thread and <code>onAcknowledgement</code> will be called on the Kafka producer I/O thread. In both cases if implementations are not fast, sending of messages could be delayed.</p>"},{"location":"kafka/writing-kafka-records/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default acks The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. Accepted values are: 0, 1, all string false <code>1</code> bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false <code>localhost:9092</code> buffer.memory The total bytes of memory the producer can use to buffer records waiting to be sent to the server. long false <code>33554432</code> client-id-prefix Prefix for Kafka client <code>client.id</code> attribute. If defined configured or generated <code>client.id</code> will be prefixed with the given value. string false close-timeout The amount of milliseconds waiting for a graceful shutdown of the Kafka producer int false <code>10000</code> cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing, the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false <code>true</code> cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default <code>datacontenttype</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>datacontenttype</code> attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default <code>dataschema</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>dataschema</code> attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the <code>time</code> attribute into the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>time</code> attribute itself boolean false <code>true</code> cloud-events-mode The Cloud Event mode (<code>structured</code> or <code>binary</code> (default)). Indicates how are written the cloud events in the outgoing record string false <code>binary</code> cloud-events-source (cloud-events-default-source) Configure the default <code>source</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>source</code> attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default <code>subject</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>subject</code> attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default <code>type</code> attribute of the outgoing Cloud Event. Requires <code>cloud-events</code> to be set to <code>true</code>. This value is used if the message does not configure the <code>type</code> attribute itself string false health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false <code>true</code> health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false <code>false</code> health-topic-verification-readiness-disabled Whether the topic verification is disabled for the readiness health check. boolean false <code>false</code> health-topic-verification-startup-disabled Whether the topic verification is disabled for the startup health check. boolean false <code>false</code> health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false <code>2000</code> interceptor-bean The name set in <code>@Identifier</code> of a bean that implements <code>org.apache.kafka.clients.producer.ProducerInterceptor</code>. If set, the identified bean will be used as the producer interceptor. string false kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key A key to used when writing the record string false key-serialization-failure-handler The name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.SerializationFailureHandler</code>. If set, serialization failure happening when serializing keys are delegated to this handler which may provide a fallback value. string false key.serializer The serializer classname used to serialize the record's key string false <code>org.apache.kafka.common.serialization.StringSerializer</code> lazy-client Whether Kafka client is created lazily or eagerly. boolean false <code>false</code> max-inflight-messages The maximum number of messages to be written to Kafka concurrently. It limits the number of messages waiting to be written and acknowledged by the broker. You can set this attribute to <code>0</code> remove the limit long false <code>1024</code> merge Whether the connector should allow multiple upstreams boolean false <code>false</code> partition The target partition id. -1 to let the client determine the partition int false <code>-1</code> propagate-headers A comma-separating list of incoming record headers to be propagated to the outgoing record string false `` propagate-record-key Propagate incoming record key to the outgoing record boolean false <code>false</code> retries If set to a positive number, the connector will try to resend any record that was not delivered successfully (with a potentially transient error) until the number of retries is reached. If set to 0, retries are disabled. If not set, the connector tries to resend any record that failed to be delivered (because of a potentially transient error) during an amount of time configured by <code>delivery.timeout.ms</code>. long false <code>2147483647</code> topic The consumed / populated Kafka topic. If neither this property nor the <code>topics</code> properties are set, the channel name is used string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> value-serialization-failure-handler The name set in <code>@Identifier</code> of a bean that implements <code>io.smallrye.reactive.messaging.kafka.SerializationFailureHandler</code>. If set, serialization failure happening when serializing values are delegated to this handler which may provide a fallback value. string false value.serializer The serializer classname used to serialize the payload string true waitForWriteCompletion Whether the client waits for Kafka to acknowledge the written record before acknowledging the message boolean false <code>true</code> <p>You can also pass any property supported by the underlying Kafka producer.</p> <p>For example, to configure the <code>batch.size</code> property, use:</p> <pre><code>mp.messaging.outgoing.[channel].batch.size=32768\n</code></pre> <p>Some producer client properties are configured to sensible default values:</p> <p>If not set, <code>reconnect.backoff.max.ms</code> is set to <code>10000</code> to avoid high load on disconnection.</p> <p>If not set, <code>key.serializer</code> is set to <code>org.apache.kafka.common.serialization.StringSerializer</code>.</p> <p>If not set, producer <code>client.id</code> is generated as <code>kafka-producer-[channel]</code>.</p>"},{"location":"mqtt/client-customization/","title":"Customizing the underlying MQTT client","text":"<p>You can customize the underlying MQTT Client configuration by producing an instance of <code>io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions</code>:</p> <pre><code>@Produces\n@Identifier(\"my-options\")\npublic MqttClientSessionOptions getOptions() {\n    // You can use the produced options to configure the TLS connection\n    PemKeyCertOptions keycert = new PemKeyCertOptions()\n            .addCertPath(\"./tls/tls.crt\")\n            .addKeyPath(\"./tls/tls.key\");\n    PemTrustOptions trust = new PemTrustOptions().addCertPath(\"./tlc/ca.crt\");\n\n    return new MqttClientSessionOptions()\n            .setSsl(true)\n            .setPemKeyCertOptions(keycert)\n            .setPemTrustOptions(trust)\n            .setHostnameVerificationAlgorithm(\"HTTPS\")\n            .setConnectTimeout(30000)\n            .setReconnectInterval(5000);\n}\n</code></pre> <p>This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the <code>client-options-name</code> attribute:</p> <pre><code>mp.messaging.incoming.prices.client-options-name=my-options\n</code></pre>"},{"location":"mqtt/mqtt/","title":"MQTT Connector","text":"<p>The MQTT connector adds support for MQTT to Reactive Messaging.</p> <p>It lets you receive messages from an MQTT server or broker as well as send MQTT messages. The MQTT connector is based on the Vert.x MQTT Client.</p>"},{"location":"mqtt/mqtt/#introduction","title":"Introduction","text":"<p>MQTT is a machine-to-machine (M2M)/\"Internet of Things\" connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport.</p> <p>The MQTT Connector allows consuming messages from MQTT as well as sending MQTT messages.</p>"},{"location":"mqtt/mqtt/#using-the-mqtt-connector","title":"Using the MQTT connector","text":"<p>To you the MQTT Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-mqtt&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-mqtt</code>.</p> <p>So, to indicate that a channel is managed by this connector you need: <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-mqtt\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-mqtt\n</code></pre></p>"},{"location":"mqtt/receiving-mqtt-messages/","title":"Receiving messages from MQTT","text":"<p>The MQTT Connector connects to a MQTT broker or router, and forward the messages to the Reactive Messaging application. It maps each of them into Reactive Messaging <code>Messages</code>.</p>"},{"location":"mqtt/receiving-mqtt-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have a MQTT server/broker running, and accessible using the <code>mqtt:1883</code> address (by default it would use <code>localhost:1883</code>). Configure your application to receive MQTT messages on the <code>prices</code> channel as follows:</p> <p><pre><code>mp.messaging.incoming.prices.connector=smallrye-mqtt # &lt;1&gt;\nmp.messaging.incoming.prices.host=mqtt # &lt;2&gt;\nmp.messaging.incoming.prices.port=1883 # &lt;3&gt;\n</code></pre> 1.  Sets the connector for the <code>prices</code> channel 2.  Configure the broker/server host name. 3.  Configure the broker/server port. 1883 is the default.</p> <p>Note</p> <p>You don\u2019t need to set the MQTT topic. By default, it uses the channel name (<code>prices</code>). You can configure the <code>topic</code> attribute to override it.</p> <p>Note</p> <p>It is generally recommended to set the <code>client-id</code>. By default, the connector is generating a unique <code>client-id</code>.</p> <p>Important</p> <p>Message coming from MQTT have a <code>byte[]</code> payload.</p> <p>Then, your application receives <code>Message&lt;byte[]&gt;</code>. You can consume the payload directly:</p> <pre><code>package mqtt.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class MqttPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(byte[] raw) {\n        double price = Double.parseDouble(new String(raw));\n\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;byte[]&gt;</code>:</p> <pre><code>package mqtt.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class MqttPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;byte[]&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message\n        return price.ack();\n    }\n\n}\n</code></pre> <p>The inbound topic can use the MQTT wildcards (<code>+</code> and <code>#</code>).</p>"},{"location":"mqtt/receiving-mqtt-messages/#deserialization","title":"Deserialization","text":"<p>The MQTT Connector does not handle the deserialization and creates a <code>Message&lt;byte[]&gt;</code>.</p>"},{"location":"mqtt/receiving-mqtt-messages/#inbound-metadata","title":"Inbound Metadata","text":"<p>The MQTT connector does not provide inbound metadata.</p>"},{"location":"mqtt/receiving-mqtt-messages/#failure-management","title":"Failure Management","text":"<p>If a message produced from a MQTT message is nacked, a failure strategy is applied. The MQTT connector supports 3 strategies:</p> <ul> <li> <p><code>fail</code> - fail the application, no more MQTT messages will be     processed. (default) The offset of the record that has not been     processed correctly is not committed.</p> </li> <li> <p><code>ignore</code> - the failure is logged, but the processing continue.</p> </li> </ul>"},{"location":"mqtt/receiving-mqtt-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default auto-clean-session Set to start with a clean session (<code>true</code> by default) boolean false <code>true</code> auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false <code>true</code> auto-keep-alive Set if the MQTT client must handle <code>PINGREQ</code> automatically boolean false <code>true</code> broadcast Whether or not the messages should be dispatched to multiple consumers boolean false <code>false</code> buffer-size The size buffer of incoming messages waiting to be processed int false <code>128</code> client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean (<code>io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions</code>) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false <code>60</code> failure-strategy Specify the failure strategy to apply when a message produced from a MQTT message is nacked. Values can be <code>fail</code> (default), or <code>ignore</code> string false <code>fail</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false <code>30</code> max-inflight-queue Set max count of unacknowledged messages int false <code>10</code> max-message-size Set max MQTT message size in bytes int false <code>8092</code> password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false <code>0</code> reconnect-interval-seconds Set the reconnect interval in seconds int false <code>1</code> server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false <code>false</code> ssl.hostname-verification-algorithm Set the hostname verifier algorithm for the TLS connection.Accepted values are <code>HTTPS</code>, <code>LDAPS</code>, and <code>NONE</code> (defaults). <code>NONE</code> disables the hostname verification. string false <code>NONE</code> ssl.keystore.location Set the keystore location. In case of <code>pem</code> type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of <code>pem</code> type this is the key path string false ssl.keystore.type Set the keystore type [<code>pkcs12</code>, <code>jks</code>, <code>pem</code>] string false <code>pkcs12</code> ssl.truststore.location Set the truststore location. In case of <code>pem</code> type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of <code>pem</code> type this is not necessary string false ssl.truststore.type Set the truststore type [<code>pkcs12</code>, <code>jks</code>, <code>pem</code>] string false <code>pkcs12</code> topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false <code>false</code> unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false <code>false</code> username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false <code>false</code> will-qos Set the QoS level for the will message int false <code>0</code> will-retain Set if the will message must be retained boolean false <code>false</code> <p>The MQTT connector is based on the Vert.x MQTT client. So you can pass any attribute supported by this client.</p> <p>Important</p> <p>A single instance of <code>MqttClient</code> and a single connection is used for each <code>host</code> / <code>port</code> / <code>server-name</code> / <code>client-id</code>. This client is reused for both the inbound and outbound connectors.</p> <p>Important</p> <p>Using <code>auto-clean-session=false</code> the MQTT Connector send Subscribe requests to the broken only if a Persistent Session is not present (like on the first  connection). This means that if a Session is already present (maybe for a  previous run) and you add a new incoming channel, this will not be subscribed. Beware to check always the subscription present on Broker when use  <code>auto-clean-session=false</code>.</p>"},{"location":"mqtt/sending-messages-to-mqtt/","title":"Sending messages to MQTT","text":"<p>The MQTT Connector can write Reactive Messaging <code>Messages</code> as MQTT Message.</p>"},{"location":"mqtt/sending-messages-to-mqtt/#example","title":"Example","text":"<p>Let\u2019s imagine you have a MQTT server/broker running, and accessible using the <code>mqtt:1883</code> address (by default it would use <code>localhost:1883</code>). Configure your application to write the messages from the <code>prices</code> channel into a MQTT Messages as follows:</p> <pre><code>mp.messaging.outgoing.prices.type=smallrye-mqtt\nmp.messaging.outgoing.prices.host=mqtt\nmp.messaging.outgoing.prices.port=1883\n</code></pre> <ol> <li>Sets the connector for the <code>prices</code> channel</li> <li>Configure the broker/server host name.</li> <li>Configure the broker/server port. 1883 is the default.</li> </ol> <p>Note</p> <p>You don\u2019t need to set the MQTT topic. By default, it uses the channel name (<code>prices</code>). You can configure the <code>topic</code> attribute to override it. NOTE: It is generally recommended to set the <code>client-id</code>. By default, the connector is generating a unique <code>client-id</code>.</p> <p>Then, your application must send <code>Message&lt;Double&gt;</code> to the <code>prices</code> channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package mqtt.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class MqttPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package mqtt.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class MqttPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble()));\n    }\n\n}\n</code></pre>"},{"location":"mqtt/sending-messages-to-mqtt/#serialization","title":"Serialization","text":"<p>The <code>Message</code> sent to MQTT can have various payload types:</p> <ul> <li> <p><code>JsonObject</code>:     JSON string encoded as <code>byte[]</code></p> </li> <li> <p><code>JsonArray</code>:     JSON string encoded as <code>byte[]</code></p> </li> <li> <p><code>java.lang.String</code> and Java primitive types: <code>toString</code> encoded as     <code>byte[]</code></p> </li> <li> <p><code>byte[]</code></p> </li> <li> <p>complex objects: The objects are encoded to JSON and passed as     <code>byte[]</code></p> </li> </ul>"},{"location":"mqtt/sending-messages-to-mqtt/#outbound-metadata","title":"Outbound Metadata","text":"<p>The MQTT connector does not provide outbound metadata.</p>"},{"location":"mqtt/sending-messages-to-mqtt/#acknowledgement","title":"Acknowledgement","text":"<p>MQTT acknowledgement depends on the QoS level. The message is acknowledged when the broker indicated the successful reception of the message (or immediately if the level of QoS does not support acknowledgment).</p> <p>If a MQTT message cannot be sent to the broker, the message is <code>nacked</code>.</p>"},{"location":"mqtt/sending-messages-to-mqtt/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default auto-clean-session Set to start with a clean session (<code>true</code> by default) boolean false <code>true</code> auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false <code>true</code> auto-keep-alive Set if the MQTT client must handle <code>PINGREQ</code> automatically boolean false <code>true</code> client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean (<code>io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions</code>) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false <code>60</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false <code>30</code> max-inflight-queue Set max count of unacknowledged messages int false <code>10</code> max-message-size Set max MQTT message size in bytes int false <code>8092</code> merge Whether the connector should allow multiple upstreams boolean false <code>false</code> password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false <code>0</code> reconnect-interval-seconds Set the reconnect interval in seconds int false <code>1</code> retain Whether the published message should be retained boolean false <code>false</code> server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false <code>false</code> ssl.hostname-verification-algorithm Set the hostname verifier algorithm for the TLS connection.Accepted values are <code>HTTPS</code>, <code>LDAPS</code>, and <code>NONE</code> (defaults). <code>NONE</code> disables the hostname verification. string false <code>NONE</code> ssl.keystore.location Set the keystore location. In case of <code>pem</code> type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of <code>pem</code> type this is the key path string false ssl.keystore.type Set the keystore type [<code>pkcs12</code>, <code>jks</code>, <code>pem</code>] string false <code>pkcs12</code> ssl.truststore.location Set the truststore location. In case of <code>pem</code> type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of <code>pem</code> type this is not necessary string false ssl.truststore.type Set the truststore type [<code>pkcs12</code>, <code>jks</code>, <code>pem</code>] string false <code>pkcs12</code> topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false <code>false</code> unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false <code>false</code> username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false <code>false</code> will-qos Set the QoS level for the will message int false <code>0</code> will-retain Set if the will message must be retained boolean false <code>false</code> <p>The MQTT connector is based on the Vert.x MQTT client. So you can pass any attribute supported by this client.</p> <p>Important</p> <p>A single instance of <code>MqttClient</code> and a single connection is used for each <code>host</code> / <code>port</code> / <code>server-name</code> / <code>client-id</code>. This client is reused for both the inbound and outbound connectors.</p>"},{"location":"pulsar/client-configuration/","title":"Configuring Pulsar clients, consumers and producers","text":"<p>Pulsar clients, consumers and producers are very customizable to configure how a Pulsar client application behaves.</p> <p>The Pulsar connector creates a Pulsar client and, a consumer or a producer per channel, each with sensible defaults to ease their configuration. Although the creation is handled, all available configuration options remain configurable through Pulsar channels.</p> <p>While idiomatic way of creating <code>PulsarClient</code>, <code>PulsarConsumer</code> or <code>PulsarProducer</code> are through builder APIs, in its essence those APIs build each time a configuration object, to pass onto the implementation. Those are ClientConfigurationData, ConsumerConfigurationData, ProducerConfigurationData.</p> <p>Pulsar Connector allows receiving properties for those configuration objects directly. For example, the broker authentication information for <code>PulsarClient</code> is received using <code>authPluginClassName</code> and <code>authParams</code> properties. In order to configure the authentication for the incoming channel <code>data</code> :</p> <pre><code>mp.messaging.incoming.data.connector=smallrye-pulsar\nmp.messaging.incoming.data.serviceUrl=pulsar://localhost:6650\nmp.messaging.incoming.data.topic=topic\nmp.messaging.incoming.data.subscriptionInitialPosition=Earliest\nmp.messaging.incoming.data.schema=INT32\nmp.messaging.incoming.data.authPluginClassName=org.apache.pulsar.client.impl.auth.AuthenticationBasic\nmp.messaging.incoming.data.authParams={\"userId\":\"superuser\",\"password\":\"admin\"}\n</code></pre> <p>Note that the Pulsar consumer property <code>subscriptionInitialPosition</code> is also configured with the <code>Earliest</code> value which represents with enum value <code>SubscriptionInitialPosition.Earliest</code>.</p> <p>This approach covers most of the configuration cases. However, non-serializable objects such as <code>CryptoKeyReader</code>, <code>ServiceUrlProvider</code> etc. cannot be configured this way. The Pulsar Connector allows taking into account instances of Pulsar configuration data objects \u2013 <code>ClientConfigurationData</code>, <code>ConsumerConfigurationData</code>, <code>ProducerConfigurationData</code>:</p> <pre><code>@Produces\n@Identifier(\"my-consumer-options\")\npublic ConsumerConfigurationData&lt;String&gt; getConsumerConfig() {\n    ConsumerConfigurationData&lt;String&gt; data = new ConsumerConfigurationData&lt;&gt;();\n    data.setAckReceiptEnabled(true);\n    data.setCryptoKeyReader(DefaultCryptoKeyReader.builder()\n            //...\n            .build());\n    return data;\n}\n</code></pre> <p>This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the <code>client-configuration</code>, <code>consumer-configuration</code> or <code>producer-configuration</code> attributes:</p> <pre><code>mp.messaging.incoming.prices.consumer-configuration=my-consumer-options\n</code></pre> <p>If no <code>[client|consumer|producer]-configuration</code> is configured, the connector will look for instances identified with the channel name:</p> <pre><code>@Produces\n@Identifier(\"prices\")\npublic ClientConfigurationData getClientConfig() {\n    ClientConfigurationData data = new ClientConfigurationData();\n    data.setEnableTransaction(true);\n    data.setServiceUrlProvider(AutoClusterFailover.builder()\n            // ...\n            .build());\n    return data;\n}\n</code></pre> <p>You also can provide a <code>Map&lt;String, Object&gt;</code> containing configuration values by key:</p> <pre><code>@Produces\n@Identifier(\"prices\")\npublic Map&lt;String, Object&gt; getProducerconfig() {\n    Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();\n    map.put(\"batcherBuilder\", BatcherBuilder.KEY_BASED);\n    map.put(\"sendTimeoutMs\", 3000);\n    map.put(\"customMessageRouter\", new PartialRoundRobinMessageRouterImpl(4));\n    return map;\n}\n</code></pre> <p>Different configuration sources are loaded in the following order of precedence, from the least important to the highest:</p> <ol> <li><code>Map&lt;String, Object&gt;</code> config map produced with default config identifier, <code>default-pulsar-client</code>, <code>default-pulsar-consumer</code>, <code>default-pulsar-producer</code>.</li> <li><code>Map&lt;String, Object&gt;</code> config map produced with identifier in the configuration or channel name</li> <li><code>[Client|Producer|Consuemr]ConfigurationData</code> object produced with identifier in the channel configuration or the channel name</li> <li>Channel configuration properties named with <code>[Client|Producer|Consuemr]ConfigurationData</code> field names.</li> </ol> <p>Following is the configuration reference for the <code>PulsarClient</code>.</p> <p>Corresponding sections list the Consumer Configuration Reference and Producer Configuration Reference. Configuration properties not configurable in configuration files (non-serializable) is noted in the column <code>Config file</code>.</p>"},{"location":"pulsar/client-configuration/#pulsarclient-configuration-reference","title":"<code>PulsarClient</code> Configuration Reference","text":"Attribute Description Type Config file Default serviceUrl Pulsar cluster HTTP URL to connect to a broker. String true serviceUrlProvider The implementation class of ServiceUrlProvider used to generate ServiceUrl. ServiceUrlProvider false authentication Authentication settings of the client. Authentication false authPluginClassName Class name of authentication plugin of the client. String true authParams Authentication parameter of the client. String true authParamMap Authentication map of the client. Map true operationTimeoutMs Client operation timeout (in milliseconds). long true 30000 lookupTimeoutMs Client lookup timeout (in milliseconds). long true -1 statsIntervalSeconds Interval to print client stats (in seconds). long true 60 numIoThreads Number of IO threads. int true 10 numListenerThreads Number of consumer listener threads. int true 10 connectionsPerBroker Number of connections established between the client and each Broker. A value of 0 means to disable connection pooling. int true 1 connectionMaxIdleSeconds Release the connection if it is not used for more than [connectionMaxIdleSeconds] seconds. If  [connectionMaxIdleSeconds] &lt; 0, disabled the feature that auto release the idle connections int true 180 useTcpNoDelay Whether to use TCP NoDelay option. boolean true true useTls Whether to use TLS. boolean true false tlsKeyFilePath Path to the TLS key file. String true tlsCertificateFilePath Path to the TLS certificate file. String true tlsTrustCertsFilePath Path to the trusted TLS certificate file. String true tlsAllowInsecureConnection Whether the client accepts untrusted TLS certificates from the broker. boolean true false tlsHostnameVerificationEnable Whether the hostname is validated when the client creates a TLS connection with brokers. boolean true false concurrentLookupRequest The number of concurrent lookup requests that can be sent on each broker connection. Setting a maximum prevents overloading a broker. int true 5000 maxLookupRequest Maximum number of lookup requests allowed on each broker connection to prevent overloading a broker. int true 50000 maxLookupRedirects Maximum times of redirected lookup requests. int true 20 maxNumberOfRejectedRequestPerConnection Maximum number of rejected requests of a broker in a certain time frame (60 seconds) after the current connection is closed and the client creating a new connection to connect to a different broker. int true 50 keepAliveIntervalSeconds Seconds of keeping alive interval for each client broker connection. int true 30 connectionTimeoutMs Duration of waiting for a connection to a broker to be established.If the duration passes without a response from a broker, the connection attempt is dropped. int true 10000 requestTimeoutMs Maximum duration for completing a request. int true 60000 readTimeoutMs Maximum read time of a request. int true 60000 autoCertRefreshSeconds Seconds of auto refreshing certificate. int true 300 initialBackoffIntervalNanos Initial backoff interval (in nanosecond). long true 100000000 maxBackoffIntervalNanos Max backoff interval (in nanosecond). long true 60000000000 enableBusyWait Whether to enable BusyWait for EpollEventLoopGroup. boolean true false listenerName Listener name for lookup. Clients can use listenerName to choose one of the listeners as the service URL to create a connection to the broker as long as the network is accessible.\"advertisedListeners\" must enabled in broker side. String true useKeyStoreTls Set TLS using KeyStore way. boolean true false sslProvider The TLS provider used by an internal client to authenticate with other Pulsar brokers. String true tlsKeyStoreType TLS KeyStore type configuration. String true JKS tlsKeyStorePath Path of TLS KeyStore. String true tlsKeyStorePassword Password of TLS KeyStore. String true tlsTrustStoreType TLS TrustStore type configuration. You need to set this configuration when client authentication is required. String true JKS tlsTrustStorePath Path of TLS TrustStore. String true tlsTrustStorePassword Password of TLS TrustStore. String true tlsCiphers Set of TLS Ciphers. Set true [] tlsProtocols Protocols of TLS. Set true [] memoryLimitBytes Limit of client memory usage (in byte). The 64M default can guarantee a high producer throughput. long true 67108864 proxyServiceUrl URL of proxy service. proxyServiceUrl and proxyProtocol must be mutually inclusive. String true proxyProtocol Protocol of proxy service. proxyServiceUrl and proxyProtocol must be mutually inclusive. ProxyProtocol true enableTransaction Whether to enable transaction. boolean true false clock Clock false dnsLookupBindAddress The Pulsar client dns lookup bind address, default behavior is bind on 0.0.0.0 String true dnsLookupBindPort The Pulsar client dns lookup bind port, takes effect when dnsLookupBindAddress is configured, default value is 0. int true 0 socks5ProxyAddress Address of SOCKS5 proxy. InetSocketAddress true socks5ProxyUsername User name of SOCKS5 proxy. String true socks5ProxyPassword Password of SOCKS5 proxy. String true description The extra description of the client version. The length cannot exceed 64. String true"},{"location":"pulsar/client-service/","title":"PulsarClientService","text":"<p>For advanced use cases, SmallRye Reactive Messaging provides a bean of type <code>PulsarClientService</code> that you can inject:</p> <pre><code>@Inject\nPulsarClientService pulsarClients;\n</code></pre> <p>From there, you can obtain <code>org.apache.pulsar.client.api.Consumer</code>, <code>org.apache.pulsar.client.api.Producer</code> and <code>org.apache.pulsar.client.api.PulsarClient</code> by channel name.</p> <p>Important</p> <p>Note that the <code>PulsarAdmin</code> is a separate library and the Pulsar connector doesn't provide it out-of-the-box. You'd need to explicitly add the dependency, i.e. <code>org.apache.pulsar:pulsar-client-admin-original</code> artifact.</p>"},{"location":"pulsar/health/","title":"Health reporting","text":"<p>The Pulsar connector reports the startup, readiness and liveness of each channel managed by the connector:</p> <ul> <li> <p>Startup &amp; Readiness For both inbound and outbound, probes report OK when the   connection with the broker is established.</p> </li> <li> <p>Liveness For both inbound and outbound, the liveness check verifies that the   connection is established AND that no failures have been caught.</p> </li> </ul> <p>Note</p> <p>To disable health reporting, set the <code>health-enabled</code> attribute for the channel to <code>false</code>.</p> <p>Note that a message processing failures nacks the message which is then handled by the failure-strategy. It is the responsibility of the failure-strategy to report the failure and influence the outcome of the liveness checks. The <code>fail</code> failure strategy reports the failure and so the liveness check will report the failure.</p>"},{"location":"pulsar/pulsar/","title":"Apache Pulsar Connector","text":"<p>The Pulsar connector adds support for Apache Pulsar to Reactive Messaging. With it you can consume Pulsar Messages as well as produce message into Pulsar.</p> <p>Apache Pulsar an open-source, distributed messaging and streaming platform built for the cloud. For more details about Pulsar, check the documentation.</p> <p>Pulsar implements the publish-subscribe pattern. Producers publish messages to topics. Consumers create subscriptions to those topics to receive and process incoming messages, and send acknowledgments to the broker when processing is finished.</p> <p>When a subscription is created, Pulsar retains all messages, even if the consumer is disconnected. The retained messages are discarded only when a consumer acknowledges that all these messages are processed successfully.</p> <p>A Pulsar message consists of</p> <ul> <li>value, payload data that message contains, while Pulsar messages contain payloads as unstructured byte array, a schema is applied to write and read with an enforced data structure</li> <li>key of type string, used for partitioning</li> <li>properties, optional key/value map</li> <li>topic, the topic that the message is published to</li> <li>producer name, the name of the producer who produces the message</li> <li>message ID, the ID assigned by bookies to a message as soon as the message is persistently stored</li> <li>sequence ID, the ID assigned by producers, indicating its order in that sequence</li> <li>publish time, timestamp automatically added by the producer</li> <li>event time, optional timestamp added by the application</li> </ul> <p>A Pulsar cluster consists of - One or more brokers, which are stateless components - A metadata store for maintaining topic metadata, schema, coordination and cluster configuration. By default, a Zookeeper cluster is used, except the standalone mode - A ensemble of bookies used for persistent storage of messages</p>"},{"location":"pulsar/pulsar/#using-the-pulsar-connector","title":"Using the Pulsar Connector","text":"<p>To use the Pulsar Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-pulsar&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-pulsar</code>.</p> <p>So, to indicate that a channel is managed by this connector you need:</p> <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-pulsar\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-pulsar\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/","title":"Receiving messages from Pulsar","text":"<p>The Pulsar Connector connects to a Pulsar broker using a Pulsar client and creates consumers to receive messages from Pulsar brokers and it maps each of them into Reactive Messaging <code>Messages</code>.</p>"},{"location":"pulsar/receiving-pulsar-messages/#example","title":"Example","text":"<p>Let\u2019s imagine you have a Pulsar broker running, and accessible using the <code>pulsar:6650</code> address (by default it would use <code>localhost:6650</code>). Configure your application to receive Pulsar messages on the <code>prices</code> channel as follows:</p> <pre><code>mp.messaging.incoming.prices.connector=smallrye-pulsar # &lt;1&gt;\nmp.messaging.incoming.prices.serviceUrl=pulsar://pulsar:6650 # &lt;2&gt;\nmp.messaging.incoming.prices.schema=DOUBLE # &lt;3&gt;\nmp.messaging.incoming.prices.subscriptionInitialPosition=Earliest # &lt;4&gt;\n</code></pre> <ol> <li>Sets the connector for the <code>prices</code> channel</li> <li>Configure the Pulsar broker service url.</li> <li>Configure the schema to consume prices as Double.</li> <li>Make sure consumer subscription starts receiving messages from the <code>Earliest</code> position.</li> </ol> <p>Note</p> <p>You don\u2019t need to set the Pulsar topic, nor the consumer name. By default, the connector uses the channel name (<code>prices</code>). You can configure the <code>topic</code> and <code>consumerName</code> attributes to override them.</p> <p>Note</p> <p>In Pulsar, consumers need to provide a <code>subscriptionName</code> for topic subscriptions. If not provided the connector is generating a unique subscription name.</p> <p>Then, your application can receive the <code>double</code> payload directly:</p> <pre><code>package pulsar.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class PulsarPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(double price) {\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;Double&gt;</code>:</p> <pre><code>package pulsar.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class PulsarPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;Double&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message\n        return price.ack();\n    }\n\n}\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/#consumer-configuration","title":"Consumer Configuration","text":"<p>The Pulsar Connector allows flexibly configuring the underlying Pulsar consumer. One of the ways is to set consumer properties directly on the channel configuration. The list of available configuration properties are listed in Configuration Reference.</p> <p>See the Configuring Pulsar consumers, producers and clients for more information.</p>"},{"location":"pulsar/receiving-pulsar-messages/#deserialization-and-pulsar-schema","title":"Deserialization and Pulsar Schema","text":"<p>The Pulsar Connector allows configuring Schema configuration for the underlying Pulsar consumer. See the Configuring the schema used for Pulsar channels for more information.</p>"},{"location":"pulsar/receiving-pulsar-messages/#inbound-metadata","title":"Inbound Metadata","text":"<p>The incoming Pulsar messages include an instance of <code>PulsarIncomingMessageMetadata</code> in the metadata. It provides the key, topic, partitions, headers and so on:</p> <pre><code>PulsarIncomingMessageMetadata metadata = incoming.getMetadata(PulsarIncomingMessageMetadata.class).orElse(null);\nif (metadata != null) {\n    // The topic\n    String topic = metadata.getTopicName();\n\n    // The key\n    String key = metadata.getKey();\n\n    // The event time\n    long timestamp = metadata.getEventTime();\n\n    // The raw data\n    byte[] rawData = metadata.getData();\n\n    // The underlying message\n    org.apache.pulsar.client.api.Message&lt;?&gt; message = metadata.getMessage();\n\n    // ...\n}\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/#acknowledgement","title":"Acknowledgement","text":"<p>When a message produced from a Pulsar Message is acknowledged, the connector sends an acknowledgement request to the Pulsar broker. All Reactive Messaging messages need to be acknowledged, which is handled automatically in most cases. Acknowledgement requests can be sent to the Pulsar broker using the following two strategies:</p> <ul> <li>Individual acknowledgement is the default strategy, an acknowledgement request is to the broker for each message.</li> <li>Cumulative acknowledgement, configured using <code>ack-strategy=cumulative</code>, the consumer only acknowledges the last message it received. All messages in the stream up to (and including) the provided message are not redelivered to that consumer.</li> </ul>"},{"location":"pulsar/receiving-pulsar-messages/#failure-management","title":"Failure Management","text":"<p>If a message produced from a Pulsar message is nacked, a failure strategy is applied. The Pulsar connector supports 4 strategies:</p> <ul> <li><code>nack</code> (default) sends negative acknowledgment to the broker, triggering the broker to redeliver this message to the consumer. The negative acknowledgment can be further configured using <code>negativeAckRedeliveryDelayMicros</code> and <code>negativeAck.redeliveryBackoff</code> properties.</li> <li><code>fail</code> fail the application, no more messages will be processed.</li> <li><code>ignore</code> the failure is logged, but the acknowledgement strategy will be applied and the processing will continue.</li> <li><code>continue</code> the failure is logged, but processing continues without applying acknowledgement or negative acknowledgement. This strategy can be used with acknowledgement timeout configuration.</li> <li><code>reconsume-later</code> sends the message to the retry letter topic using the <code>reconsumeLater</code> API to be reconsumed with a delay. The delay can be configured using the <code>reconsumeLater.delay</code> property and defaults to 3 seconds. Custom delay or properties per message can be configured by adding an instance of PulsarReconsumeLaterMetadata to the failure metadata.</li> </ul> <p>For example the following configuration for the incoming channel <code>data</code> uses <code>reconsumer-later</code> failure strategy with default delays of 60 seconds:</p> <pre><code>mp.messaging.incoming.data.connector=smallrye-pulsar\nmp.messaging.incoming.data.serviceUrl=pulsar://localhost:6650\nmp.messaging.incoming.data.topic=data\nmp.messaging.incoming.data.schema=INT32\nmp.messaging.incoming.data.failure-strategy=reconsume-later\nmp.messaging.incoming.data.retryEnable=true\nmp.messaging.incoming.data.reconsumeLater.delay=60 // in seconds\nmp.messaging.incoming.data.deadLetterPolicy.retryLetterTopic=data-retry\nmp.messaging.incoming.data.deadLetterPolicy.maxRedeliverCount=2\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/#acknowledgement-timeout","title":"Acknowledgement timeout","text":"<p>Similar to the negative acknowledgement, with the acknowledgment timeout mechanism, the Pulsar client tracks the unacknowledged messages, for the given ackTimeout period and sends redeliver unacknowledged messages request to the broker, thus the broker resends the unacknowledged messages to the consumer.</p> <p>To configure the timeout and redelivery backoff mechanism you can set <code>ackTimeoutMillis</code> and <code>ackTimeout.redeliveryBackoff</code> properties. The <code>ackTimeout.redeliveryBackoff</code> value accepts comma separated values of min delay in milliseconds, max delay in milliseconds and multiplier respectively:</p> <pre><code>mp.messaging.incoming.data.connector=smallrye-pulsar\nmp.messaging.incoming.data.failure-strategy=continue\nmp.messaging.incoming.data.ackTimeoutMillis=10000\nmp.messaging.incoming.data.ackTimeout.redeliveryBackoff=1000,60000,2\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/#dead-letter-topic","title":"Dead-letter topic","text":"<p>The dead letter topic pushes messages that are not consumed successfully to a dead letter topic an continue message consumption. Note that dead letter topic can be used in different message redelivery methods, such as acknowledgment timeout, negative acknowledgment or retry letter topic.</p> <pre><code>mp.messaging.incoming.data.connector=smallrye-pulsar\nmp.messaging.incoming.data.failure-strategy=nack\nmp.messaging.incoming.data.deadLetterPolicy.maxRedeliverCount=2\nmp.messaging.incoming.data.deadLetterPolicy.deadLetterTopic=my-dead-letter-topic\nmp.messaging.incoming.data.deadLetterPolicy.initialSubscriptionName=my-dlq-subscription\nmp.messaging.incoming.data.subscriptionType=Shared\n</code></pre> <p>Important</p> <p>Negative acknowledgment or acknowledgment timeout methods for redelivery will redeliver the whole batch of messages containing at least an unprocessed message. See producer batching for more information.</p>"},{"location":"pulsar/receiving-pulsar-messages/#receiving-pulsar-messages-in-batches","title":"Receiving Pulsar Messages in Batches","text":"<p>By default, incoming methods receive each Pulsar message individually. You can enable batch mode using <code>batchReceive=true</code> property, or setting a <code>batchReceivePolicy</code> in consumer configuration.</p> <pre><code>@Incoming(\"prices\")\npublic CompletionStage&lt;Void&gt; consumeMessage(Message&lt;List&lt;Double&gt;&gt; messages) {\n    messages.getMetadata(PulsarIncomingBatchMessageMetadata.class).ifPresent(metadata -&gt; {\n        for (org.apache.pulsar.client.api.Message&lt;Object&gt; message : metadata.getMessages()) {\n            String key = message.getKey();\n            String topic = message.getTopicName();\n            long timestamp = message.getEventTime();\n            //... process messages\n        }\n    });\n    // ack will commit the latest offsets (per partition) of the batch.\n    return messages.ack();\n}\n\n@Incoming(\"prices\")\npublic void consumeMessages(org.apache.pulsar.client.api.Messages&lt;Double&gt; messages) {\n    for (org.apache.pulsar.client.api.Message&lt;Double&gt; msg : messages) {\n        //... process messages\n    }\n}\n</code></pre> <p>Or you can directly receive the list of payloads to the consume method:</p> <pre><code>@Incoming(\"prices\")\npublic void consume(List&lt;Double&gt; prices) {\n    for (double price : prices) {\n        // process price\n    }\n}\n</code></pre>"},{"location":"pulsar/receiving-pulsar-messages/#accessing-metadata-of-batch-records","title":"Accessing metadata of batch records","text":"<p>When receiving records in batch mode, the metadata of each record is accessible through the <code>PulsarIncomingBatchMessageMetadata</code> :</p> <pre><code>@Incoming(\"prices\")\npublic void consumeMessages(org.apache.pulsar.client.api.Messages&lt;Double&gt; messages,\n        PulsarIncomingBatchMessageMetadata metadata) {\n    for (org.apache.pulsar.client.api.Message&lt;Double&gt; message : messages) {\n        TracingMetadata tracing = metadata.getMetadataForMessage(message, TracingMetadata.class);\n        if (tracing != null) {\n            tracing.getCurrentContext().makeCurrent();\n        }\n        //... process messages\n    }\n}\n</code></pre> <p>Like in this example, this can be useful to propagate the tracing information of each record.</p>"},{"location":"pulsar/receiving-pulsar-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default ack-strategy Specify the commit strategy to apply when a message produced from a record is acknowledged. Values can be <code>ack</code>, <code>cumulative</code>. string false <code>ack</code> ackTimeout.redeliveryBackoff Comma separated values for configuring ack timeout MultiplierRedeliveryBackoff, min delay, max delay, multiplier. string false batchReceive Whether batch receive is used to consume messages boolean false <code>false</code> client-configuration Identifier of a CDI bean that provides the default Pulsar client configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false consumer-configuration Identifier of a CDI bean that provides the default Pulsar consumer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false deadLetterPolicy.deadLetterTopic Name of the dead letter topic where the failing messages will be sent string false deadLetterPolicy.initialSubscriptionName Name of the initial subscription name of the dead letter topic string false deadLetterPolicy.maxRedeliverCount Maximum number of times that a message will be redelivered before being sent to the dead letter topic int false deadLetterPolicy.retryLetterTopic Name of the retry topic where the failing messages will be sent string false failure-strategy Specify the failure strategy to apply when a message produced from a record is acknowledged negatively (nack). Values can be <code>nack</code> (default), <code>fail</code>, <code>ignore</code> or <code>reconsume-later | string | false |</code>nack` health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> negativeAck.redeliveryBackoff Comma separated values for configuring negative ack MultiplierRedeliveryBackoff, min delay, max delay, multiplier. string false reconsumeLater.delay Default delay for reconsume failure-strategy, in seconds long false <code>3</code> schema The Pulsar schema type of this channel. When configured a schema is built with the given SchemaType and used for the channel. When absent, the schema is resolved searching for a CDI bean typed <code>Schema</code> qualified with <code>@Identifier</code> and the channel name. As a fallback AUTO_CONSUME or AUTO_PRODUCE are used. string false serviceUrl The service URL for the Pulsar service string false <code>pulsar://localhost:6650</code> topic The consumed / populated Pulsar topic. If not set, the channel name is used string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> <p>In addition to the configuration properties provided by the connector, following Pulsar consumer properties can also be set on the channel:</p> Attribute Description Type Config file Default topicNames Topic name Set true [] topicsPattern Topic pattern Pattern true subscriptionName Subscription name String true subscriptionType Subscription type.Four subscription types are available:* Exclusive* Failover* Shared* Key_Shared SubscriptionType true Exclusive subscriptionProperties Map true subscriptionMode SubscriptionMode true Durable messageListener MessageListener false consumerEventListener ConsumerEventListener false negativeAckRedeliveryBackoff Interface for custom message is negativeAcked policy. You can specify <code>RedeliveryBackoff</code> for a consumer. RedeliveryBackoff false ackTimeoutRedeliveryBackoff Interface for custom message is ackTimeout policy. You can specify <code>RedeliveryBackoff</code> for a consumer. RedeliveryBackoff false receiverQueueSize Size of a consumer's receiver queue.For example, the number of messages accumulated by a consumer before an application calls <code>Receive</code>.A value higher than the default value increases consumer throughput, though at the expense of more memory utilization. int true 1000 acknowledgementsGroupTimeMicros Group a consumer acknowledgment for a specified time.By default, a consumer uses 100ms grouping time to send out acknowledgments to a broker.Setting a group time of 0 sends out acknowledgments immediately.A longer ack group time is more efficient at the expense of a slight increase in message re-deliveries after a failure. long true 100000 maxAcknowledgmentGroupSize Group a consumer acknowledgment for the number of messages. int true 1000 negativeAckRedeliveryDelayMicros Delay to wait before redelivering messages that failed to be processed.When an application uses <code>Consumer#negativeAcknowledge(Message)</code>, failed messages are redelivered after a fixed timeout. long true 60000000 maxTotalReceiverQueueSizeAcrossPartitions The max total receiver queue size across partitions.This setting reduces the receiver queue size for individual partitions if the total receiver queue size exceeds this value. int true 50000 consumerName Consumer name String true ackTimeoutMillis Timeout of unacked messages long true 0 tickDurationMillis Granularity of the ack-timeout redelivery.Using an higher <code>tickDurationMillis</code> reduces the memory overhead to track messages when setting ack-timeout to a bigger value (for example, 1 hour). long true 1000 priorityLevel Priority level for a consumer to which a broker gives more priority while dispatching messages in Shared subscription type.The broker follows descending priorities. For example, 0=max-priority, 1, 2,...In Shared subscription type, the broker first dispatches messages to the max priority level consumers if they have permits. Otherwise, the broker considers next priority level consumers.Example 1If a subscription has consumerA with <code>priorityLevel</code> 0 and consumerB with <code>priorityLevel</code> 1, then the broker only dispatches messages to consumerA until it runs out permits and then starts dispatching messages to consumerB.Example 2Consumer Priority, Level, PermitsC1, 0, 2C2, 0, 1C3, 0, 1C4, 1, 2C5, 1, 1Order in which a broker dispatches messages to consumers is: C1, C2, C3, C1, C4, C5, C4. int true 0 maxPendingChunkedMessage The maximum size of a queue holding pending chunked messages. When the threshold is reached, the consumer drops pending messages to optimize memory utilization. int true 10 autoAckOldestChunkedMessageOnQueueFull Whether to automatically acknowledge pending chunked messages when the threshold of <code>maxPendingChunkedMessage</code> is reached. If set to <code>false</code>, these messages will be redelivered by their broker. boolean true false expireTimeOfIncompleteChunkedMessageMillis The time interval to expire incomplete chunks if a consumer fails to receive all the chunks in the specified time period. The default value is 1 minute. long true 60000 cryptoKeyReader CryptoKeyReader false messageCrypto MessageCrypto false cryptoFailureAction Consumer should take action when it receives a message that can not be decrypted.* FAIL: this is the default option to fail messages until crypto succeeds.* DISCARD:silently acknowledge and not deliver message to an application.* CONSUME: deliver encrypted messages to applications. It is the application's responsibility to decrypt the message.The decompression of message fails.If messages contain batch messages, a client is not be able to retrieve individual messages in batch.Delivered encrypted message contains <code>EncryptionContext</code> which contains encryption and compression information in it using which application can decrypt consumed message payload. ConsumerCryptoFailureAction true FAIL properties A name or value property of this consumer.<code>properties</code> is application defined metadata attached to a consumer.When getting a topic stats, associate this metadata with the consumer stats for easier identification. SortedMap true {} readCompacted If enabling <code>readCompacted</code>, a consumer reads messages from a compacted topic rather than reading a full message backlog of a topic.A consumer only sees the latest value for each key in the compacted topic, up until reaching the point in the topic message when compacting backlog. Beyond that point, send messages as normal.Only enabling <code>readCompacted</code> on subscriptions to persistent topics, which have a single active consumer (like failure or exclusive subscriptions).Attempting to enable it on subscriptions to non-persistent topics or on shared subscriptions leads to a subscription call throwing a <code>PulsarClientException</code>. boolean true false subscriptionInitialPosition Initial position at which to set cursor when subscribing to a topic at first time. SubscriptionInitialPosition true Latest patternAutoDiscoveryPeriod Topic auto discovery period when using a pattern for topic's consumer.The default and minimum value is 1 minute. int true 60 regexSubscriptionMode When subscribing to a topic using a regular expression, you can pick a certain type of topics.* PersistentOnly: only subscribe to persistent topics.* NonPersistentOnly: only subscribe to non-persistent topics.* AllTopics: subscribe to both persistent and non-persistent topics. RegexSubscriptionMode true PersistentOnly deadLetterPolicy Dead letter policy for consumers.By default, some messages are probably redelivered many times, even to the extent that it never stops.By using the dead letter mechanism, messages have the max redelivery count. When exceeding the maximum number of redeliveries, messages are sent to the Dead Letter Topic and acknowledged automatically.You can enable the dead letter mechanism by setting <code>deadLetterPolicy</code>.Example<code>client.newConsumer().deadLetterPolicy(DeadLetterPolicy.builder().maxRedeliverCount(10).build()).subscribe();</code>Default dead letter topic name is <code>{TopicName}-{Subscription}-DLQ</code>.To set a custom dead letter topic name:<code>client.newConsumer().deadLetterPolicy(DeadLetterPolicy.builder().maxRedeliverCount(10).deadLetterTopic(\"your-topic-name\").build()).subscribe();</code>When specifying the dead letter policy while not specifying <code>ackTimeoutMillis</code>, you can set the ack timeout to 30000 millisecond. DeadLetterPolicy true retryEnable boolean true false batchReceivePolicy BatchReceivePolicy false autoUpdatePartitions If <code>autoUpdatePartitions</code> is enabled, a consumer subscribes to partition increasement automatically.Note: this is only for partitioned consumers. boolean true true autoUpdatePartitionsIntervalSeconds long true 60 replicateSubscriptionState If <code>replicateSubscriptionState</code> is enabled, a subscription state is replicated to geo-replicated clusters. boolean true false resetIncludeHead boolean true false keySharedPolicy KeySharedPolicy false batchIndexAckEnabled boolean true false ackReceiptEnabled boolean true false poolMessages boolean true false payloadProcessor MessagePayloadProcessor false startPaused boolean true false autoScaledReceiverQueueSizeEnabled boolean true false topicConfigurations List true []"},{"location":"pulsar/schema-configuration/","title":"Configuring the schema used for Pulsar channels","text":"<p>Pulsar messages are stored with payloads as unstructured byte array. A Pulsar schema defines how to serialize structured data to the raw message bytes The schema is applied in producers and consumers to write and read with an enforced data structure. It serializes data into raw bytes before they are published to a topic and deserializes the raw bytes before they are delivered to consumers.</p> <p>Pulsar uses a schema registry as a central repository to store the registered schema information, which enables producers/consumers to coordinate the schema of a topic's messages through brokers. By default the Apache BookKeeper is used to store schemas.</p> <p>Pulsar API provides built-in schema information for a number of primitive types and complex types such as Key/Value, Avro and Protobuf.</p> <p>The Pulsar Connector allows specifying the schema as a primitive type using the <code>schema</code> property:</p> <pre><code>mp.messaging.incoming.prices.connector=smallrye-pulsar\nmp.messaging.incoming.prices.schema=INT32\n\nmp.messaging.outgoing.prices-out.connector=smallrye-pulsar\nmp.messaging.outgoing.prices-out.schema=DOUBLE\n</code></pre> <p>If the value for the <code>schema</code> property matches a Schema Type a simple schema will be created with that type and will be used for that channel.</p> <p>The Pulsar Connector allows configuring complex schema types by providing <code>Schema</code> beans through CDI, identified with the <code>@Identifier</code> qualifier.</p> <p>For example the following bean provides an Avro schema and a Key/Value schema:</p> <pre><code>package pulsar.configuration;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.enterprise.inject.Produces;\n\nimport org.apache.pulsar.client.api.Schema;\nimport org.apache.pulsar.common.schema.KeyValue;\nimport org.apache.pulsar.common.schema.KeyValueEncodingType;\n\nimport io.smallrye.common.annotation.Identifier;\n\n@ApplicationScoped\npublic class PulsarSchemaProvider {\n\n    @Produces\n    @Identifier(\"user-schema\")\n    Schema&lt;User&gt; userSchema = Schema.AVRO(User.class);\n\n    @Produces\n    @Identifier(\"a-channel\")\n    Schema&lt;KeyValue&lt;Integer, User&gt;&gt; keyValueSchema() {\n        return Schema.KeyValue(Schema.INT32, Schema.JSON(User.class), KeyValueEncodingType.SEPARATED);\n    }\n\n    public static class User {\n        String name;\n        int age;\n\n    }\n}\n</code></pre> <p>To configure the incoming channel <code>users</code> with defined schema, you need to set the <code>schema</code> property to the identifier of the schema <code>user-schema</code>:</p> <pre><code>mp.messaging.incoming.users.connector=smallrye-pulsar\nmp.messaging.incoming.users.schema=user-schema\n</code></pre> <p>If no <code>schema</code> property is found, the connector looks for <code>Schema</code> beans identified with the channel name. For example, the outgoing channel <code>a-channel</code> will use the key/value schema.</p> <pre><code>mp.messaging.outgoing.a-channel.connector=smallrye-pulsar\n</code></pre> <p>If no schema information is provided incoming channels will use <code>Schema.AUTO_CONSUME()</code>, whereas outgoing channels will use <code>Schema.AUTO_PRODUCE_BYTES()</code> schemas.</p>"},{"location":"pulsar/sending-messages-to-pulsar/","title":"Sending messages to Pulsar","text":"<p>The Pulsar Connector can write Reactive Messaging <code>Message</code>s as Pulsar Message.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#example","title":"Example","text":"<p>Let\u2019s imagine you have a Pulsar broker running, and accessible using the <code>pulsar:6650</code> address (by default it would use <code>localhost:1883</code>). Configure your application to write the messages from the <code>prices</code> channel into a Pulsar Messages as follows:</p> <pre><code>mp.messaging.outgoing.prices.connector=smallrye-pulsar # &lt;1&gt;\nmp.messaging.outgoing.prices.serviceUrl=pulsar://pulsar:6650 # &lt;2&gt;\nmp.messaging.outgoing.prices.schema=DOUBLE # &lt;3&gt;\n</code></pre> <ol> <li>Sets the connector for the <code>prices</code> channel</li> <li>Configure the Pulsar broker service url.</li> <li>Configure the schema to consume prices as Double.</li> </ol> <p>Note</p> <p>You don\u2019t need to set the Pulsar topic, nor the producer name. By default, the connector uses the channel name (<code>prices</code>). You can configure the <code>topic</code> and <code>producerName</code> attributes to override them.</p> <p>Then, your application must send <code>Message&lt;Double&gt;</code> to the <code>prices</code> channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package pulsar.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class PulsarPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>:</p> <pre><code>package pulsar.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class PulsarPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble()));\n    }\n\n}\n</code></pre>"},{"location":"pulsar/sending-messages-to-pulsar/#producer-configuration","title":"Producer Configuration","text":"<p>The Pulsar Connector allows flexibly configuring the underlying Pulsar producer. One of the ways is to set producer properties directly on the channel configuration. The list of available configuration properties are listed in Configuration Reference.</p> <p>See the Configuring Pulsar consumers, producers and clients for more information.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#serialization-and-pulsar-schema","title":"Serialization and Pulsar Schema","text":"<p>The Pulsar Connector allows configuring Schema configuration for the underlying Pulsar producer. See the Configuring the schema used for Pulsar channels for more information.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#sending-keyvalue-pairs","title":"Sending key/value pairs","text":"<p>In order to send Kev/Value pairs to Pulsar, you can configure the Pulsar producer Schema with a org.apache.pulsar.common.schema.KeyValue type:</p> <pre><code>@Identifier(\"out\")\n@Produces\nSchema&lt;KeyValue&lt;String, Long&gt;&gt; schema = Schema.KeyValue(Schema.STRING, Schema.INT64);\n\n@Incoming(\"in\")\n@Outgoing(\"out\")\npublic KeyValue&lt;String, Long&gt; process(long in) {\n    return new KeyValue&lt;&gt;(\"my-key\", in);\n}\n</code></pre> <p>If you need more control on the written records, use <code>PulsarOutgoingMessageMetadata</code>.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Message</code>s, you can add an instance of PulsarOutgoingMessageMetadata to influence how the message is going to be written to Pulsar. For example, configure the record key, and set message properties:</p> <pre><code>// Creates an PulsarOutgoingMessageMetadata\n// The type parameter is the type of the record's key\nPulsarOutgoingMessageMetadata metadata = PulsarOutgoingMessageMetadata.builder()\n        .withKey(\"my-key\")\n        .withProperties(Map.of(\"property-key\", \"value\"))\n        .build();\n\n// Create a new message from the `incoming` message\n// Add `metadata` to the metadata from the `incoming` message.\nreturn incoming.addMetadata(metadata);\n</code></pre>"},{"location":"pulsar/sending-messages-to-pulsar/#outgoingmessage","title":"OutgoingMessage","text":"<p>Using OutgoingMessage, is an easy way of customizing the Pulsar message to be published when dealing with payloads and not <code>Message</code>s.</p> <p>You can create an <code>OutgoingMessage</code> with key and value, or from an incoming Pulsar Message:</p> <pre><code>@Incoming(\"in\")\n@Outgoing(\"out\")\nOutgoingMessage&lt;Long&gt; process(org.apache.pulsar.client.api.Message&lt;String&gt; in) {\n    return OutgoingMessage.from(in)\n            .withValue(Long.valueOf(in.getValue()));\n}\n</code></pre>"},{"location":"pulsar/sending-messages-to-pulsar/#acknowledgement","title":"Acknowledgement","text":"<p>Upon receiving a message from a Producer, a Pulsar broker assigns a <code>MessageId</code> to the message and sends it back to the producer, confirming that the message is published.</p> <p>By default, the connector does wait for Pulsar to acknowledge the record to continue the processing (acknowledging the received <code>Message</code>). You can disable this by setting the <code>waitForWriteCompletion</code> attribute to <code>false</code>.</p> <p>If a record cannot be written, the message is <code>nacked</code>.</p> <p>Important</p> <p>The Pulsar client automatically retries sending messages in case of failure, until the send timeout is reached. The send timeout is configurable with <code>sendTimeoutMs</code> attribute and by default is is 30 seconds.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#back-pressure-and-inflight-records","title":"Back-pressure and inflight records","text":"<p>The Pulsar outbound connector handles back-pressure monitoring the number of pending messages waiting to be written to the Pulsar broker. The number of pending messages is configured using the <code>max-inflight-messages</code> attribute and defaults to 1000.</p> <p>The connector only sends that amount of messages concurrently. No other messages will be sent until at least one pending message gets acknowledged by the broker. Then, the connector writes a new message to Pulsar when one of the broker\u2019s pending messages get acknowledged.</p> <p>The Pulsar producer also has a limit on the number of pending messages controlled by <code>maxPendingMessages</code> and for partitioned topics <code>maxPendingMessagesAcrossPartitions</code> producer configuration properties. The default value for the both is <code>0</code> which means the limit is enforced by the connector and not the underlying producer. If a <code>maxPendingMessagesAcrossPartitions</code> is set and subscribed topic is partitioned, the connector will set the <code>max-inflight-messages</code> use the producer's limit.</p> <p>Deprecation</p> <p>Previously a single connector attribute <code>maxPendingMessages</code> controlled both the connector back-pressure and the Pulsar producer configuration. The <code>max-inflight-messages</code> attribute is not the same as the Pulsar producer <code>maxPendingMessages</code> configuration property.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#producer-batching","title":"Producer Batching","text":"<p>By default, the Pulsar producer batches individual messages together to be published to the broker. You can configure batching parameters using <code>batchingMaxPublishDelayMicros</code>, <code>batchingPartitionSwitchFrequencyByPublishDelay</code>, <code>batchingMaxMessages</code>, <code>batchingMaxBytes</code> configuration properties, or disable it completely with <code>batchingEnabled=false</code>.</p> <p>When using <code>Key_Shared</code> consumer subscriptions, the <code>batcherBuilder</code> can be configured to <code>BatcherBuilder.KEY_BASED</code>.</p>"},{"location":"pulsar/sending-messages-to-pulsar/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default client-configuration Identifier of a CDI bean that provides the default Pulsar client configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> max-inflight-messages The maximum size of a queue holding pending messages, i.e messages waiting to receive an acknowledgment from a broker. Defaults to 1000 messages int false producer-configuration Identifier of a CDI bean that provides the default Pulsar producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false schema The Pulsar schema type of this channel. When configured a schema is built with the given SchemaType and used for the channel. When absent, the schema is resolved searching for a CDI bean typed <code>Schema</code> qualified with <code>@Identifier</code> and the channel name. As a fallback AUTO_CONSUME or AUTO_PRODUCE are used. string false serviceUrl The service URL for the Pulsar service string false <code>pulsar://localhost:6650</code> topic The consumed / populated Pulsar topic. If not set, the channel name is used string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> waitForWriteCompletion Whether the client waits for the broker to acknowledge the written record before acknowledging the message boolean false <code>true</code> <p>In addition to the configuration properties provided by the connector, following Pulsar producer properties can also be set on the channel:</p> Attribute Description Type Config file Default topicName Topic name String true producerName Producer name String true sendTimeoutMs Message send timeout in ms.If a message is not acknowledged by a server before the <code>sendTimeout</code> expires, an error occurs. long true 30000 blockIfQueueFull If it is set to <code>true</code>, when the outgoing message queue is full, the <code>Send</code> and <code>SendAsync</code> methods of producer block, rather than failing and throwing errors.If it is set to <code>false</code>, when the outgoing message queue is full, the <code>Send</code> and <code>SendAsync</code> methods of producer fail and <code>ProducerQueueIsFullError</code> exceptions occur.The <code>MaxPendingMessages</code> parameter determines the size of the outgoing message queue. boolean true false maxPendingMessages The maximum size of a queue holding pending messages.For example, a message waiting to receive an acknowledgment from a broker.By default, when the queue is full, all calls to the <code>Send</code> and <code>SendAsync</code> methods fail unless you set <code>BlockIfQueueFull</code> to <code>true</code>. int true 0 maxPendingMessagesAcrossPartitions The maximum number of pending messages across partitions.Use the setting to lower the max pending messages for each partition (<code>#setMaxPendingMessages(int)</code>) if the total number exceeds the configured value. int true 0 messageRoutingMode Message routing logic for producers on partitioned topics.Apply the logic only when setting no key on messages.Available options are as follows:* <code>pulsar.RoundRobinDistribution</code>: round robin* <code>pulsar.UseSinglePartition</code>: publish all messages to a single partition* <code>pulsar.CustomPartition</code>: a custom partitioning scheme MessageRoutingMode true hashingScheme Hashing function determining the partition where you publish a particular message (partitioned topics only).Available options are as follows:* <code>pulsar.JavastringHash</code>: the equivalent of <code>string.hashCode()</code> in Java* <code>pulsar.Murmur3_32Hash</code>: applies the Murmur3 hashing function* <code>pulsar.BoostHash</code>: applies the hashing function from C++'sBoost library HashingScheme true JavaStringHash cryptoFailureAction Producer should take action when encryption fails.* FAIL: if encryption fails, unencrypted messages fail to send.* SEND: if encryption fails, unencrypted messages are sent. ProducerCryptoFailureAction true FAIL customMessageRouter MessageRouter false batchingMaxPublishDelayMicros Batching time period of sending messages. long true 1000 batchingPartitionSwitchFrequencyByPublishDelay int true 10 batchingMaxMessages The maximum number of messages permitted in a batch. int true 1000 batchingMaxBytes int true 131072 batchingEnabled Enable batching of messages. boolean true true batcherBuilder BatcherBuilder false chunkingEnabled Enable chunking of messages. boolean true false chunkMaxMessageSize int true -1 cryptoKeyReader CryptoKeyReader false messageCrypto MessageCrypto false encryptionKeys Set true [] compressionType Message data compression type used by a producer.Available options:* LZ4* ZLIB* ZSTD* SNAPPY CompressionType true NONE initialSequenceId Long true autoUpdatePartitions boolean true true autoUpdatePartitionsIntervalSeconds long true 60 multiSchema boolean true true accessMode ProducerAccessMode true Shared lazyStartPartitionedProducers boolean true false properties SortedMap true {} initialSubscriptionName Use this configuration to automatically create an initial subscription when creating a topic. If this field is not set, the initial subscription is not created. String true"},{"location":"pulsar/transactions/","title":"Pulsar Transactions and Exactly-Once Processing","text":"<p>Pulsar transactions enable event streaming applications to consume, process, and produce messages in one atomic operation.</p> <p>Transactions allow one or multiple producers to send batch of messages to multiple topics where all messages in the batch are eventually visible to any consumer, or none is ever visible to consumers.</p> <p>In order to be used, transaction support needs to be activated on the broker configuration: <pre><code># used to enable transaction coordinator\ntransactionCoordinatorEnabled=true\n\n# used to create systemTopic used for transaction buffer snapshot\nsystemTopicEnabled=true\n</code></pre></p> <p>On the client side, the transaction support also needs to be enabled on <code>PulsarClient</code> configuration:</p> <pre><code>mp.messaging.outgoing.tx-producer.enableTransaction=true\n</code></pre> <p>Pulsar connector provides <code>PulsarTransactions</code> custom emitter for writing records inside a transaction.</p> <p>It can be used as a regular emitter <code>@Channel</code>:</p> <pre><code>package pulsar.outbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.pulsar.OutgoingMessage;\nimport io.smallrye.reactive.messaging.pulsar.transactions.PulsarTransactions;\n\n@ApplicationScoped\npublic class PulsarTransactionalProducer {\n\n    @Inject\n    @Channel(\"tx-out-example\")\n    PulsarTransactions&lt;OutgoingMessage&lt;Integer&gt;&gt; txProducer;\n\n    @Inject\n    @Channel(\"other-producer\")\n    PulsarTransactions&lt;String&gt; producer;\n\n    @Incoming(\"in\")\n    public Uni&lt;Void&gt; emitInTransaction(Message&lt;Integer&gt; in) {\n        return txProducer.withTransaction(emitter -&gt; {\n            emitter.send(OutgoingMessage.of(\"a\", 1));\n            emitter.send(OutgoingMessage.of(\"b\", 2));\n            emitter.send(OutgoingMessage.of(\"c\", 3));\n            producer.send(emitter, \"4\");\n            producer.send(emitter, \"5\");\n            producer.send(emitter, \"6\");\n            return Uni.createFrom().completionStage(in::ack);\n        });\n    }\n\n}\n</code></pre> <p>The function given to the <code>withTransaction</code> method receives a <code>TransactionalEmitter</code> for producing records, and returns a <code>Uni</code> that provides the result of the transaction. If the processing completes successfully, the producer is flushed and the transaction is committed. If the processing throws an exception, returns a failing <code>Uni</code>, or marks the <code>TransactionalEmitter</code> for abort, the transaction is aborted.</p> <p>Note</p> <p>Multiple transactional producers can participate in a single transaction. This ensures all messages are sent using the started transaction and before the transaction is committed, all participating producers are flushed.</p> <p>If this method is called on a Vert.x context, the processing function is also called on that context. Otherwise, it is called on the sending thread of the producer.</p>"},{"location":"pulsar/transactions/#exactly-once-processing","title":"Exactly-Once Processing","text":"<p>Pulsar Transactions API also allows managing consumer offsets inside a transaction, together with produced messages. This in turn enables coupling a consumer with a transactional producer in a consume-transform-produce pattern, also known as exactly-once processing. It means that an application consumes messages, processes them, publishes the results to a topic, and commits offsets of the consumed messages in a transaction.</p> <p>The <code>PulsarTransactions</code> emitter also provides a way to apply exactly-once processing to an incoming Pulsar message inside a transaction.</p> <p>The following example includes a batch of Pulsar messages inside a transaction.</p> <pre><code>    mp.messaging.outgoing.tx-out-example.enableTransaction=true\n    # ...\n    mp.messaging.incoming.in-channel.enableTransaction=true\n    mp.messaging.incoming.in-channel.batchReceive=true\n</code></pre> <pre><code>package pulsar.outbound;\n\nimport java.util.List;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.inject.Inject;\n\nimport org.eclipse.microprofile.reactive.messaging.Channel;\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.pulsar.PulsarIncomingBatchMessageMetadata;\nimport io.smallrye.reactive.messaging.pulsar.PulsarMessage;\nimport io.smallrye.reactive.messaging.pulsar.transactions.PulsarTransactions;\n\n@ApplicationScoped\npublic class PulsarExactlyOnceProcessor {\n\n    @Inject\n    @Channel(\"tx-out-example\")\n    PulsarTransactions&lt;Integer&gt; txProducer;\n\n    @Incoming(\"in-channel\")\n    public Uni&lt;Void&gt; emitInTransaction(Message&lt;List&lt;Integer&gt;&gt; batch) {\n        return txProducer.withTransactionAndAck(batch, emitter -&gt; {\n            PulsarIncomingBatchMessageMetadata metadata = batch.getMetadata(PulsarIncomingBatchMessageMetadata.class).get();\n            for (org.apache.pulsar.client.api.Message&lt;Integer&gt; message : metadata.&lt;Integer&gt; getMessages()) {\n                emitter.send(PulsarMessage.of(message.getValue() + 1, message.getKey()));\n            }\n            return Uni.createFrom().voidItem();\n        });\n    }\n\n}\n</code></pre> <p>If the processing completes successfully, the message is acknowledged inside the transaction and the transaction is committed.</p> <p>Important</p> <p>When using exactly-once processing, messages can only be acked individually rather than cumulatively.</p> <p>If the processing needs to abort, the message is nack'ed. One of the failure strategies can be employed in order to retry the processing or simply fail-stop. Note that the <code>Uni</code> returned from the <code>#withTransaction</code> will yield a failure if the transaction fails and is aborted.</p> <p>The application can choose to handle the error case, but for the message consumption to continue, <code>Uni</code> returned from the <code>@Incoming</code> method must not result in failure. <code>PulsarTransactions#withTransactionAndAck</code> method will ack and nack the message but will not stop the reactive stream. Ignoring the failure simply resets the consumer to the last committed offsets and resumes the processing from there.</p>"},{"location":"pulsar/config/smallrye-pulsar-client/","title":"Smallrye pulsar client","text":"Attribute Description Type Config file Default serviceUrl Pulsar cluster HTTP URL to connect to a broker. String true serviceUrlProvider The implementation class of ServiceUrlProvider used to generate ServiceUrl. ServiceUrlProvider false authentication Authentication settings of the client. Authentication false authPluginClassName Class name of authentication plugin of the client. String true authParams Authentication parameter of the client. String true authParamMap Authentication map of the client. Map true operationTimeoutMs Client operation timeout (in milliseconds). long true 30000 lookupTimeoutMs Client lookup timeout (in milliseconds). long true -1 statsIntervalSeconds Interval to print client stats (in seconds). long true 60 numIoThreads Number of IO threads. int true 10 numListenerThreads Number of consumer listener threads. int true 10 connectionsPerBroker Number of connections established between the client and each Broker. A value of 0 means to disable connection pooling. int true 1 connectionMaxIdleSeconds Release the connection if it is not used for more than [connectionMaxIdleSeconds] seconds. If  [connectionMaxIdleSeconds] &lt; 0, disabled the feature that auto release the idle connections int true 180 useTcpNoDelay Whether to use TCP NoDelay option. boolean true true useTls Whether to use TLS. boolean true false tlsKeyFilePath Path to the TLS key file. String true tlsCertificateFilePath Path to the TLS certificate file. String true tlsTrustCertsFilePath Path to the trusted TLS certificate file. String true tlsAllowInsecureConnection Whether the client accepts untrusted TLS certificates from the broker. boolean true false tlsHostnameVerificationEnable Whether the hostname is validated when the client creates a TLS connection with brokers. boolean true false concurrentLookupRequest The number of concurrent lookup requests that can be sent on each broker connection. Setting a maximum prevents overloading a broker. int true 5000 maxLookupRequest Maximum number of lookup requests allowed on each broker connection to prevent overloading a broker. int true 50000 maxLookupRedirects Maximum times of redirected lookup requests. int true 20 maxNumberOfRejectedRequestPerConnection Maximum number of rejected requests of a broker in a certain time frame (60 seconds) after the current connection is closed and the client creating a new connection to connect to a different broker. int true 50 keepAliveIntervalSeconds Seconds of keeping alive interval for each client broker connection. int true 30 connectionTimeoutMs Duration of waiting for a connection to a broker to be established.If the duration passes without a response from a broker, the connection attempt is dropped. int true 10000 requestTimeoutMs Maximum duration for completing a request. int true 60000 readTimeoutMs Maximum read time of a request. int true 60000 autoCertRefreshSeconds Seconds of auto refreshing certificate. int true 300 initialBackoffIntervalNanos Initial backoff interval (in nanosecond). long true 100000000 maxBackoffIntervalNanos Max backoff interval (in nanosecond). long true 60000000000 enableBusyWait Whether to enable BusyWait for EpollEventLoopGroup. boolean true false listenerName Listener name for lookup. Clients can use listenerName to choose one of the listeners as the service URL to create a connection to the broker as long as the network is accessible.\"advertisedListeners\" must enabled in broker side. String true useKeyStoreTls Set TLS using KeyStore way. boolean true false sslProvider The TLS provider used by an internal client to authenticate with other Pulsar brokers. String true tlsKeyStoreType TLS KeyStore type configuration. String true JKS tlsKeyStorePath Path of TLS KeyStore. String true tlsKeyStorePassword Password of TLS KeyStore. String true tlsTrustStoreType TLS TrustStore type configuration. You need to set this configuration when client authentication is required. String true JKS tlsTrustStorePath Path of TLS TrustStore. String true tlsTrustStorePassword Password of TLS TrustStore. String true tlsCiphers Set of TLS Ciphers. Set true [] tlsProtocols Protocols of TLS. Set true [] memoryLimitBytes Limit of client memory usage (in byte). The 64M default can guarantee a high producer throughput. long true 67108864 proxyServiceUrl URL of proxy service. proxyServiceUrl and proxyProtocol must be mutually inclusive. String true proxyProtocol Protocol of proxy service. proxyServiceUrl and proxyProtocol must be mutually inclusive. ProxyProtocol true enableTransaction Whether to enable transaction. boolean true false clock Clock false dnsLookupBindAddress The Pulsar client dns lookup bind address, default behavior is bind on 0.0.0.0 String true dnsLookupBindPort The Pulsar client dns lookup bind port, takes effect when dnsLookupBindAddress is configured, default value is 0. int true 0 socks5ProxyAddress Address of SOCKS5 proxy. InetSocketAddress true socks5ProxyUsername User name of SOCKS5 proxy. String true socks5ProxyPassword Password of SOCKS5 proxy. String true description The extra description of the client version. The length cannot exceed 64. String true"},{"location":"pulsar/config/smallrye-pulsar-consumer/","title":"Smallrye pulsar consumer","text":"Attribute Description Type Config file Default topicNames Topic name Set true [] topicsPattern Topic pattern Pattern true subscriptionName Subscription name String true subscriptionType Subscription type.Four subscription types are available:* Exclusive* Failover* Shared* Key_Shared SubscriptionType true Exclusive subscriptionProperties Map true subscriptionMode SubscriptionMode true Durable messageListener MessageListener false consumerEventListener ConsumerEventListener false negativeAckRedeliveryBackoff Interface for custom message is negativeAcked policy. You can specify <code>RedeliveryBackoff</code> for a consumer. RedeliveryBackoff false ackTimeoutRedeliveryBackoff Interface for custom message is ackTimeout policy. You can specify <code>RedeliveryBackoff</code> for a consumer. RedeliveryBackoff false receiverQueueSize Size of a consumer's receiver queue.For example, the number of messages accumulated by a consumer before an application calls <code>Receive</code>.A value higher than the default value increases consumer throughput, though at the expense of more memory utilization. int true 1000 acknowledgementsGroupTimeMicros Group a consumer acknowledgment for a specified time.By default, a consumer uses 100ms grouping time to send out acknowledgments to a broker.Setting a group time of 0 sends out acknowledgments immediately.A longer ack group time is more efficient at the expense of a slight increase in message re-deliveries after a failure. long true 100000 maxAcknowledgmentGroupSize Group a consumer acknowledgment for the number of messages. int true 1000 negativeAckRedeliveryDelayMicros Delay to wait before redelivering messages that failed to be processed.When an application uses <code>Consumer#negativeAcknowledge(Message)</code>, failed messages are redelivered after a fixed timeout. long true 60000000 maxTotalReceiverQueueSizeAcrossPartitions The max total receiver queue size across partitions.This setting reduces the receiver queue size for individual partitions if the total receiver queue size exceeds this value. int true 50000 consumerName Consumer name String true ackTimeoutMillis Timeout of unacked messages long true 0 tickDurationMillis Granularity of the ack-timeout redelivery.Using an higher <code>tickDurationMillis</code> reduces the memory overhead to track messages when setting ack-timeout to a bigger value (for example, 1 hour). long true 1000 priorityLevel Priority level for a consumer to which a broker gives more priority while dispatching messages in Shared subscription type.The broker follows descending priorities. For example, 0=max-priority, 1, 2,...In Shared subscription type, the broker first dispatches messages to the max priority level consumers if they have permits. Otherwise, the broker considers next priority level consumers.Example 1If a subscription has consumerA with <code>priorityLevel</code> 0 and consumerB with <code>priorityLevel</code> 1, then the broker only dispatches messages to consumerA until it runs out permits and then starts dispatching messages to consumerB.Example 2Consumer Priority, Level, PermitsC1, 0, 2C2, 0, 1C3, 0, 1C4, 1, 2C5, 1, 1Order in which a broker dispatches messages to consumers is: C1, C2, C3, C1, C4, C5, C4. int true 0 maxPendingChunkedMessage The maximum size of a queue holding pending chunked messages. When the threshold is reached, the consumer drops pending messages to optimize memory utilization. int true 10 autoAckOldestChunkedMessageOnQueueFull Whether to automatically acknowledge pending chunked messages when the threshold of <code>maxPendingChunkedMessage</code> is reached. If set to <code>false</code>, these messages will be redelivered by their broker. boolean true false expireTimeOfIncompleteChunkedMessageMillis The time interval to expire incomplete chunks if a consumer fails to receive all the chunks in the specified time period. The default value is 1 minute. long true 60000 cryptoKeyReader CryptoKeyReader false messageCrypto MessageCrypto false cryptoFailureAction Consumer should take action when it receives a message that can not be decrypted.* FAIL: this is the default option to fail messages until crypto succeeds.* DISCARD:silently acknowledge and not deliver message to an application.* CONSUME: deliver encrypted messages to applications. It is the application's responsibility to decrypt the message.The decompression of message fails.If messages contain batch messages, a client is not be able to retrieve individual messages in batch.Delivered encrypted message contains <code>EncryptionContext</code> which contains encryption and compression information in it using which application can decrypt consumed message payload. ConsumerCryptoFailureAction true FAIL properties A name or value property of this consumer.<code>properties</code> is application defined metadata attached to a consumer.When getting a topic stats, associate this metadata with the consumer stats for easier identification. SortedMap true {} readCompacted If enabling <code>readCompacted</code>, a consumer reads messages from a compacted topic rather than reading a full message backlog of a topic.A consumer only sees the latest value for each key in the compacted topic, up until reaching the point in the topic message when compacting backlog. Beyond that point, send messages as normal.Only enabling <code>readCompacted</code> on subscriptions to persistent topics, which have a single active consumer (like failure or exclusive subscriptions).Attempting to enable it on subscriptions to non-persistent topics or on shared subscriptions leads to a subscription call throwing a <code>PulsarClientException</code>. boolean true false subscriptionInitialPosition Initial position at which to set cursor when subscribing to a topic at first time. SubscriptionInitialPosition true Latest patternAutoDiscoveryPeriod Topic auto discovery period when using a pattern for topic's consumer.The default and minimum value is 1 minute. int true 60 regexSubscriptionMode When subscribing to a topic using a regular expression, you can pick a certain type of topics.* PersistentOnly: only subscribe to persistent topics.* NonPersistentOnly: only subscribe to non-persistent topics.* AllTopics: subscribe to both persistent and non-persistent topics. RegexSubscriptionMode true PersistentOnly deadLetterPolicy Dead letter policy for consumers.By default, some messages are probably redelivered many times, even to the extent that it never stops.By using the dead letter mechanism, messages have the max redelivery count. When exceeding the maximum number of redeliveries, messages are sent to the Dead Letter Topic and acknowledged automatically.You can enable the dead letter mechanism by setting <code>deadLetterPolicy</code>.Example<code>client.newConsumer().deadLetterPolicy(DeadLetterPolicy.builder().maxRedeliverCount(10).build()).subscribe();</code>Default dead letter topic name is <code>{TopicName}-{Subscription}-DLQ</code>.To set a custom dead letter topic name:<code>client.newConsumer().deadLetterPolicy(DeadLetterPolicy.builder().maxRedeliverCount(10).deadLetterTopic(\"your-topic-name\").build()).subscribe();</code>When specifying the dead letter policy while not specifying <code>ackTimeoutMillis</code>, you can set the ack timeout to 30000 millisecond. DeadLetterPolicy true retryEnable boolean true false batchReceivePolicy BatchReceivePolicy false autoUpdatePartitions If <code>autoUpdatePartitions</code> is enabled, a consumer subscribes to partition increasement automatically.Note: this is only for partitioned consumers. boolean true true autoUpdatePartitionsIntervalSeconds long true 60 replicateSubscriptionState If <code>replicateSubscriptionState</code> is enabled, a subscription state is replicated to geo-replicated clusters. boolean true false resetIncludeHead boolean true false keySharedPolicy KeySharedPolicy false batchIndexAckEnabled boolean true false ackReceiptEnabled boolean true false poolMessages boolean true false payloadProcessor MessagePayloadProcessor false startPaused boolean true false autoScaledReceiverQueueSizeEnabled boolean true false topicConfigurations List true []"},{"location":"pulsar/config/smallrye-pulsar-producer/","title":"Smallrye pulsar producer","text":"Attribute Description Type Config file Default topicName Topic name String true producerName Producer name String true sendTimeoutMs Message send timeout in ms.If a message is not acknowledged by a server before the <code>sendTimeout</code> expires, an error occurs. long true 30000 blockIfQueueFull If it is set to <code>true</code>, when the outgoing message queue is full, the <code>Send</code> and <code>SendAsync</code> methods of producer block, rather than failing and throwing errors.If it is set to <code>false</code>, when the outgoing message queue is full, the <code>Send</code> and <code>SendAsync</code> methods of producer fail and <code>ProducerQueueIsFullError</code> exceptions occur.The <code>MaxPendingMessages</code> parameter determines the size of the outgoing message queue. boolean true false maxPendingMessages The maximum size of a queue holding pending messages.For example, a message waiting to receive an acknowledgment from a broker.By default, when the queue is full, all calls to the <code>Send</code> and <code>SendAsync</code> methods fail unless you set <code>BlockIfQueueFull</code> to <code>true</code>. int true 0 maxPendingMessagesAcrossPartitions The maximum number of pending messages across partitions.Use the setting to lower the max pending messages for each partition (<code>#setMaxPendingMessages(int)</code>) if the total number exceeds the configured value. int true 0 messageRoutingMode Message routing logic for producers on partitioned topics.Apply the logic only when setting no key on messages.Available options are as follows:* <code>pulsar.RoundRobinDistribution</code>: round robin* <code>pulsar.UseSinglePartition</code>: publish all messages to a single partition* <code>pulsar.CustomPartition</code>: a custom partitioning scheme MessageRoutingMode true hashingScheme Hashing function determining the partition where you publish a particular message (partitioned topics only).Available options are as follows:* <code>pulsar.JavastringHash</code>: the equivalent of <code>string.hashCode()</code> in Java* <code>pulsar.Murmur3_32Hash</code>: applies the Murmur3 hashing function* <code>pulsar.BoostHash</code>: applies the hashing function from C++'sBoost library HashingScheme true JavaStringHash cryptoFailureAction Producer should take action when encryption fails.* FAIL: if encryption fails, unencrypted messages fail to send.* SEND: if encryption fails, unencrypted messages are sent. ProducerCryptoFailureAction true FAIL customMessageRouter MessageRouter false batchingMaxPublishDelayMicros Batching time period of sending messages. long true 1000 batchingPartitionSwitchFrequencyByPublishDelay int true 10 batchingMaxMessages The maximum number of messages permitted in a batch. int true 1000 batchingMaxBytes int true 131072 batchingEnabled Enable batching of messages. boolean true true batcherBuilder BatcherBuilder false chunkingEnabled Enable chunking of messages. boolean true false chunkMaxMessageSize int true -1 cryptoKeyReader CryptoKeyReader false messageCrypto MessageCrypto false encryptionKeys Set true [] compressionType Message data compression type used by a producer.Available options:* LZ4* ZLIB* ZSTD* SNAPPY CompressionType true NONE initialSequenceId Long true autoUpdatePartitions boolean true true autoUpdatePartitionsIntervalSeconds long true 60 multiSchema boolean true true accessMode ProducerAccessMode true Shared lazyStartPartitionedProducers boolean true false properties SortedMap true {} initialSubscriptionName Use this configuration to automatically create an initial subscription when creating a topic. If this field is not set, the initial subscription is not created. String true"},{"location":"rabbitmq/rabbitmq-client-customization/","title":"Customizing the underlying RabbitMQ client","text":"<p>You can customize the underlying RabbitMQ Client configuration by producing an instance of <code>RabbitMQOptions</code>:</p> <pre><code>@Produces\n@Identifier(\"my-named-options\")\npublic RabbitMQOptions getNamedOptions() {\n    // You can use the produced options to configure the TLS connection\n    PemKeyCertOptions keycert = new PemKeyCertOptions()\n            .addCertPath(\"./tls/tls.crt\")\n            .addKeyPath(\"./tls/tls.key\");\n    PemTrustOptions trust = new PemTrustOptions().addCertPath(\"./tlc/ca.crt\");\n\n    return (RabbitMQOptions) new RabbitMQOptions()\n            .setUser(\"admin\")\n            .setPassword(\"test\")\n            .setSsl(true)\n            .setPemKeyCertOptions(keycert)\n            .setPemTrustOptions(trust)\n            .setHostnameVerificationAlgorithm(\"HTTPS\")\n            .setConnectTimeout(30000)\n            .setReconnectInterval(5000);\n}\n</code></pre> <p>This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the <code>client-options-name</code> attribute:</p> <pre><code>mp.messaging.incoming.prices.client-options-name=my-named-options\n</code></pre>"},{"location":"rabbitmq/rabbitmq-cloud/","title":"Connecting to managed instances","text":"<p>This section describes the connector configuration to use managed RabbitMQ instances (hosted on the Cloud).</p>"},{"location":"rabbitmq/rabbitmq-cloud/#cloud-amqp","title":"Cloud AMQP","text":"<p>To connect to an instance of RabbitMQ hosted on Cloud AMQP, use the following configuration:</p> <pre><code>rabbitmq-host=host-name\nrabbitmq-port=5671\nrabbitmq-username=user-name\nrabbitmq-password=password\nrabbitmq-virtual-host=user-name\nrabbitmq-ssl=true\n</code></pre> <p>You can extract the values from the <code>AMQPS</code> url displayed on the administration portal:</p> <pre><code>amqps://user-name:password@host/user-name\n</code></pre>"},{"location":"rabbitmq/rabbitmq-cloud/#amazon-mq","title":"Amazon MQ","text":"<p>Amazon MQ can host RabbitMQ brokers (as well as AMQP 1.0 brokers). To connect to a RabbitMQ instance hosted on Amazon MQ, use the following configuration:</p> <pre><code>rabbitmq-host=host-name\nrabbitmq-port=5671\nrabbitmq-username=user-name\nrabbitmq-password=password\nrabbitmq-ssl=true\n</code></pre> <p>You can extract the host value from the <code>AMQPS</code> url displayed on the administration console:</p> <pre><code>amqps://foobarbaz.mq.us-east-2.amazonaws.com:5671\n</code></pre> <p>The username and password are configured during the broker creation.</p>"},{"location":"rabbitmq/rabbitmq-health/","title":"Health reporting","text":"<p>The RabbitMQ connector reports the readiness and liveness of each channel managed by the connector.</p> <p>On the inbound side (receiving messages from RabbitMQ), the check verifies that the receiver is connected to the broker.</p> <p>On the outbound side (sending records to RabbitMQ), the check verifies that the sender is not disconnected from the broker; the sender may still be in an initialized state (connection not yet attempted), but this is regarded as live/ready.</p> <p>You can disable health reporting by setting the <code>health-enabled</code> attribute of the channel to <code>false</code>. It disables both liveness and readiness. You can disable readiness reporting by setting the <code>health-readiness-enabled</code> attribute of the channel to <code>false</code>.</p>"},{"location":"rabbitmq/rabbitmq-health/#channel-and-lazy-subscription","title":"@Channel and lazy subscription","text":"<p>When you inject a channel using <code>@Channel</code> annotation, you are responsible for subscribing to the channel. Until the subscription happens, the channel is not connected to the broker and thus cannot receive messages. The default health check will fail in this case.</p> <p>To handle this use case, you need to configure the <code>health-lazy-subscription</code> attribute of the channel to <code>true</code>. It configures the health check to not fail if there are no subscription yet.</p>"},{"location":"rabbitmq/rabbitmq/","title":"RabbitMQ Connector","text":"<p>The RabbitMQ Connector adds support for RabbitMQ to Reactive Messaging, based on the AMQP 0-9-1 Protocol Specification.</p> <p>Advanced Message Queuing Protocol 0-9-1 (AMQP 0-9-1) is an open standard for passing business messages between applications or organizations.</p> <p>With this connector, your application can:</p> <ul> <li>receive messages from a RabbitMQ queue</li> <li>send messages to a RabbitMQ exchange</li> </ul> <p>The RabbitMQ connector is based on the Vert.x RabbitMQ Client.</p> <p>Important</p> <p>The AMQP connector supports the AMQP 1.0 protocol, which is very different from AMQP 0-9-1. You can use the AMQP connector with RabbitMQ provided that the latter has the AMQP 1.0 Plugin installed, albeit with reduced functionality.</p>"},{"location":"rabbitmq/rabbitmq/#using-the-rabbitmq-connector","title":"Using the RabbitMQ connector","text":"<p>To use the RabbitMQ Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-rabbitmq&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-rabbitmq</code>.</p> <p>So, to indicate that a channel is managed by this connector you need: <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-rabbitmq\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-rabbitmq\n</code></pre></p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/","title":"Receiving messages from RabbitMQ","text":"<p>The RabbitMQ connector lets you retrieve messages from a RabbitMQ broker. The RabbitMQ connector retrieves RabbitMQ Messages and maps each of them into Reactive Messaging <code>Messages</code>.</p> <p>Note</p> <p>In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Queue.</p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#example","title":"Example","text":"<p>Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the <code>rabbitmq:5672</code> address (by default it would use <code>localhost:5672</code>). Configure your application to receive RabbitMQ Messages on the <code>prices</code> channel as follows:</p> <pre><code>rabbitmq-host=rabbitmq  # &lt;1&gt;\nrabbitmq-port=5672      # &lt;2&gt;\nrabbitmq-username=my-username   # &lt;3&gt;\nrabbitmq-password=my-password   # &lt;4&gt;\n\nmp.messaging.incoming.prices.connector=smallrye-rabbitmq # &lt;5&gt;\nmp.messaging.incoming.prices.queue.name=my-queue         # &lt;6&gt;\nmp.messaging.incoming.prices.routing-keys=urgent         # &lt;7&gt;\n</code></pre> <ol> <li> <p>Configures the broker/router host name. You can do it per channel     (using the <code>host</code> attribute) or globally using <code>rabbitmq-host</code>.</p> </li> <li> <p>Configures the broker/router port. You can do it per channel (using     the <code>port</code> attribute) or globally using <code>rabbitmq-port</code>. The default     is 5672.</p> </li> <li> <p>Configures the broker/router username if required. You can do it per     channel (using the <code>username</code> attribute) or globally using     <code>rabbitmq-username</code>.</p> </li> <li> <p>Configures the broker/router password if required. You can do it per     channel (using the <code>password</code> attribute) or globally using     <code>rabbitmq-password</code>.</p> </li> <li> <p>Instructs the <code>prices</code> channel to be managed by the RabbitMQ     connector.</p> </li> <li> <p>Configures the RabbitMQ queue to read messages from.</p> </li> <li> <p>Configures the binding between the RabbitMQ exchange and the     RabbitMQ queue using a routing key. The default is <code>#</code> (all messages     will be forwarded from the exchange to the queue) but in general     this can be a comma-separated list of one or more keys.</p> </li> </ol> <p>Then, your application receives <code>Message&lt;String&gt;</code>. You can consume the payload directly:</p> <pre><code>package rabbitmq.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class RabbitMQPriceConsumer {\n\n    @Incoming(\"prices\")\n    public void consume(String price) {\n        // process your price.\n    }\n\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;String&gt;</code>:</p> <pre><code>package rabbitmq.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class RabbitMQPriceMessageConsumer {\n\n    @Incoming(\"prices\")\n    public CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; price) {\n        // process your price.\n\n        // Acknowledge the incoming message, marking the RabbitMQ message as `accepted`.\n        return price.ack();\n    }\n\n}\n</code></pre> <p>Note</p> <p>Whether you need to explicitly acknowledge the message depends on the <code>auto-acknowledgement</code> channel setting; if that is set to <code>true</code> then your message will be automatically acknowledged on receipt.</p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#deserialization","title":"Deserialization","text":"<p>The connector converts incoming RabbitMQ Messages into Reactive Messaging <code>Message&lt;T&gt;</code> instances. The payload type <code>T</code> depends on the value of the RabbitMQ received message Envelope <code>content_type</code> and <code>content_encoding</code> properties.</p> content_encoding content_type Type Value present n/a <code>byte[]</code> No value <code>text/plain</code> <code>String</code> No value <code>application/json</code> a JSON element which can be a <code>JsonArray</code>, <code>JsonObject</code>, <code>String</code>, ... if the buffer contains an array, object, string,... No value Anything else <code>byte[]</code> <p>If you send objects with this RabbitMQ connector (outbound connector), they are encoded as JSON and sent with <code>content_type</code> set to <code>application/json</code>. You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want:</p> <pre><code>@ApplicationScoped\npublic static class Generator {\n\n    @Outgoing(\"to-rabbitmq\")\n    public Multi&lt;Price&gt; prices() {             // &lt;1&gt;\n        AtomicInteger count = new AtomicInteger();\n        return Multi.createFrom().ticks().every(Duration.ofMillis(1000))\n                .map(l -&gt; new Price().setPrice(count.incrementAndGet()))\n                .onOverflow().drop();\n    }\n\n}\n\n@ApplicationScoped\npublic static class Consumer {\n\n    List&lt;Price&gt; prices = new CopyOnWriteArrayList&lt;&gt;();\n\n    @Incoming(\"from-rabbitmq\")\n    public void consume(JsonObject p) {      // &lt;2&gt;\n        Price price = p.mapTo(Price.class);  // &lt;3&gt;\n        prices.add(price);\n    }\n\n    public List&lt;Price&gt; list() {\n        return prices;\n    }\n}\n</code></pre> <ol> <li>The <code>Price</code> instances are automatically encoded to JSON by the     connector</li> <li>You can receive it using a <code>JsonObject</code></li> <li>Then, you can reconstruct the instance using the <code>mapTo</code> method</li> </ol>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from RabbitMQ contain an instance of IncomingRabbitMQMetadata in the metadata.</p> <p>RabbitMQ message headers can be accessed from the metadata either by calling <code>getHeader(String header, Class&lt;T&gt; type)</code> to retrieve a single header value. or <code>getHeaders()</code> to get a map of all header values.</p> <pre><code>final Optional&lt;IncomingRabbitMQMetadata&gt; metadata = incomingMessage.getMetadata(IncomingRabbitMQMetadata.class);\nmetadata.ifPresent(meta -&gt; {\n    final Optional&lt;String&gt; contentEncoding = meta.getContentEncoding();\n    final Optional&lt;String&gt; contentType = meta.getContentType();\n    final Optional&lt;String&gt; correlationId = meta.getCorrelationId();\n    final Optional&lt;ZonedDateTime&gt; timestamp = meta.getTimestamp(ZoneId.systemDefault());\n    final Optional&lt;Integer&gt; priority = meta.getPriority();\n    final Optional&lt;String&gt; replyTo = meta.getReplyTo();\n    final Optional&lt;String&gt; userId = meta.getUserId();\n\n    // Access a single String-valued header\n    final Optional&lt;String&gt; stringHeader = meta.getHeader(\"my-header\", String.class);\n\n    // Access all headers\n    final Map&lt;String, Object&gt; headers = meta.getHeaders();\n    // ...\n});\n</code></pre> <p>The type <code>&lt;T&gt;</code> of the header value depends on the RabbitMQ type used for the header:</p> RabbitMQ Header Type T String <code>String</code> Boolean <code>Boolean</code> Number <code>Number</code> List <code>java.util.List</code> <p>If you need to create <code>IncomingRabbitMQMetadata</code> e.g. for testing purposes, you can create an <code>&lt;a class='docissimo, docissimo-javadoc' href='https://javadoc.io/doc/io.smallrye.reactive/smallrye-reactive-messaging-api/4.33.0/io/smallrye/reactive/messaging/rabbitmq/OutgoingRabbitMQMetadata.html'&gt;OutgoingRabbitMQMetadata&lt;/a&gt;</code> using its builder and convert it using <code>OutgoingRabbitMQMetadata.toIncomingMetadata(String, boolean)</code></p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#acknowledgement","title":"Acknowledgement","text":"<p>When a Reactive Messaging Message associated with a RabbitMQ Message is acknowledged, it informs the broker that the message has been accepted.</p> <p>Whether you need to explicitly acknowledge the message depends on the <code>auto-acknowledgement</code> setting for the channel; if that is set to <code>true</code> then your message will be automatically acknowledged on receipt.</p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#failure-management","title":"Failure Management","text":"<p>If a message produced from a RabbitMQ message is nacked, a failure strategy is applied. The RabbitMQ connector supports three strategies, controlled by the <code>failure-strategy</code> channel setting:</p> <ul> <li> <p><code>fail</code> - fail the application; no more RabbitMQ messages will be     processed. The RabbitMQ message is marked as rejected.</p> </li> <li> <p><code>accept</code> - this strategy marks the RabbitMQ message as accepted. The     processing continues ignoring the failure.</p> </li> <li> <p><code>reject</code> - this strategy marks the RabbitMQ message as rejected     (default). The processing continues with the next message.</p> </li> <li> <p><code>requeue</code> - this strategy marks the RabbitMQ message as rejected     with requeue flag to true. The processing continues with the next message,     but the requeued message will be redelivered to the consumer.</p> </li> </ul> <p>When using <code>dead-letter-queue</code>, it is also possible to change some metadata of the record that is sent to the dead letter topic. To do that, use the <code>Message.nack(Throwable, Metadata)</code> method:</p> <p>The RabbitMQ reject <code>requeue</code> flag can be controlled on different failure strategies using the RabbitMQRejectMetadata. To do that, use the <code>Message.nack(Throwable, Metadata)</code> method by including the <code>RabbitMQRejectMetadata</code> metadata with <code>requeue</code> to <code>true</code>.</p> <pre><code>@Incoming(\"in\")\npublic CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; message) {\n    return message.nack(new Exception(\"Failed!\"), Metadata.of(\n            new RabbitMQRejectMetadata(true)));\n}\n</code></pre> <p>Experimental</p> <p><code>RabbitMQFailureHandler</code> is experimental and APIs are subject to change in the future</p> <p>In addition, you can also provide your own failure strategy. To provide a failure strategy implement a bean exposing the interface RabbitMQFailureHandler, qualified with a <code>@Identifier</code>. Set the name of the bean as the <code>failure-strategy</code> channel setting.</p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default addresses (rabbitmq-addresses) The multiple addresses for cluster mode, when given overrides the host and port string false arguments A comma-separated list of arguments [key1:value1,key2:value2,...] to bind the queue to the exchange. Relevant only if 'exchange.type' is headers string false auto-acknowledgement Whether the received RabbitMQ messages must be acknowledged when received; if true then delivery constitutes acknowledgement boolean false <code>false</code> auto-bind-dlq Whether to automatically declare the DLQ and bind it to the binder DLX boolean false <code>false</code> automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false <code>false</code> automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false <code>true</code> broadcast Whether the received RabbitMQ messages must be dispatched to multiple subscribers boolean false <code>false</code> client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false <code>60000</code> consumer-arguments A comma-separated list of arguments [key1:value1,key2:value2,...] for created consumer string false consumer-exclusive The exclusive flag for created consumer boolean false consumer-tag The consumer-tag option for created consumer, if not provided the consumer gets assigned a tag generated by the broker string false content-type-override Override the content_type attribute of the incoming message, should be a valid MINE type string false credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false dead-letter-dlx If specified, a DLX to assign to the DLQ. Relevant only if auto-bind-dlq is true string false dead-letter-dlx-routing-key If specified, a dead letter routing key to assign to the DLQ. Relevant only if auto-bind-dlq is true string false dead-letter-exchange A DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false <code>DLX</code> dead-letter-exchange-type The type of the DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false <code>direct</code> dead-letter-exchange.arguments The identifier of the key-value Map exposed as bean used to provide arguments for dead-letter-exchange creation string false dead-letter-queue-mode If automatically declare DLQ, we can choose different modes of DLQ [lazy, default] string false dead-letter-queue-name The name of the DLQ; if not supplied will default to the queue name with '.dlq' appended string false dead-letter-queue-type If automatically declare DLQ, we can choose different types of DLQ [quorum, classic, stream] string false dead-letter-queue.arguments The identifier of the key-value Map exposed as bean used to provide arguments for dead-letter-queue creation string false dead-letter-routing-key A dead letter routing key to assign to the queue; if not supplied will default to the queue name string false dead-letter-ttl If specified, the time (ms) for which a message can remain in DLQ undelivered before it is dead. Relevant only if auto-bind-dlq is true long false dlx.declare Whether to declare the dead letter exchange binding. Relevant only if auto-bind-dlq is true; set to false if these are expected to be set up independently boolean false <code>false</code> exchange.arguments The identifier of the key-value Map exposed as bean used to provide arguments for exchange creation string false <code>rabbitmq-exchange-arguments</code> exchange.auto-delete Whether the exchange should be deleted after use boolean false <code>false</code> exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false <code>true</code> exchange.durable Whether the exchange is durable boolean false <code>true</code> exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false <code>topic</code> failure-strategy The failure strategy to apply when a RabbitMQ message is nacked. Accepted values are <code>fail</code>, <code>accept</code>, <code>reject</code> (default), <code>requeue</code> or name of a bean string false <code>reject</code> handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false <code>10000</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-lazy-subscription Whether the liveness and readiness checks should report 'ok' when there is no subscription yet. This is useful when injecting the channel with <code>@Inject @Channel(\"...\") Multi&lt;...&gt; multi;</code> boolean false <code>false</code> health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false <code>true</code> host (rabbitmq-host) The broker hostname string false <code>localhost</code> include-properties Whether to include properties when a broker message is passed on the event bus boolean false <code>false</code> keep-most-recent Whether to discard old messages instead of recent ones boolean false <code>false</code> max-incoming-internal-queue-size The maximum size of the incoming internal queue int false <code>500000</code> max-outstanding-messages The maximum number of outstanding/unacknowledged messages being processed by the connector at a time; must be a positive number int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false <code>5000</code> password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false <code>5672</code> queue.arguments The identifier of the key-value Map exposed as bean used to provide arguments for queue creation string false <code>rabbitmq-queue-arguments</code> queue.auto-delete Whether the queue should be deleted after use boolean false <code>false</code> queue.declare Whether to declare the queue and binding; set to false if these are expected to be set up independently boolean false <code>true</code> queue.durable Whether the queue is durable boolean false <code>true</code> queue.exclusive Whether the queue is for exclusive use boolean false <code>false</code> queue.name The queue from which messages are consumed. If not set, the channel name is used. string false queue.single-active-consumer If set to true, only one consumer can actively consume messages boolean false queue.ttl If specified, the time (ms) for which a message can remain in the queue undelivered before it is dead long false queue.x-delivery-limit If queue.x-queue-type is quorum, when a message has been returned more times than the limit the message will be dropped or dead-lettered long false queue.x-max-priority Define priority level queue consumer int false queue.x-queue-mode If automatically declare queue, we can choose different modes of queue [lazy, default] string false queue.x-queue-type If automatically declare queue, we can choose different types of queue [quorum, classic, stream] string false reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false <code>100</code> reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false <code>10</code> requested-channel-max The initially requested maximum channel number int false <code>2047</code> requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false <code>60</code> routing-keys A comma-separated list of routing keys to bind the queue to the exchange. Relevant only if 'exchange.type' is topic or direct string false <code>#</code> ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false <code>false</code> ssl.hostname-verification-algorithm Set the hostname verifier algorithm for the TLS connection. Accepted values are <code>HTTPS</code>, and <code>NONE</code> (defaults). <code>NONE</code> disables the hostname verification. string false <code>NONE</code> tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false <code>false</code> trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false <code>false</code> user The user name to use when connecting to the broker string false <code>guest</code> username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false <code>/</code> <p>To use an existing queue, you need to configure the <code>queue.name</code> attribute.</p> <p>For example, if you have a RabbitMQ broker already configured with a queue called <code>peopleQueue</code> that you wish to read messages from, you need the following configuration:</p> <pre><code>mp.messaging.incoming.people.connector=smallrye-rabbitmq\nmp.messaging.incoming.people.queue.name=peopleQueue\n</code></pre> <p>If you want RabbitMQ to create the queue for you but bind it to an existing topic exchange <code>people</code>, you need the following configuration:</p> <pre><code>mp.messaging.incoming.people.connector=smallrye-rabbitmq\nmp.messaging.incoming.people.queue.name=peopleQueue\nmp.messaging.incoming.people.queue.declare=true\n</code></pre> <p>Note</p> <p>In the above the channel name <code>people</code> is implicitly assumed to be the name of the exchange; if this is not the case you would need to name the exchange explicitly using the <code>exchange.name</code> property.</p> <p>Note</p> <p>The connector supports RabbitMQ's \"Server-named Queues\" feature to create an exclusive, auto-deleting, non-durable and randomly named queue. To enable this feature you set the queue name to exactly <code>(server.auto)</code>. Using this name not only enables the server named queue feature but also automatically makes ths queue exclusive, auto-deleting, and non-durable; therefore ignoring any values provided for the <code>exclusive</code>, <code>auto-delete</code> and <code>durable</code> options.</p> <p>If you want RabbitMQ to create the <code>people</code> exchange, queue and binding, you need the following configuration:</p> <pre><code>mp.messaging.incoming.people.connector=smallrye-rabbitmq\nmp.messaging.incoming.people.exchange.declare=true\nmp.messaging.incoming.people.queue.name=peopleQueue\nmp.messaging.incoming.people.queue.declare=true\nmp.messaging.incoming.people.queue.routing-keys=tall,short\n</code></pre> <p>In the above we have used an explicit list of routing keys rather than the default (<code>#</code>). Each component of the list creates a separate binding between the queue and the exchange, so in the case above we would have two bindings; one based on a routing key of <code>tall</code>, the other based on one of <code>short</code>.</p> <p>Note</p> <p>The default value of <code>routing-keys</code> is <code>#</code> (indicating a match against all possible routing keys) which is only appropriate for topic Exchanges. If you are using other types of exchange and/or need to declare queue bindings, you\u2019ll need to supply a valid value for the exchange in question.</p>"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#custom-arguments-for-queue-declaration","title":"Custom arguments for Queue declaration","text":"<p>When queue declaration is made by the Reactive Messaging channel, using the <code>queue.declare=true</code> configuration, custom queue arguments can be specified using the <code>queue.arguments</code> attribute. <code>queue.arguments</code> accepts the identifier (using the <code>@Identifier</code> qualifier) of a <code>Map&lt;String, Object&gt;</code> exposed as a CDI bean. If no arguments has been configured, the default rabbitmq-queue-arguments identifier is looked for.</p> <p>The following CDI bean produces such a configuration identified with my-arguments:</p> <pre><code>package rabbitmq.customization;\n\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.enterprise.inject.Produces;\n\nimport io.smallrye.common.annotation.Identifier;\n\n@ApplicationScoped\npublic class ArgumentProducers {\n    @Produces\n    @Identifier(\"my-arguments\")\n    Map&lt;String, Object&gt; customArguments() {\n        return Map.of(\"custom-arg\", \"value\");\n    }\n}\n</code></pre> <p>Then the channel can be configured to use those arguments in exchange declaration:</p> <pre><code>mp.messaging.outgoing.data.queue.arguments=my-arguments\n</code></pre> <p>Similarly, the <code>dead-letter-queue.arguments</code> allows configuring custom arguments for dead letter queue when one is declared (<code>auto-bind-dlq=true</code>).</p>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/","title":"Sending messages to RabbitMQ","text":"<p>The RabbitMQ connector can write Reactive Messaging <code>Messages</code> as RabbitMQ Messages.</p> <p>Note</p> <p>In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Exchange.</p>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#example","title":"Example","text":"<p>Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the <code>rabbitmq:5672</code> address (by default it would use <code>localhost:5672</code>). Configure your application to send the messages from the <code>prices</code> channel as a RabbitMQ Message as follows:</p> <pre><code>rabbitmq-host=rabbitmq   # &lt;1&gt;\nrabbitmq-port=5672       # &lt;2&gt;\nrabbitmq-username=my-username # &lt;3&gt;\nrabbitmq-password=my-password  # &lt;4&gt;\n\nmp.messaging.outgoing.prices.connector=smallrye-rabbitmq # &lt;5&gt;\nmp.messaging.outgoing.prices.default-routing-key=normal    # &lt;6&gt;\n</code></pre> <ol> <li> <p>Configures the broker/router host name. You can do it per channel     (using the <code>host</code> attribute) or globally using <code>rabbitmq-host</code></p> </li> <li> <p>Configures the broker/router port. You can do it per channel (using     the <code>port</code> attribute) or globally using <code>rabbitmq-port</code>. The default     is <code>5672</code>.</p> </li> <li> <p>Configures the broker/router username if required. You can do it per     channel (using the <code>username</code> attribute) or globally using     <code>rabbitmq-username</code>.</p> </li> <li> <p>Configures the broker/router password if required. You can do it per     channel (using the <code>password</code> attribute) or globally using     <code>rabbitmq-password</code>.</p> </li> <li> <p>Instructs the <code>prices</code> channel to be managed by the RabbitMQ     connector</p> </li> <li> <p>Supplies the default routing key to be included in outbound     messages; this will be if the \"raw payload\" form of message sending     is used (see below).</p> </li> </ol> <p>Note</p> <p>You don\u2019t need to set the RabbitMQ exchange name. By default, it uses the channel name (<code>prices</code>) as the name of the exchange to send messages to. You can configure the <code>exchange.name</code> attribute to override it.</p> <p>Then, your application can send <code>Message&lt;Double&gt;</code> to the prices channel. It can use <code>double</code> payloads as in the following snippet:</p> <pre><code>package rabbitmq.outbound;\n\nimport java.time.Duration;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class RabbitMQPriceProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Double&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; random.nextDouble());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>, which affords the opportunity to explicitly specify metadata on the outgoing message:</p> <pre><code>package rabbitmq.outbound;\n\nimport java.time.Duration;\nimport java.time.ZonedDateTime;\nimport java.util.Random;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.reactive.messaging.rabbitmq.OutgoingRabbitMQMetadata;\n\n@ApplicationScoped\npublic class RabbitMQPriceMessageProducer {\n\n    private Random random = new Random();\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;Double&gt;&gt; generate() {\n        // Build an infinite stream of random prices\n        // It emits a price every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(random.nextDouble(),\n                        Metadata.of(new OutgoingRabbitMQMetadata.Builder()\n                                .withRoutingKey(\"normal\")\n                                .withTimestamp(ZonedDateTime.now())\n                                .build())));\n    }\n\n}\n</code></pre>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#serialization","title":"Serialization","text":"<p>When sending a <code>Message&lt;T&gt;</code>, the connector converts the message into a RabbitMQ Message. The payload is converted to the RabbitMQ Message body.</p> T RabbitMQ Message Body primitive types or <code>UUID</code>/<code>String</code> String value with <code>content_type</code> set to <code>text/plain</code> <code>JsonObject</code> or <code>JsonArray</code> Serialized String payload with <code>content_type</code> set to <code>application/json</code> <code>io.vertx.mutiny.core.buffer.Buffer</code> Binary content, with <code>content_type</code> set to <code>application/octet-stream</code> <code>byte[]</code> Binary content, with content_type set to <code>application/octet-stream</code> Any other class The payload is converted to JSON (using a Json Mapper) then serialized with <code>content_type</code> set to <code>application/json</code> <p>If the message payload cannot be serialized to JSON, the message is nacked.</p>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of OutgoingRabbitMQMetadata to influence how the message is handled by RabbitMQ. For example, you can configure the routing key, timestamp and headers:</p> <pre><code>final OutgoingRabbitMQMetadata metadata = new OutgoingRabbitMQMetadata.Builder()\n        .withHeader(\"my-header\", \"xyzzy\")\n        .withRoutingKey(\"urgent\")\n        .withTimestamp(ZonedDateTime.now())\n        .build();\n\n// Add `metadata` to the metadata of the outgoing message.\nreturn Message.of(\"Hello\", Metadata.of(metadata));\n</code></pre>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#acknowledgement","title":"Acknowledgement","text":"<p>By default, the Reactive Messaging <code>Message</code> is acknowledged when the broker acknowledges the message.</p>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default addresses (rabbitmq-addresses) The multiple addresses for cluster mode, when given overrides the host and port string false automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false <code>false</code> automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false <code>true</code> client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false <code>60000</code> credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false default-routing-key The default routing key to use when sending messages to the exchange string false `` default-ttl If specified, the time (ms) sent messages can remain in queues undelivered before they are dead long false exchange.arguments The identifier of the key-value Map exposed as bean used to provide arguments for exchange creation string false <code>rabbitmq-exchange-arguments</code> exchange.auto-delete Whether the exchange should be deleted after use boolean false <code>false</code> exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false <code>true</code> exchange.durable Whether the exchange is durable boolean false <code>true</code> exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false <code>topic</code> handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false <code>10000</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false <code>true</code> host (rabbitmq-host) The broker hostname string false <code>localhost</code> include-properties Whether to include properties when a broker message is passed on the event bus boolean false <code>false</code> max-inflight-messages The maximum number of messages to be written to RabbitMQ concurrently; must be a positive number long false <code>1024</code> max-outgoing-internal-queue-size The maximum size of the outgoing internal queue int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false <code>5000</code> password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false <code>5672</code> publish-confirms If set to true, published messages are acknowledged when the publish confirm is received from the broker boolean false <code>false</code> reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false <code>100</code> reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false <code>10</code> requested-channel-max The initially requested maximum channel number int false <code>2047</code> requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false <code>60</code> retry-on-fail-attempts The number of tentative to retry on failure int false <code>6</code> retry-on-fail-interval The interval (in seconds) between two sending attempts int false <code>5</code> ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false <code>false</code> ssl.hostname-verification-algorithm Set the hostname verifier algorithm for the TLS connection. Accepted values are <code>HTTPS</code>, and <code>NONE</code> (defaults). <code>NONE</code> disables the hostname verification. string false <code>NONE</code> tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false <code>false</code> trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false <code>false</code> user The user name to use when connecting to the broker string false <code>guest</code> username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false <code>/</code>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#using-existing-destinations","title":"Using existing destinations","text":"<p>To use an existing exchange, you may need to configure the <code>exchange.name</code> attribute.</p> <p>For example, if you have a RabbitMQ broker already configured with an exchange called <code>people</code> that you wish to send messages to, you need the following configuration:</p> <pre><code>mp.messaging.outgoing.people.connector=smallrye-rabbitmq\n</code></pre> <p>You would need to configure the <code>exchange.name</code> attribute, if the exchange name were not the channel name:</p> <pre><code>mp.messaging.outgoing.people-out.connector=smallrye-rabbitmq\nmp.messaging.outgoing.people-out.exchange.name=people\n</code></pre> <p>If you want RabbitMQ to create the <code>people</code> exchange, you need the following configuration:</p> <pre><code>mp.messaging.outgoing.people-out.connector=smallrye-amqp\nmp.messaging.outgoing.people-out.exchange.name=people\nmp.messaging.outgoing.people-out.exchange.declare=true\n</code></pre> <p>Note</p> <p>The above example will create a <code>topic</code> exchange and use an empty default <code>routing-key</code> (unless overridden programatically using outgoing metadata for the message). If you want to create a different type of exchange or have a different default routing key, then the <code>exchange.type</code> and <code>default-routing-key</code> properties need to be explicitly specified.</p>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#sending-to-specific-queues-via-the-default-exchange","title":"Sending to specific queues via the default exchange","text":"<p>To send a message to a specific queue (usually a reply queue), you have to configure the default exchange as an outgoing channel and set the name of the queue as routing key in the message metadata. The name of the exchange needs to be set to <code>\"\"</code>.</p> <pre><code>mp.messaging.outgoing.channel-name-for-default-exchange.connector=smallrye-rabbitmq\nmp.messaging.outgoing.channel-name-for-default-exchange.exchange.name=\"\"\n</code></pre>"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#custom-arguments-for-exchange-declaration","title":"Custom arguments for Exchange declaration","text":"<p>When exchange declaration is made by the Reactive Messaging channel, using the <code>exchange.declare=true</code> configuration, custom exchange arguments can be specified using the <code>exchange.arguments</code> attribute. <code>exchange.arguments</code> accepts the identifier (using the <code>@Identifier</code> qualifier) of a <code>Map&lt;String, Object&gt;</code> exposed as a CDI bean. If no arguments has been configured, the default rabbitmq-exchange-arguments identifier is looked for.</p> <p>The following CDI bean produces such a configuration identified with my-arguments:</p> <pre><code>package rabbitmq.customization;\n\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.enterprise.inject.Produces;\n\nimport io.smallrye.common.annotation.Identifier;\n\n@ApplicationScoped\npublic class ArgumentProducers {\n    @Produces\n    @Identifier(\"my-arguments\")\n    Map&lt;String, Object&gt; customArguments() {\n        return Map.of(\"custom-arg\", \"value\");\n    }\n}\n</code></pre> <p>Then the channel can be configured to use those arguments in exchange declaration:</p> <pre><code>mp.messaging.outgoing.data.exchange.arguments=my-arguments\n</code></pre> <p>Similarly, the <code>dead-letter-exchange.arguments</code> allows configuring custom arguments for dead letter exchange when one is declared (<code>dlx.declare=true</code>).</p>"},{"location":"sns/sending-aws-sns-messages/","title":"Sending AWS SNS Messages","text":"<p>The AWS SNS connector allows you to send messages to an AWS SNS topic.</p>"},{"location":"sns/sending-aws-sns-messages/#sending-messages","title":"Sending messages","text":"<p>Before you start, you need to have an AWS account and an SNS topic created. To send messages to an SNS topic, you need to create a method that produces messages to the topic.</p> <p>Then, your application can send <code>Message&lt;String&gt;</code> to the prices channel. It can use <code>String</code> payloads as in the following snippet:</p> <pre><code>package sns.outbound;\n\nimport java.time.Duration;\nimport java.util.UUID;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class SnsProducer {\n\n    @Outgoing(\"data\")\n    public Multi&lt;String&gt; generate() {\n        // It emits a UUID every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; UUID.randomUUID().toString());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>, which affords the opportunity to explicitly specify metadata on the outgoing message:</p> <pre><code>package sns.outbound;\n\nimport java.time.Duration;\nimport java.util.UUID;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.reactive.messaging.aws.sns.SnsOutboundMetadata;\n\n@ApplicationScoped\npublic class SnsMessageProducer {\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;String&gt;&gt; generate() {\n        // It emits a UUID every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(UUID.randomUUID().toString(),\n                        Metadata.of(SnsOutboundMetadata.builder()\n                                .groupId(\"group-1\")\n                                .build())));\n    }\n}\n</code></pre>"},{"location":"sns/sending-aws-sns-messages/#sending-messages-in-batch","title":"Sending messages in batch","text":"<p>You can configure the outbound channel to send messages in batch of maximum 10 messages (AWS SNS limitation).</p> <p>You can customize the size of batches, <code>10</code> being the default batch size, and the delay to wait for new messages to be added to the batch, 3000ms being the default delay:</p> <pre><code>mp.messaging.outgoing.prices.connector=smallrye-sns\nmp.messaging.outgoing.prices.topic=prices\nmp.messaging.outgoing.prices.batch=true\nmp.messaging.outgoing.prices.batch-size=5\nmp.messaging.outgoing.prices.batch-delay=3000\n</code></pre>"},{"location":"sns/sending-aws-sns-messages/#serialization","title":"Serialization","text":"<p>When sending a <code>Message&lt;T&gt;</code>, the connector converts the message into a AWS SNS Message. How the message is converted depends on the payload type:</p> <ul> <li>If the payload is of type <code>PublishRequest</code> it is sent as is.</li> <li>If the payload is of type <code>PublishRequest.Builder</code>, the topic url is set and sent.</li> <li>If the payload is of primitive types the payload is converted to String and the message attribute <code>_classname</code> is set to the class name of the payload.</li> <li>If the payload is of any other object type, the payload is serialized (using the <code>JsonMapping</code> implementation discovered) and the message attribute <code>_classname</code> is set to the class name of the payload.</li> </ul> <p>If the message payload cannot be serialized to JSON, the message is nacked.</p>"},{"location":"sns/sending-aws-sns-messages/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of SnsOutboundMetadata to influence how the message is handled by AWS SNS. For example, you can configure the routing key, timestamp and headers:</p> <pre><code>final SnsOutboundMetadata metadata = SnsOutboundMetadata.builder()\n        .messageAttributes(Map.of(\"my-attribute\", MessageAttributeValue.builder()\n                .dataType(\"String\").stringValue(\"my-value\").build()))\n        .groupId(\"group-1\")\n        .smsPhoneNumber(\"+1234567890\")\n        .build();\n\n// Add `metadata` to the metadata of the outgoing message.\nreturn Message.of(\"Hello\", Metadata.of(metadata));\n</code></pre>"},{"location":"sns/sending-aws-sns-messages/#acknowledgement","title":"Acknowledgement","text":"<p>By default, the Reactive Messaging <code>Message</code> is acknowledged when the send message request is successful. If the message is not sent successfully, the message is nacked.</p>"},{"location":"sns/sending-aws-sns-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default batch When set, sends messages in batches of maximum 10 messages boolean false <code>false</code> batch-delay In batch send mode, the maximum delay in milliseconds to wait for messages to be included in the batch. Defaults to 3000 ms int false <code>3000</code> batch-size In batch send mode, the maximum number of messages to include in batch, currently SNS maximum is 10 messages int false <code>10</code> credentials-provider The credential provider to be used in the client, defaults to AWS default provider chain string false email.subject When set, sends messages with the specified subject string false endpoint-override The endpoint override string false group.id When set, sends messages with the specified group id string false health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> message-structure Set to json if you want to send a different message for each protocol. string false region The name of the SNS region, defaults to AWS region resolver string false sms.phoneNumber When set, sends messages to the specified phone number. Not supported in combination with batching. string false topic The name of the SNS topic, defaults to channel name if not provided string false topic.arn The arn of the SNS topic string false"},{"location":"sns/sns/","title":"AWS SNS Connector","text":"<p>Preview</p> <p>The AWS SNS Connector is currently in preview.</p> <p>The AWS SNS Connector adds support for AWS SNS to Reactive Messaging.</p> <p>With this connector, your application can send messages to a AWS SNS topic.</p> <p>The AWS SNS connector is based on the AWS SDK for Java V2.</p>"},{"location":"sns/sns/#using-the-aws-sns-connector","title":"Using the AWS SNS connector","text":"<p>To use the AWS SNS Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-aws-sns&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-sns</code>.</p> <p>So, to indicate that a channel is managed by this connector you need: <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-sns\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-sns\n</code></pre></p>"},{"location":"sns/sns/#configuration","title":"Configuration","text":"<p>When available the AWS SNS connector discovers the SNS client as a CDI bean. This allows using the connector with the Quarkiverse AWS SNS extension. If the SNS client is not available as a CDI bean, the connector creates a new client using the provided configuration.</p> <ul> <li><code>topic</code> - The name of the SNS queue, defaults to channel name if not provided.</li> <li><code>region</code> - The name of the SNS region.</li> <li><code>endpoint-override</code> - The endpoint url override.</li> <li><code>credentials-provider</code> - The fully qualified class name of the credential provider to be used in the client, if not provided the default provider chain is used.</li> </ul>"},{"location":"sns/sns/#additional-resources","title":"Additional Resources","text":"<ul> <li>AWS SNS Documentation</li> <li>Quarkiverse AWS SNS Extension</li> </ul>"},{"location":"sqs/receiving-aws-sqs-messages/","title":"Receiving AWS SQS Messages","text":"<p>The AWS SQS connector allows you to receive messages from an AWS SQS queue.</p>"},{"location":"sqs/receiving-aws-sqs-messages/#receiving-messages","title":"Receiving messages","text":"<p>Before you start, you need to have an AWS account and a SQS queue created. To receive messages from an SQS queue, you need to create a method that consumes messages from the queue. <pre><code>mp.messaging.incoming.data.connector=smallrye-sqs\nmp.messaging.incoming.data.queue=my-queue\n</code></pre></p> <p>Then, your application receives <code>Message&lt;String&gt;</code>. You can consume the payload directly:</p> <pre><code>package sqs.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\n@ApplicationScoped\npublic class SqsStringConsumer {\n    @Incoming(\"data\")\n    void consume(String messageBody) {\n        System.out.println(\"Received: \" + messageBody);\n    }\n}\n</code></pre> <p>Or, you can retrieve the <code>Message&lt;String&gt;</code>:</p> <pre><code>package sqs.inbound;\n\nimport java.util.concurrent.CompletionStage;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class SqsMessageStringConsumer {\n    @Incoming(\"data\")\n    CompletionStage&lt;Void&gt; consume(Message&lt;String&gt; msg) {\n        System.out.println(\"Received: \" + msg.getPayload());\n        return msg.ack();\n    }\n}\n</code></pre> <p>You also can directly consume the <code>software.amazon.awssdk.services.sqs.model.Message</code>:</p> <pre><code>package sqs.inbound;\n\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\nimport software.amazon.awssdk.services.sqs.model.Message;\nimport software.amazon.awssdk.services.sqs.model.MessageAttributeValue;\n\n@ApplicationScoped\npublic class SqsSdkMessageConsumer {\n\n    @Incoming(\"data\")\n    void consume(Message msg) {\n        System.out.println(\"Received: \" + msg.body());\n        Map&lt;String, MessageAttributeValue&gt; attributes = msg.messageAttributes();\n        // ...\n    }\n}\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#receive-message-request-customizer","title":"Receive message request customizer","text":"<p>The receive message requests sent to AWS SQS can be customized by providing a CDI bean implementation of SqsReceiveMessageRequestCustomizer and configuring it's identifier using the <code>receive.request.customizer</code> connector attribute.</p> <pre><code>package sqs.inbound;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsReceiveMessageRequestCustomizer;\nimport software.amazon.awssdk.services.sqs.model.ReceiveMessageRequest;\n\n@Identifier(\"my-customizer\") // or with channel name @Identifier(\"data\")\n@ApplicationScoped\npublic class SqsReceiveMessageRequestCustomizerExample implements SqsReceiveMessageRequestCustomizer {\n    @Override\n    public void customize(ReceiveMessageRequest.Builder builder) {\n        builder.visibilityTimeout(10)\n                .messageAttributeNames(\"my-attribute\");\n    }\n}\n</code></pre> <pre><code>mp.messaging.incoming.data.connector=smallrye-sqs\nmp.messaging.incoming.data.queue=my-queue\nmp.messaging.incoming.data.receive.request.customizer=my-customizer\n</code></pre> <p>Receive requests failed with retryable exceptions are retried automatically, by setting the failed request id.</p>"},{"location":"sqs/receiving-aws-sqs-messages/#receive-message-request-pause-and-resume","title":"Receive message request pause and resume","text":"<p>The AWS SQS connector fetches messages by continuously sending receive message requests. If messages are not processed in a timely manner, the connector pauses fetching messages until queued messages are processed.</p> <p>The pause resume can be disabled using the <code>receive.request.pause.resume</code> connector attribute.</p> <pre><code>mp.messaging.incoming.data.receive.request.pause.resume=false\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#deserialization","title":"Deserialization","text":"<p>The connector converts incoming SQS Messages into Reactive Messaging <code>Message&lt;T&gt;</code> instances.</p> <p>The payload type <code>T</code> is determined based on the value of the SQS message attribute <code>_classname</code>.</p> <p>If you send messages with the AWS SQS connector (outbound connector), the <code>_classname</code> attribute is automatically added to the message. The primitive types are transformed from the string representation to the corresponding Java type. For objects, if one of the <code>JsonMapping</code> modules is present on the classpath, the connector used that JSON module to deserialize the message body to objects.</p> <p>If the <code>_classname</code> attribute is not present, the payload is deserialized as a <code>String</code>.</p> <pre><code>@ApplicationScoped\npublic static class Generator {\n\n    @Outgoing(\"to-rabbitmq\")\n    public Multi&lt;Price&gt; prices() {\n        AtomicInteger count = new AtomicInteger();\n        return Multi.createFrom().ticks().every(Duration.ofMillis(1000))\n                .map(l -&gt; new Price().setPrice(count.incrementAndGet()))\n                .onOverflow().drop();\n    }\n\n}\n\n@ApplicationScoped\npublic static class Consumer {\n\n    List&lt;Price&gt; prices = new CopyOnWriteArrayList&lt;&gt;();\n\n    @Incoming(\"from-rabbitmq\")\n    public void consume(Price price) {\n        prices.add(price);\n    }\n\n    public List&lt;Price&gt; list() {\n        return prices;\n    }\n}\n\npublic static class Price {\n    public int price;\n\n    public Price setPrice(int price) {\n        this.price = price;\n        return this;\n    }\n}\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#inbound-metadata","title":"Inbound Metadata","text":"<p>Messages coming from SQS contain an instance of SqsIncomingMetadata in the metadata.</p> <p>SQS message attributes can be accessed from the metadata either by name or by the <code>MessageAttributeValue</code> object.</p> <pre><code>package sqs.inbound;\n\nimport java.util.Map;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\n\nimport io.smallrye.reactive.messaging.aws.sqs.SqsIncomingMetadata;\nimport software.amazon.awssdk.services.sqs.model.MessageAttributeValue;\n\n@ApplicationScoped\npublic class SqsMetadataExample {\n\n    @Incoming(\"queue\")\n    public void metadata(String body, SqsIncomingMetadata metadata) {\n        Map&lt;String, MessageAttributeValue&gt; attributes = metadata.getMessage().messageAttributes();\n        attributes.forEach((k, v) -&gt; System.out.println(k + \" -&gt; \" + v.stringValue()));\n        System.out.println(\"Message body: \" + body);\n    }\n}\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#acknowledgement-strategies","title":"Acknowledgement Strategies","text":"<p>The default strategy for acknowledging AWS SQS Message is to delete the message from the queue. You can set the <code>ack-strategy</code> attribute to <code>ignore</code> if you want to ignore the message.</p> <p>[NOTE] Deprecated     <code>ack.delete</code> attribute is deprecated and will be removed in a future release.</p> <p>You can implement a custom strategy by implementing the SqsAckHandler, interface with a <code>Factory</code> class and registering it as a CDI bean with an <code>@Identifier</code>.</p> <pre><code>package sqs.inbound;\n\nimport java.util.function.BiConsumer;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsAckHandler;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsConnectorIncomingConfiguration;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsMessage;\nimport io.vertx.mutiny.core.Vertx;\nimport software.amazon.awssdk.services.sqs.SqsAsyncClient;\n\npublic class SqsCustomAckStrategy implements SqsAckHandler {\n\n    @ApplicationScoped\n    @Identifier(\"custom\")\n    public static class Factory implements SqsAckHandler.Factory {\n\n        @Override\n        public SqsAckHandler create(SqsConnectorIncomingConfiguration conf,\n                Vertx vertx,\n                SqsAsyncClient client,\n                Uni&lt;String&gt; queueUrlUni,\n                BiConsumer&lt;Throwable, Boolean&gt; reportFailure) {\n            return new SqsCustomAckStrategy();\n        }\n    }\n\n    @Override\n    public Uni&lt;Void&gt; handle(SqsMessage&lt;?&gt; message) {\n        return Uni.createFrom().voidItem()\n                .emitOn(message::runOnMessageContext);\n    }\n}\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#failure-strategies","title":"Failure Strategies","text":"<p>The default strategy for handling message processing failures is <code>ignore</code>. It lets the visibility timeout of the message consumer to expire and reconsume the message.</p> <p>Other possible strategies are:</p> <ul> <li><code>fail</code>: the failure is logged and the channel fail-stops.</li> <li><code>delete</code>: the message is removed from the queue.</li> <li><code>visibility</code>: the message visibility timeout is reset to 0.</li> </ul> <p>You can implement a custom strategy by implementing the SqsFailureHandler, interface with a <code>Factory</code> class and registering it as a CDI bean with an <code>@Identifier</code>.</p> <pre><code>package sqs.inbound;\n\nimport java.util.function.BiConsumer;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\n\nimport io.smallrye.common.annotation.Identifier;\nimport io.smallrye.mutiny.Uni;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsFailureHandler;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsMessage;\nimport software.amazon.awssdk.services.sqs.SqsAsyncClient;\n\npublic class SqsCustomNackStrategy implements SqsFailureHandler {\n\n    @ApplicationScoped\n    @Identifier(\"custom\")\n    public static class Factory implements SqsFailureHandler.Factory {\n\n        @Override\n        public SqsFailureHandler create(String channel, SqsAsyncClient client, Uni&lt;String&gt; queueUrlUni,\n                BiConsumer&lt;Throwable, Boolean&gt; reportFailure) {\n            return new SqsCustomNackStrategy();\n        }\n    }\n\n    @Override\n    public Uni&lt;Void&gt; handle(SqsMessage&lt;?&gt; message, Metadata metadata, Throwable throwable) {\n        return Uni.createFrom().voidItem()\n                .emitOn(message::runOnMessageContext);\n    }\n}\n</code></pre>"},{"location":"sqs/receiving-aws-sqs-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default ack-strategy The identifier for the bean implementing ack strategy factory. Strategies: 'delete', 'ignore' string false <code>delete</code> ack.delete deprecated - Whether the acknowledgement deletes the message from the queue. Deprecated, use ack-strategy instead boolean false credentials-provider The credential provider to be used in the client string false endpoint-override The endpoint override string false failure-strategy The identifier for the bean implementing failure strategy factory. Strategies: 'ignore', 'fail', 'visibility', 'delete' string false <code>ignore</code> health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> max-number-of-messages The maximum number of messages to receive int false <code>10</code> queue The name of the SQS queue, defaults to channel name if not provided string false queue.url The url of the SQS queue string false receive.request.customizer The identifier for the bean implementing a customizer to receive requests, defaults to channel name if not provided string false receive.request.message-attribute-names The message attribute names to retrieve when receiving messages. string false receive.request.pause.resume Whether the polling must be paused when the application does not request items and resume when it does. This allows implementing back-pressure based on the application capacity. Note that polling is not stopped, but will not retrieve any records when paused. boolean false <code>true</code> receive.request.retries If set to a positive number, the connector will try to retry the request that was not delivered successfully (with a potentially transient error) until the number of retries is reached. If set to 0, retries are disabled. long false <code>2147483647</code> region The name of the SQS region string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code> visibility-timeout The duration in seconds that the received messages are hidden from subsequent retrieve requests after being retrieved by a receive request int false wait-time-seconds The maximum amount of time in seconds to wait for messages to be received int false <code>1</code>"},{"location":"sqs/sending-aws-sqs-messages/","title":"Sending AWS SQS Messages","text":"<p>The AWS SQS connector allows you to send messages to an AWS SQS queue.</p>"},{"location":"sqs/sending-aws-sqs-messages/#sending-messages","title":"Sending messages","text":"<p>Before you start, you need to have an AWS account and an SQS queue created. To send messages to an SQS queue, you need to create a method that produces messages to the queue.</p> <p>Then, your application can send <code>Message&lt;String&gt;</code> to the prices channel. It can use <code>String</code> payloads as in the following snippet:</p> <pre><code>package sqs.outbound;\n\nimport java.time.Duration;\nimport java.util.UUID;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\n\n@ApplicationScoped\npublic class SqsStringProducer {\n\n    @Outgoing(\"data\")\n    public Multi&lt;String&gt; generate() {\n        // It emits a UUID every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; UUID.randomUUID().toString());\n    }\n\n}\n</code></pre> <p>Or, you can send <code>Message&lt;Double&gt;</code>, which affords the opportunity to explicitly specify metadata on the outgoing message:</p> <pre><code>package sqs.outbound;\n\nimport java.time.Duration;\nimport java.util.UUID;\n\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Message;\nimport org.eclipse.microprofile.reactive.messaging.Metadata;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.reactive.messaging.aws.sqs.SqsOutboundMetadata;\n\n@ApplicationScoped\npublic class SqsMessageStringProducer {\n\n    @Outgoing(\"prices\")\n    public Multi&lt;Message&lt;String&gt;&gt; generate() {\n        // It emits a UUID every second\n        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n                .map(x -&gt; Message.of(UUID.randomUUID().toString(),\n                        Metadata.of(SqsOutboundMetadata.builder()\n                                .groupId(\"group-1\")\n                                .build())));\n    }\n}\n</code></pre>"},{"location":"sqs/sending-aws-sqs-messages/#sending-messages-in-batch","title":"Sending messages in batch","text":"<p>You can configure the outbound channel to send messages in batch of maximum 10 messages (AWS SQS limitation).</p> <p>You can customize the size of batches, <code>10</code> being the default batch size, and the delay to wait for new messages to be added to the batch, 3000ms being the default delay:</p> <pre><code>mp.messaging.outgoing.prices.connector=smallrye-sqs\nmp.messaging.outgoing.prices.queue=prices\nmp.messaging.outgoing.prices.batch=true\nmp.messaging.outgoing.prices.batch-size=5\nmp.messaging.outgoing.prices.batch-delay=3000\n</code></pre>"},{"location":"sqs/sending-aws-sqs-messages/#serialization","title":"Serialization","text":"<p>When sending a <code>Message&lt;T&gt;</code>, the connector converts the message into a AWS SQS Message. How the message is converted depends on the payload type:</p> <ul> <li>If the payload is of type <code>SendMessageRequest</code> it is sent as is.</li> <li>If the payload is of type <code>SendMessageRequest.Builder</code>, the queue url is set and sent.</li> <li>If the payload is of type <code>software.amazon.awssdk.services.sqs.model.Message</code> it is usd to set the message body and attributes.</li> <li>If the payload is of primitive types the paylaod is converted to String and the message attribute <code>_classname</code> is set to the class name of the payload.</li> <li>If the payload is of any other object type, the payload is serialized (using the <code>JsonMapping</code> implementation discovered) and the message attribute <code>_classname</code> is set to the class name of the payload.</li> </ul> <p>If the message payload cannot be serialized to JSON, the message is nacked.</p>"},{"location":"sqs/sending-aws-sqs-messages/#outbound-metadata","title":"Outbound Metadata","text":"<p>When sending <code>Messages</code>, you can add an instance of SqsOutboundMetadata to influence how the message is handled by AWS SQS. For example, you can configure the routing key, timestamp and headers:</p> <pre><code>final SqsOutboundMetadata metadata = SqsOutboundMetadata.builder()\n        .messageAttributes(Map.of(\"my-attribute\", MessageAttributeValue.builder()\n                .dataType(\"String\").stringValue(\"my-value\").build()))\n        .groupId(\"group-1\")\n        .build();\n\n// Add `metadata` to the metadata of the outgoing message.\nreturn Message.of(\"Hello\", Metadata.of(metadata));\n</code></pre>"},{"location":"sqs/sending-aws-sqs-messages/#acknowledgement","title":"Acknowledgement","text":"<p>By default, the Reactive Messaging <code>Message</code> is acknowledged when the send message request is successful. If the message is not sent successfully, the message is nacked.</p>"},{"location":"sqs/sending-aws-sqs-messages/#configuration-reference","title":"Configuration Reference","text":"Attribute (alias) Description Type Mandatory Default batch When set, sends messages in batches of maximum 10 messages boolean false <code>false</code> batch-delay In batch send mode, the maximum delay in milliseconds to wait for messages to be included in the batch int false <code>3000</code> batch-size In batch send mode, the maximum number of messages to include in batch, currently SQS maximum is 10 messages int false <code>10</code> credentials-provider The credential provider to be used in the client string false endpoint-override The endpoint override string false group.id When set, sends messages with the specified group id string false health-enabled Whether health reporting is enabled (default) or disabled boolean false <code>true</code> queue The name of the SQS queue, defaults to channel name if not provided string false queue.url The url of the SQS queue string false region The name of the SQS region string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false <code>true</code>"},{"location":"sqs/sqs/","title":"AWS SQS Connector","text":"<p>Preview</p> <p>The AWS SQS Connector is currently in preview.</p> <p>The AWS SQS Connector adds support for AWS SQS to Reactive Messaging.</p> <p>With this connector, your application can:</p> <ul> <li>receive messages from a RabbitMQ queue</li> <li>send messages to a RabbitMQ exchange</li> </ul> <p>The AWS SQS connector is based on the AWS SDK for Java V2.</p>"},{"location":"sqs/sqs/#using-the-aws-sqs-connector","title":"Using the AWS SQS connector","text":"<p>To use the AWS SQS Connector, add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.smallrye.reactive&lt;/groupId&gt;\n  &lt;artifactId&gt;smallrye-reactive-messaging-aws-sqs&lt;/artifactId&gt;\n  &lt;version&gt;4.33.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The connector name is: <code>smallrye-sqs</code>.</p> <p>So, to indicate that a channel is managed by this connector you need: <pre><code># Inbound\nmp.messaging.incoming.[channel-name].connector=smallrye-sqs\n\n# Outbound\nmp.messaging.outgoing.[channel-name].connector=smallrye-sqs\n</code></pre></p>"},{"location":"sqs/sqs/#configuration","title":"Configuration","text":"<p>When available the AWS SQS connector discovers the SQS client as a CDI bean. This allows using the connector with the Quarkiverse AWS SQS extension. If the SQS client is not available as a CDI bean, the connector creates a new client using the provided configuration.</p> <ul> <li><code>queue</code> - The name of the SQS queue, defaults to channel name if not provided.</li> <li><code>region</code> - The name of the SQS region.</li> <li><code>endpoint-override</code> - The endpoint url override.</li> <li><code>credentials-provider</code> - The fully qualified class name of the credential provider to be used in the client, if not provided the default provider chain is used.</li> </ul>"},{"location":"sqs/sqs/#additional-resources","title":"Additional Resources","text":"<ul> <li>AWS SQS Documentation</li> <li>Quarkiverse AWS SQS Extension</li> </ul>"}]}