{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SmallRye Reactive Messaging SmallRye Reactive Messaging is a framework for building event-driven, data streaming, and event-sourcing applications. It lets your application interact using various messaging technologies such as Apache Kafka, AMQP or MQTT. The framework provides a flexible programming model bridging CDI and event-driven. SmallRye Reactive Messaging is an implementation of the Eclipse MicroProfile Reactive Messaging specification 3.0. Event-Driven Architectures SmallRye Reactive Messaging is used in Quarkus , and Open Liberty . Start with the Getting Started guide to know how it can be used and introduce various features.","title":"Home"},{"location":"#smallrye-reactive-messaging","text":"SmallRye Reactive Messaging is a framework for building event-driven, data streaming, and event-sourcing applications. It lets your application interact using various messaging technologies such as Apache Kafka, AMQP or MQTT. The framework provides a flexible programming model bridging CDI and event-driven. SmallRye Reactive Messaging is an implementation of the Eclipse MicroProfile Reactive Messaging specification 3.0. Event-Driven Architectures SmallRye Reactive Messaging is used in Quarkus , and Open Liberty . Start with the Getting Started guide to know how it can be used and introduce various features.","title":"SmallRye Reactive Messaging"},{"location":"getting-started/","text":"Getting Started The easiest way to start using SmallRye Reactive Messaging is from Quarkus . SmallRye Reactive Messaging can also be used standalone, or with Open Liberty . First, go to code.quarkus.io . Select the smallrye-reactive-messaging extension (already done if you use the link), and then click on the generate button to download the code. One downloaded, unzip the project and import it in your IDE. If you look at the pom.xml file, you will see the following dependency: 1 2 3 4 <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging </artifactId> </dependency> It provides the support for SmallRye Reactive Messaging. Ok, so far so good, but we need event-driven beans . Create the quickstart package, and copy the following class into it: For instance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package quickstart ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class ReactiveMessagingExample { @Outgoing ( \"source\" ) public Multi < String > source () { return Multi . createFrom (). items ( \"hello\" , \"from\" , \"SmallRye\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"processed-a\" ) public String toUpperCase ( String payload ) { return payload . toUpperCase (); } @Incoming ( \"processed-a\" ) @Outgoing ( \"processed-b\" ) public Multi < String > filter ( Multi < String > input ) { return input . select (). where ( item -> item . length () > 4 ); } @Incoming ( \"processed-b\" ) public void sink ( String word ) { System . out . println ( \">> \" + word ); } } This class contains a set of methods: producing messages ( source ) processing messages ( toUpperCase ) transforming the stream by skipping messages ( filter ) consuming messages ( sink ) Each of these methods are connected through channels . Now, let's see this in action. For the terminal, run: 1 > ./mvnw quarkus:dev Running the previous example should give the following output: 1 2 3 4 >> HELLO >> SMALLRYE >> REACTIVE >> MESSAGE Of course, this is a very simple example. To go further, let's have a look to the core concepts behind SmallRye Reactive Messaging.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The easiest way to start using SmallRye Reactive Messaging is from Quarkus . SmallRye Reactive Messaging can also be used standalone, or with Open Liberty . First, go to code.quarkus.io . Select the smallrye-reactive-messaging extension (already done if you use the link), and then click on the generate button to download the code. One downloaded, unzip the project and import it in your IDE. If you look at the pom.xml file, you will see the following dependency: 1 2 3 4 <dependency> <groupId> io.quarkus </groupId> <artifactId> quarkus-smallrye-reactive-messaging </artifactId> </dependency> It provides the support for SmallRye Reactive Messaging. Ok, so far so good, but we need event-driven beans . Create the quickstart package, and copy the following class into it: For instance: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package quickstart ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class ReactiveMessagingExample { @Outgoing ( \"source\" ) public Multi < String > source () { return Multi . createFrom (). items ( \"hello\" , \"from\" , \"SmallRye\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"processed-a\" ) public String toUpperCase ( String payload ) { return payload . toUpperCase (); } @Incoming ( \"processed-a\" ) @Outgoing ( \"processed-b\" ) public Multi < String > filter ( Multi < String > input ) { return input . select (). where ( item -> item . length () > 4 ); } @Incoming ( \"processed-b\" ) public void sink ( String word ) { System . out . println ( \">> \" + word ); } } This class contains a set of methods: producing messages ( source ) processing messages ( toUpperCase ) transforming the stream by skipping messages ( filter ) consuming messages ( sink ) Each of these methods are connected through channels . Now, let's see this in action. For the terminal, run: 1 > ./mvnw quarkus:dev Running the previous example should give the following output: 1 2 3 4 >> HELLO >> SMALLRYE >> REACTIVE >> MESSAGE Of course, this is a very simple example. To go further, let's have a look to the core concepts behind SmallRye Reactive Messaging.","title":"Getting Started"},{"location":"amqp/amqp/","text":"AMQP 1.0 Connector The AMQP Connector adds support for AMQP 1.0 to Reactive Messaging. Advanced Message Queuing Protocol 1.0 (AMQP 1.0) is an open standard for passing business messages between applications or organizations. With this connector, your application can: receive messages from an AMQP Broker or Router. send Message to an AMQP address The AMQP connector is based on the Vert.x AMQP Client . Using the AMQP connector To use the AMQP Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-amqp </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-amqp . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-amqp # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-amqp RabbitMQ To use RabbitMQ, refer to Using RabbitMQ .","title":"AMQP 1.0 Connector"},{"location":"amqp/amqp/#amqp-10-connector","text":"The AMQP Connector adds support for AMQP 1.0 to Reactive Messaging. Advanced Message Queuing Protocol 1.0 (AMQP 1.0) is an open standard for passing business messages between applications or organizations. With this connector, your application can: receive messages from an AMQP Broker or Router. send Message to an AMQP address The AMQP connector is based on the Vert.x AMQP Client .","title":"AMQP 1.0 Connector"},{"location":"amqp/amqp/#using-the-amqp-connector","text":"To use the AMQP Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-amqp </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-amqp . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-amqp # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-amqp RabbitMQ To use RabbitMQ, refer to Using RabbitMQ .","title":"Using the AMQP connector"},{"location":"amqp/client-customization/","text":"Customizing the underlying AMQP client You can customize the underlying AMQP Client configuration by producing an instance of AmqpClientOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Produces @Identifier ( \"my-options\" ) public AmqpClientOptions getNamedOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return new AmqpClientOptions () . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . addEnabledSaslMechanism ( \"EXTERNAL\" ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ) . setContainerId ( \"my-container\" ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name = my-named-options Client capabilities Both incoming and outgoing AMQP channels can be configured with a list of capabilities to declare during sender and receiver connections with the AMQP broker. Note that supported capability names are broker specific. 1 2 3 mp.messaging.incoming.sink.capabilities = temporary-topic ... mp.messaging.outgoing.source.capabilities = shared","title":"Client Customization"},{"location":"amqp/client-customization/#customizing-the-underlying-amqp-client","text":"You can customize the underlying AMQP Client configuration by producing an instance of AmqpClientOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Produces @Identifier ( \"my-options\" ) public AmqpClientOptions getNamedOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return new AmqpClientOptions () . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . addEnabledSaslMechanism ( \"EXTERNAL\" ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ) . setContainerId ( \"my-container\" ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name = my-named-options","title":"Customizing the underlying AMQP client"},{"location":"amqp/client-customization/#client-capabilities","text":"Both incoming and outgoing AMQP channels can be configured with a list of capabilities to declare during sender and receiver connections with the AMQP broker. Note that supported capability names are broker specific. 1 2 3 mp.messaging.incoming.sink.capabilities = temporary-topic ... mp.messaging.outgoing.source.capabilities = shared","title":"Client capabilities"},{"location":"amqp/health/","text":"Health reporting The AMQP connector reports the startup, liveness, and readiness of each inbound (Receiving messages) and outbound (sending messages) channel managed by the connector: Startup :: For both inbound and outbound, the startup probe reports OK when the connection with the broker is established, and the AMQP senders and receivers are opened (the links are attached to the broker). Liveness :: For both inbound and outbound, the liveness check verifies that the connection is established. The check still returns OK if the connection got cut, but we are attempting a reconnection. Readiness :: For the inbound, it checks that the connection is established and the receiver is opened. Unlike the liveness check, this probe reports KO until the connection is re-established. For the outbound, it checks that the connection is established and the sender is opened. Unlike the liveness check, this probe reports KO until the connection is re-established. Note To disable health reporting, set the health-enabled attribute for the channel to false . Note that a message processing failures nacks the message, which is then handled by the failure-strategy. It is the responsibility of the failure-strategy to report the failure and influence the outcome of the checks. The fail failure strategy reports the failure, and so the check will report the fault.","title":"Health Checks"},{"location":"amqp/health/#health-reporting","text":"The AMQP connector reports the startup, liveness, and readiness of each inbound (Receiving messages) and outbound (sending messages) channel managed by the connector: Startup :: For both inbound and outbound, the startup probe reports OK when the connection with the broker is established, and the AMQP senders and receivers are opened (the links are attached to the broker). Liveness :: For both inbound and outbound, the liveness check verifies that the connection is established. The check still returns OK if the connection got cut, but we are attempting a reconnection. Readiness :: For the inbound, it checks that the connection is established and the receiver is opened. Unlike the liveness check, this probe reports KO until the connection is re-established. For the outbound, it checks that the connection is established and the sender is opened. Unlike the liveness check, this probe reports KO until the connection is re-established. Note To disable health reporting, set the health-enabled attribute for the channel to false . Note that a message processing failures nacks the message, which is then handled by the failure-strategy. It is the responsibility of the failure-strategy to report the failure and influence the outcome of the checks. The fail failure strategy reports the failure, and so the check will report the fault.","title":"Health reporting"},{"location":"amqp/rabbitmq/","text":"Using RabbitMQ This connector is for AMQP 1.0. RabbitMQ implements AMQP 0.9.1. RabbitMQ does not provide AMQP 1.0 by default, but there is a plugin for it. To use RabbitMQ with this connector, enable and configure the AMQP 1.0 plugin . Despite the plugin, a few features won\u2019t work with RabbitMQ. Thus, we recommend the following configurations. To receive messages from RabbitMQ: Set durable to false 1 2 mp.messaging.incoming.prices.connector = smallrye-amqp mp.messaging.incoming.prices.durable = false To send messages to RabbitMQ: set the destination address (anonymous sender are not supported) 1 2 mp.messaging.outgoing.generated-price.connector = smallrye-amqp mp.messaging.outgoing.generated-price.address = prices It\u2019s not possible to change the destination dynamically (using message metadata) when using RabbitMQ. The connector automatically detects that the broker does not support anonymous sender (See http://docs.oasis-open.org/amqp/anonterm/v1.0/anonterm-v1.0.html ). Alternatively, you can use the RabbitMQ connector .","title":"Using RabbitMQ"},{"location":"amqp/rabbitmq/#using-rabbitmq","text":"This connector is for AMQP 1.0. RabbitMQ implements AMQP 0.9.1. RabbitMQ does not provide AMQP 1.0 by default, but there is a plugin for it. To use RabbitMQ with this connector, enable and configure the AMQP 1.0 plugin . Despite the plugin, a few features won\u2019t work with RabbitMQ. Thus, we recommend the following configurations. To receive messages from RabbitMQ: Set durable to false 1 2 mp.messaging.incoming.prices.connector = smallrye-amqp mp.messaging.incoming.prices.durable = false To send messages to RabbitMQ: set the destination address (anonymous sender are not supported) 1 2 mp.messaging.outgoing.generated-price.connector = smallrye-amqp mp.messaging.outgoing.generated-price.address = prices It\u2019s not possible to change the destination dynamically (using message metadata) when using RabbitMQ. The connector automatically detects that the broker does not support anonymous sender (See http://docs.oasis-open.org/amqp/anonterm/v1.0/anonterm-v1.0.html ). Alternatively, you can use the RabbitMQ connector .","title":"Using RabbitMQ"},{"location":"amqp/receiving-amqp-messages/","text":"Receiving messages from AMQP The AMQP connector lets you retrieve messages from an AMQP broker or router . The AMQP connector retrieves AMQP Messages and maps each of them into Reactive Messaging Messages . Example Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis ) running, and accessible using the amqp:5672 address (by default it would use localhost:5672 ). Configure your application to receive AMQP Messages on the prices channel as follows: 1 2 3 4 5 6 amqp-host = amqp # <1> amqp-port = 5672 # <2> amqp-username = my-username # <3> amqp-password = my-password # <4> mp.messaging.incoming.prices.connector = smallrye-amqp # <5> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using amqp-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using amqp-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using amqp-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using amqp-password . Instructs the prices channel to be managed by the AMQP connector Note You don\u2019t need to set the AMQP address . By default, it uses the channel name ( prices ). You can configure the address attribute to override it. Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package amqp.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class AmqpPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package amqp.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class AmqpPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message, marking the AMQP message as `accepted`. return price . ack (); } } Deserialization The connector converts incoming AMQP Messages into Reactive Messaging Message<T> instances. T depends on the body of the received AMQP Message. The AMQP Type System defines the supported types. AMQP Body Type <T> AMQP Value containing a AMQP Primitive Type the corresponding Java type AMQP Value using the Binary type byte[] AMQP Sequence List AMQP Data (with binary content) and the content-type is set to application/json JsonObject AMQP Data with a different content-type byte[] If you send objects with this AMQP connector (outbound connector), it gets encoded as JSON and sent as binary. The content-type is set to application/json . You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @ApplicationScoped public static class Generator { @Outgoing ( \"to-amqp\" ) public Multi < Price > prices () { // <1> AtomicInteger count = new AtomicInteger (); return Multi . createFrom (). ticks (). every ( Duration . ofMillis ( 1000 )) . map ( l -> new Price (). setPrice ( count . incrementAndGet ())) . onOverflow (). drop (); } } @ApplicationScoped public static class Consumer { List < Price > prices = new CopyOnWriteArrayList <> (); @Incoming ( \"from-amqp\" ) public void consume ( JsonObject p ) { // <2> Price price = p . mapTo ( Price . class ); // <3> prices . add ( price ); } public List < Price > list () { return prices ; } } The Price instances are automatically encoded to JSON by the connector You can receive it using a JsonObject Then, you can reconstruct the instance using the mapTo method Inbound Metadata Messages coming from AMQP contains an instance of IncomingAmqpMetadata 1 2 3 4 5 6 7 8 9 Optional < IncomingAmqpMetadata > metadata = incoming . getMetadata ( IncomingAmqpMetadata . class ); metadata . ifPresent ( meta -> { String address = meta . getAddress (); String subject = meta . getSubject (); boolean durable = meta . isDurable (); // Use io.vertx.core.json.JsonObject JsonObject properties = meta . getProperties (); // ... }); Acknowledgement When a Reactive Messaging Message associated with an AMQP Message is acknowledged, it informs the broker that the message has been accepted . Failure Management If a message produced from an AMQP message is nacked , a failure strategy is applied. The AMQP connector supports six strategies: fail - fail the application; no more AMQP messages will be processed (default). The AMQP message is marked as rejected. accept - this strategy marks the AMQP message as accepted . The processing continues ignoring the failure. Refer to the accepted delivery state documentation . release - this strategy marks the AMQP message as released . The processing continues with the next message. The broker can redeliver the message. Refer to the released delivery state documentation . reject - this strategy marks the AMQP message as rejected. The processing continues with the next message. Refer to the rejected delivery state documentation . modified-failed - this strategy marks the AMQP message as modified and indicates that it failed (with the delivery-failed attribute). The processing continues with the next message, but the broker may attempt to redeliver the message. Refer to the modified delivery state documentation modified-failed-undeliverable-here - this strategy marks the AMQP message as modified and indicates that it failed (with the delivery-failed attribute). It also indicates that the application cannot process the message, meaning that the broker will not attempt to redeliver the message to this node. The processing continues with the next message. Refer to the modified delivery state documentation Configuration Reference Attribute ( alias ) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false auto-acknowledgement Whether the received AMQP messages must be acknowledged when received boolean false false broadcast Whether the received AMQP messages must be dispatched to multiple subscribers boolean false false capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false 1000 container-id The AMQP container id string false durable Whether AMQP subscription is durable boolean false false failure-strategy Specify the failure strategy to apply when a message produced from an AMQP message is nacked. Accepted values are fail (default), accept , release , reject , modified-failed , modified-failed-undeliverable-here string false fail health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false 3 host (amqp-host) The broker hostname string false localhost link-name The name of the link. If not set, the channel name is used. string false password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false 5672 reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false 10 selector Sets a message selector. This attribute is used to define an apache.org:selector-filter:string filter on the source terminus, using SQL-based syntax to request the server filters which messages are delivered to the receiver (if supported by the server in question). Precise functionality supported and syntax needed can vary depending on the server. string false sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false false username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false You can also pass any property supported by the Vert.x AMQP client as attribute. To use an existing address or queue , you need to configure the address , container-id and, optionally, the link-name attributes. For example, if you have an Apache Artemis broker configured with: 1 2 3 4 5 6 7 <queues> <queue name= \"people\" > <address> people </address> <durable> true </durable> <user> artemis </user> </queue> </queues> You need the following configuration: 1 2 3 4 mp.messaging.incoming.people.connector = smallrye-amqp mp.messaging.incoming.people.durable = true mp.messaging.incoming.people.address = people mp.messaging.incoming.people.container-id = people You may need to configure the link-name attribute, if the queue name is not the channel name: 1 2 3 4 5 mp.messaging.incoming.people-in.connector = smallrye-amqp mp.messaging.incoming.people-in.durable = true mp.messaging.incoming.people-in.address = people mp.messaging.incoming.people-in.container-id = people mp.messaging.incoming.people-in.link-name = people Receiving Cloud Events The AMQP connector supports Cloud Events . When the connector detects a structured or binary Cloud Events, it adds a IncomingCloudEventMetadata into the metadata of the Message . IncomingCloudEventMetadata contains accessors to the mandatory and optional Cloud Event attributes. If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata. Binary Cloud Events For binary Cloud Events, all mandatory Cloud Event attributes must be set in the AMQP application properties, prefixed by cloudEvents: (as mandated by the protocol binding ). The connector considers headers starting with the cloudEvents: prefix but not listed in the specification as extensions. You can access them using the getExtension method from IncomingCloudEventMetadata . The datacontenttype attribute is mapped to the content-type header of the record. Structured Cloud Events For structured Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value. Structured Cloud Event must set the content-type header of the record to application/cloudevents+json; charset=UTF-8 . The message body must be a valid JSON object containing at least all the mandatory Cloud Events attributes. If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event data .","title":"Receiving messages"},{"location":"amqp/receiving-amqp-messages/#receiving-messages-from-amqp","text":"The AMQP connector lets you retrieve messages from an AMQP broker or router . The AMQP connector retrieves AMQP Messages and maps each of them into Reactive Messaging Messages .","title":"Receiving messages from AMQP"},{"location":"amqp/receiving-amqp-messages/#example","text":"Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis ) running, and accessible using the amqp:5672 address (by default it would use localhost:5672 ). Configure your application to receive AMQP Messages on the prices channel as follows: 1 2 3 4 5 6 amqp-host = amqp # <1> amqp-port = 5672 # <2> amqp-username = my-username # <3> amqp-password = my-password # <4> mp.messaging.incoming.prices.connector = smallrye-amqp # <5> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using amqp-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using amqp-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using amqp-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using amqp-password . Instructs the prices channel to be managed by the AMQP connector Note You don\u2019t need to set the AMQP address . By default, it uses the channel name ( prices ). You can configure the address attribute to override it. Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package amqp.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class AmqpPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package amqp.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class AmqpPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message, marking the AMQP message as `accepted`. return price . ack (); } }","title":"Example"},{"location":"amqp/receiving-amqp-messages/#deserialization","text":"The connector converts incoming AMQP Messages into Reactive Messaging Message<T> instances. T depends on the body of the received AMQP Message. The AMQP Type System defines the supported types. AMQP Body Type <T> AMQP Value containing a AMQP Primitive Type the corresponding Java type AMQP Value using the Binary type byte[] AMQP Sequence List AMQP Data (with binary content) and the content-type is set to application/json JsonObject AMQP Data with a different content-type byte[] If you send objects with this AMQP connector (outbound connector), it gets encoded as JSON and sent as binary. The content-type is set to application/json . You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @ApplicationScoped public static class Generator { @Outgoing ( \"to-amqp\" ) public Multi < Price > prices () { // <1> AtomicInteger count = new AtomicInteger (); return Multi . createFrom (). ticks (). every ( Duration . ofMillis ( 1000 )) . map ( l -> new Price (). setPrice ( count . incrementAndGet ())) . onOverflow (). drop (); } } @ApplicationScoped public static class Consumer { List < Price > prices = new CopyOnWriteArrayList <> (); @Incoming ( \"from-amqp\" ) public void consume ( JsonObject p ) { // <2> Price price = p . mapTo ( Price . class ); // <3> prices . add ( price ); } public List < Price > list () { return prices ; } } The Price instances are automatically encoded to JSON by the connector You can receive it using a JsonObject Then, you can reconstruct the instance using the mapTo method","title":"Deserialization"},{"location":"amqp/receiving-amqp-messages/#inbound-metadata","text":"Messages coming from AMQP contains an instance of IncomingAmqpMetadata 1 2 3 4 5 6 7 8 9 Optional < IncomingAmqpMetadata > metadata = incoming . getMetadata ( IncomingAmqpMetadata . class ); metadata . ifPresent ( meta -> { String address = meta . getAddress (); String subject = meta . getSubject (); boolean durable = meta . isDurable (); // Use io.vertx.core.json.JsonObject JsonObject properties = meta . getProperties (); // ... });","title":"Inbound Metadata"},{"location":"amqp/receiving-amqp-messages/#acknowledgement","text":"When a Reactive Messaging Message associated with an AMQP Message is acknowledged, it informs the broker that the message has been accepted .","title":"Acknowledgement"},{"location":"amqp/receiving-amqp-messages/#failure-management","text":"If a message produced from an AMQP message is nacked , a failure strategy is applied. The AMQP connector supports six strategies: fail - fail the application; no more AMQP messages will be processed (default). The AMQP message is marked as rejected. accept - this strategy marks the AMQP message as accepted . The processing continues ignoring the failure. Refer to the accepted delivery state documentation . release - this strategy marks the AMQP message as released . The processing continues with the next message. The broker can redeliver the message. Refer to the released delivery state documentation . reject - this strategy marks the AMQP message as rejected. The processing continues with the next message. Refer to the rejected delivery state documentation . modified-failed - this strategy marks the AMQP message as modified and indicates that it failed (with the delivery-failed attribute). The processing continues with the next message, but the broker may attempt to redeliver the message. Refer to the modified delivery state documentation modified-failed-undeliverable-here - this strategy marks the AMQP message as modified and indicates that it failed (with the delivery-failed attribute). It also indicates that the application cannot process the message, meaning that the broker will not attempt to redeliver the message to this node. The processing continues with the next message. Refer to the modified delivery state documentation","title":"Failure Management"},{"location":"amqp/receiving-amqp-messages/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false auto-acknowledgement Whether the received AMQP messages must be acknowledged when received boolean false false broadcast Whether the received AMQP messages must be dispatched to multiple subscribers boolean false false capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false 1000 container-id The AMQP container id string false durable Whether AMQP subscription is durable boolean false false failure-strategy Specify the failure strategy to apply when a message produced from an AMQP message is nacked. Accepted values are fail (default), accept , release , reject , modified-failed , modified-failed-undeliverable-here string false fail health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false 3 host (amqp-host) The broker hostname string false localhost link-name The name of the link. If not set, the channel name is used. string false password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false 5672 reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false 10 selector Sets a message selector. This attribute is used to define an apache.org:selector-filter:string filter on the source terminus, using SQL-based syntax to request the server filters which messages are delivered to the receiver (if supported by the server in question). Precise functionality supported and syntax needed can vary depending on the server. string false sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false false username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false You can also pass any property supported by the Vert.x AMQP client as attribute. To use an existing address or queue , you need to configure the address , container-id and, optionally, the link-name attributes. For example, if you have an Apache Artemis broker configured with: 1 2 3 4 5 6 7 <queues> <queue name= \"people\" > <address> people </address> <durable> true </durable> <user> artemis </user> </queue> </queues> You need the following configuration: 1 2 3 4 mp.messaging.incoming.people.connector = smallrye-amqp mp.messaging.incoming.people.durable = true mp.messaging.incoming.people.address = people mp.messaging.incoming.people.container-id = people You may need to configure the link-name attribute, if the queue name is not the channel name: 1 2 3 4 5 mp.messaging.incoming.people-in.connector = smallrye-amqp mp.messaging.incoming.people-in.durable = true mp.messaging.incoming.people-in.address = people mp.messaging.incoming.people-in.container-id = people mp.messaging.incoming.people-in.link-name = people","title":"Configuration Reference"},{"location":"amqp/receiving-amqp-messages/#receiving-cloud-events","text":"The AMQP connector supports Cloud Events . When the connector detects a structured or binary Cloud Events, it adds a IncomingCloudEventMetadata into the metadata of the Message . IncomingCloudEventMetadata contains accessors to the mandatory and optional Cloud Event attributes. If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.","title":"Receiving Cloud Events"},{"location":"amqp/receiving-amqp-messages/#binary-cloud-events","text":"For binary Cloud Events, all mandatory Cloud Event attributes must be set in the AMQP application properties, prefixed by cloudEvents: (as mandated by the protocol binding ). The connector considers headers starting with the cloudEvents: prefix but not listed in the specification as extensions. You can access them using the getExtension method from IncomingCloudEventMetadata . The datacontenttype attribute is mapped to the content-type header of the record.","title":"Binary Cloud Events"},{"location":"amqp/receiving-amqp-messages/#structured-cloud-events","text":"For structured Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value. Structured Cloud Event must set the content-type header of the record to application/cloudevents+json; charset=UTF-8 . The message body must be a valid JSON object containing at least all the mandatory Cloud Events attributes. If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event data .","title":"Structured Cloud Events"},{"location":"amqp/sending-amqp-messages/","text":"Sending messages to AMQP The AMQP connector can write Reactive Messaging Messages as AMQP Messages. Example Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis ) running, and accessible using the amqp:5672 address (by default it would use localhost:5672 ). Configure your application to send the messages from the prices channel as AMQP Message as follows: 1 2 3 4 5 6 amqp-host = amqp # <1> amqp-port = 5672 # <2> amqp-username = my-username # <3> amqp-password = my-password # <4> mp.messaging.outgoing.prices.connector = smallrye-amqp # <5> 1. Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using amqp-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using amqp-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using amqp-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using amqp-password . Instructs the prices channel to be managed by the AMQP connector Note You don\u2019t need to set the address . By default, it uses the channel name ( prices ). You can configure the address attribute to override it. Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package amqp.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class AmqpPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package amqp.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class AmqpPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } } Serialization When receiving a Message<T> , the connector convert the message into an AMQP Message. The payload is converted to the AMQP Message body . T AMQP Message Body primitive types or String AMQP Value with the payload Instant or UUID AMQP Value using the corresponding AMQP Type JsonObject or JsonArray AMQP Data using a binary content. The content-type is set to application/json io.vertx.mutiny.core.buffer.Buffer AMQP Data using a binary content. No content-type set Any other class The payload is converted to JSON (using a Json Mapper). The result is wrapped into AMQP Data using a binary content. The content-type is set to application/json If the message payload cannot be serialized to JSON, the message is nacked . Outbound Metadata When sending Messages , you can add an instance of OutgoingAmqpMetadata to influence how the message is going to be sent to AMQP. For example, you can configure the subjects, properties: 1 2 3 4 5 6 7 8 OutgoingAmqpMetadata metadata = OutgoingAmqpMetadata . builder () . withDurable ( true ) . withSubject ( \"my-subject\" ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); Dynamic address names Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the address inside your application configuration file, but instead, use the outbound metadata to set the address. For example, you can send to a dynamic address based on the incoming message: 1 2 3 4 5 6 7 8 9 String addressName = selectAddressFromIncommingMessage ( incoming ); OutgoingAmqpMetadata metadata = OutgoingAmqpMetadata . builder () . withAddress ( addressName ) . withDurable ( true ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); Note To be able to set the address per message, the connector is using an anonymous sender . Acknowledgement By default, the Reactive Messaging Message is acknowledged when the broker acknowledged the message. When using routers, this acknowledgement may not be enabled. In this case, configure the auto-acknowledgement attribute to acknowledge the message as soon as it has been sent to the router. If an AMQP message is rejected/released/modified by the broker (or cannot be sent successfully), the message is nacked. Back Pressure and Credits The back-pressure is handled by AMQP credits . The outbound connector only requests the amount of allowed credits. When the amount of credits reaches 0, it waits (in a non-blocking fashion) until the broker grants more credits to the AMQP sender. Configuration Reference Attribute ( alias ) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default datacontenttype attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the datacontenttype attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default dataschema attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the dataschema attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the time attribute into the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the time attribute itself boolean false true cloud-events-mode The Cloud Event mode ( structured or binary (default)). Indicates how are written the cloud events in the outgoing record string false binary cloud-events-source (cloud-events-default-source) Configure the default source attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the source attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default subject attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the subject attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default type attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the type attribute itself string false connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false 1000 container-id The AMQP container id string false credit-retrieval-period The period (in milliseconds) between two attempts to retrieve the credits granted by the broker. This time is used when the sender run out of credits. int false 2000 durable Whether sent AMQP messages are marked durable boolean false false health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false 3 host (amqp-host) The broker hostname string false localhost link-name The name of the link. If not set, the channel name is used. string false merge Whether the connector should allow multiple upstreams boolean false false password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false 5672 reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false 10 sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true ttl The time-to-live of the send AMQP messages. 0 to disable the TTL long false 0 use-anonymous-sender Whether or not the connector should use an anonymous sender. Default value is true if the broker supports it, false otherwise. If not supported, it is not possible to dynamically change the destination address. boolean false use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false false username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false You can also pass any property supported by the Vert.x AMQP client as attribute. Using existing destinations To use an existing address or queue , you need to configure the address , container-id and, optionally, the link-name attributes. For example, if you have an Apache Artemis broker configured with: 1 2 3 4 5 6 7 <queues> <queue name= \"people\" > <address> people </address> <durable> true </durable> <user> artemis </user> </queue> </queues> You need the following configuration: 1 2 3 4 mp.messaging.outgoing.people.connector = smallrye-amqp mp.messaging.outgoing.people.durable = true mp.messaging.outgoing.people.address = people mp.messaging.outgoing.people.container-id = people You may need to configure the link-name attribute, if the queue name is not the channel name: 1 2 3 4 5 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.durable = true mp.messaging.outgoing.people-out.address = people mp.messaging.outgoing.people-out.container-id = people mp.messaging.outgoing.people-out.link-name = people To use a MULTICAST queue, you need to provide the FQQN (Fully-qualified queue name) instead of just the name of the queue: 1 2 3 4 5 6 7 8 9 10 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.durable = true mp.messaging.outgoing.people-out.address = foo mp.messaging.outgoing.people-out.container-id = foo mp.messaging.incoming.people-out.connector = smallrye-amqp mp.messaging.incoming.people-out.durable = true mp.messaging.incoming.people-out.address = foo::bar # Note the syntax: address-name::queue-name mp.messaging.incoming.people-out.container-id = bar mp.messaging.incoming.people-out.link-name = people More details about the AMQP Address model can be found on the Artemis documentation . Sending Cloud Events The AMQP connector supports Cloud Events . The connector sends the outbound record as Cloud Events if: the message metadata contains an io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instance, the channel configuration defines the cloud-events-type and cloud-events-source attributes. You can create io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instances using: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package amqp.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class AmqpCloudEventProcessor { @Outgoing ( \"cloud-events\" ) public Message < String > toCloudEvents ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greetings\" ) . withSource ( URI . create ( \"http://example.com\" )) . withSubject ( \"greeting-message\" ) . build ()); } } If the metadata does not contain an id, the connector generates one (random UUID). The type and source can be configured per message or at the channel level using the cloud-events-type and cloud-events-source attributes. Other attributes are also configurable. The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package amqp.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class AmqpCloudEventMultipleProcessors { @Incoming ( \"source\" ) @Outgoing ( \"processed\" ) public Message < String > process ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greeting\" ) . build ()); } @SuppressWarnings ( \"unchecked\" ) @Incoming ( \"processed\" ) @Outgoing ( \"cloud-events\" ) public Message < String > process2 ( Message < String > in ) { OutgoingCloudEventMetadata < String > metadata = in . getMetadata ( OutgoingCloudEventMetadata . class ) . orElseGet (() -> OutgoingCloudEventMetadata . builder (). build ()); return in . addMetadata ( OutgoingCloudEventMetadata . from ( metadata ) . withSource ( URI . create ( \"source://me\" )) . withSubject ( \"test\" ) . build ()); } } By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the cloud-events-mode to structured . Only JSON is supported, so the created records had its content-type header set to application/cloudevents+json; charset=UTF-8 Note you can disable the Cloud Event support by setting the cloud-events attribute to false","title":"Sending messages"},{"location":"amqp/sending-amqp-messages/#sending-messages-to-amqp","text":"The AMQP connector can write Reactive Messaging Messages as AMQP Messages.","title":"Sending messages to AMQP"},{"location":"amqp/sending-amqp-messages/#example","text":"Let\u2019s imagine you have an AMQP broker (such as Apache ActiveMQ Artemis ) running, and accessible using the amqp:5672 address (by default it would use localhost:5672 ). Configure your application to send the messages from the prices channel as AMQP Message as follows: 1 2 3 4 5 6 amqp-host = amqp # <1> amqp-port = 5672 # <2> amqp-username = my-username # <3> amqp-password = my-password # <4> mp.messaging.outgoing.prices.connector = smallrye-amqp # <5> 1. Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using amqp-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using amqp-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using amqp-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using amqp-password . Instructs the prices channel to be managed by the AMQP connector Note You don\u2019t need to set the address . By default, it uses the channel name ( prices ). You can configure the address attribute to override it. Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package amqp.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class AmqpPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package amqp.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class AmqpPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } }","title":"Example"},{"location":"amqp/sending-amqp-messages/#serialization","text":"When receiving a Message<T> , the connector convert the message into an AMQP Message. The payload is converted to the AMQP Message body . T AMQP Message Body primitive types or String AMQP Value with the payload Instant or UUID AMQP Value using the corresponding AMQP Type JsonObject or JsonArray AMQP Data using a binary content. The content-type is set to application/json io.vertx.mutiny.core.buffer.Buffer AMQP Data using a binary content. No content-type set Any other class The payload is converted to JSON (using a Json Mapper). The result is wrapped into AMQP Data using a binary content. The content-type is set to application/json If the message payload cannot be serialized to JSON, the message is nacked .","title":"Serialization"},{"location":"amqp/sending-amqp-messages/#outbound-metadata","text":"When sending Messages , you can add an instance of OutgoingAmqpMetadata to influence how the message is going to be sent to AMQP. For example, you can configure the subjects, properties: 1 2 3 4 5 6 7 8 OutgoingAmqpMetadata metadata = OutgoingAmqpMetadata . builder () . withDurable ( true ) . withSubject ( \"my-subject\" ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata );","title":"Outbound Metadata"},{"location":"amqp/sending-amqp-messages/#dynamic-address-names","text":"Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the address inside your application configuration file, but instead, use the outbound metadata to set the address. For example, you can send to a dynamic address based on the incoming message: 1 2 3 4 5 6 7 8 9 String addressName = selectAddressFromIncommingMessage ( incoming ); OutgoingAmqpMetadata metadata = OutgoingAmqpMetadata . builder () . withAddress ( addressName ) . withDurable ( true ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); Note To be able to set the address per message, the connector is using an anonymous sender .","title":"Dynamic address names"},{"location":"amqp/sending-amqp-messages/#acknowledgement","text":"By default, the Reactive Messaging Message is acknowledged when the broker acknowledged the message. When using routers, this acknowledgement may not be enabled. In this case, configure the auto-acknowledgement attribute to acknowledge the message as soon as it has been sent to the router. If an AMQP message is rejected/released/modified by the broker (or cannot be sent successfully), the message is nacked.","title":"Acknowledgement"},{"location":"amqp/sending-amqp-messages/#back-pressure-and-credits","text":"The back-pressure is handled by AMQP credits . The outbound connector only requests the amount of allowed credits. When the amount of credits reaches 0, it waits (in a non-blocking fashion) until the broker grants more credits to the AMQP sender.","title":"Back Pressure and Credits"},{"location":"amqp/sending-amqp-messages/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default address The AMQP address. If not set, the channel name is used string false capabilities A comma-separated list of capabilities proposed by the sender or receiver client. string false client-options-name (amqp-client-options-name) The name of the AMQP Client Option bean used to customize the AMQP client configuration string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default datacontenttype attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the datacontenttype attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default dataschema attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the dataschema attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the time attribute into the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the time attribute itself boolean false true cloud-events-mode The Cloud Event mode ( structured or binary (default)). Indicates how are written the cloud events in the outgoing record string false binary cloud-events-source (cloud-events-default-source) Configure the default source attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the source attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default subject attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the subject attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default type attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the type attribute itself string false connect-timeout (amqp-connect-timeout) The connection timeout in milliseconds int false 1000 container-id The AMQP container id string false credit-retrieval-period The period (in milliseconds) between two attempts to retrieve the credits granted by the broker. This time is used when the sender run out of credits. int false 2000 durable Whether sent AMQP messages are marked durable boolean false false health-timeout The max number of seconds to wait to determine if the connection with the broker is still established for the readiness check. After that threshold, the check is considered as failed. int false 3 host (amqp-host) The broker hostname string false localhost link-name The name of the link. If not set, the channel name is used. string false merge Whether the connector should allow multiple upstreams boolean false false password (amqp-password) The password used to authenticate to the broker string false port (amqp-port) The broker port int false 5672 reconnect-attempts (amqp-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (amqp-reconnect-interval) The interval in second between two reconnection attempts int false 10 sni-server-name (amqp-sni-server-name) If set, explicitly override the hostname to use for the TLS SNI server name string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true ttl The time-to-live of the send AMQP messages. 0 to disable the TTL long false 0 use-anonymous-sender Whether or not the connector should use an anonymous sender. Default value is true if the broker supports it, false otherwise. If not supported, it is not possible to dynamically change the destination address. boolean false use-ssl (amqp-use-ssl) Whether the AMQP connection uses SSL/TLS boolean false false username (amqp-username) The username used to authenticate to the broker string false virtual-host (amqp-virtual-host) If set, configure the hostname value used for the connection AMQP Open frame and TLS SNI server name (if TLS is in use) string false You can also pass any property supported by the Vert.x AMQP client as attribute.","title":"Configuration Reference"},{"location":"amqp/sending-amqp-messages/#using-existing-destinations","text":"To use an existing address or queue , you need to configure the address , container-id and, optionally, the link-name attributes. For example, if you have an Apache Artemis broker configured with: 1 2 3 4 5 6 7 <queues> <queue name= \"people\" > <address> people </address> <durable> true </durable> <user> artemis </user> </queue> </queues> You need the following configuration: 1 2 3 4 mp.messaging.outgoing.people.connector = smallrye-amqp mp.messaging.outgoing.people.durable = true mp.messaging.outgoing.people.address = people mp.messaging.outgoing.people.container-id = people You may need to configure the link-name attribute, if the queue name is not the channel name: 1 2 3 4 5 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.durable = true mp.messaging.outgoing.people-out.address = people mp.messaging.outgoing.people-out.container-id = people mp.messaging.outgoing.people-out.link-name = people To use a MULTICAST queue, you need to provide the FQQN (Fully-qualified queue name) instead of just the name of the queue: 1 2 3 4 5 6 7 8 9 10 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.durable = true mp.messaging.outgoing.people-out.address = foo mp.messaging.outgoing.people-out.container-id = foo mp.messaging.incoming.people-out.connector = smallrye-amqp mp.messaging.incoming.people-out.durable = true mp.messaging.incoming.people-out.address = foo::bar # Note the syntax: address-name::queue-name mp.messaging.incoming.people-out.container-id = bar mp.messaging.incoming.people-out.link-name = people More details about the AMQP Address model can be found on the Artemis documentation .","title":"Using existing destinations"},{"location":"amqp/sending-amqp-messages/#sending-cloud-events","text":"The AMQP connector supports Cloud Events . The connector sends the outbound record as Cloud Events if: the message metadata contains an io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instance, the channel configuration defines the cloud-events-type and cloud-events-source attributes. You can create io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instances using: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package amqp.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class AmqpCloudEventProcessor { @Outgoing ( \"cloud-events\" ) public Message < String > toCloudEvents ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greetings\" ) . withSource ( URI . create ( \"http://example.com\" )) . withSubject ( \"greeting-message\" ) . build ()); } } If the metadata does not contain an id, the connector generates one (random UUID). The type and source can be configured per message or at the channel level using the cloud-events-type and cloud-events-source attributes. Other attributes are also configurable. The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package amqp.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class AmqpCloudEventMultipleProcessors { @Incoming ( \"source\" ) @Outgoing ( \"processed\" ) public Message < String > process ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greeting\" ) . build ()); } @SuppressWarnings ( \"unchecked\" ) @Incoming ( \"processed\" ) @Outgoing ( \"cloud-events\" ) public Message < String > process2 ( Message < String > in ) { OutgoingCloudEventMetadata < String > metadata = in . getMetadata ( OutgoingCloudEventMetadata . class ) . orElseGet (() -> OutgoingCloudEventMetadata . builder (). build ()); return in . addMetadata ( OutgoingCloudEventMetadata . from ( metadata ) . withSource ( URI . create ( \"source://me\" )) . withSubject ( \"test\" ) . build ()); } } By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the cloud-events-mode to structured . Only JSON is supported, so the created records had its content-type header set to application/cloudevents+json; charset=UTF-8 Note you can disable the Cloud Event support by setting the cloud-events attribute to false","title":"Sending Cloud Events"},{"location":"concepts/acknowledgement/","text":"Acknowledgement Acknowledgment is an essential concept in messaging. A message is acknowledged when its processing or reception has been successful. It allows the broker to move to the next message. How acknowledgment is used, and the exact behavior in terms of retry and resilience depends on the broker. For example, for Kafka, it would commit the offset. For AMQP, it would inform the broker that the message has been accepted . Reactive Messaging supports acknowledgement. The default acknowledgement depends on the method signature. Also, the acknowledgement policy can be configured using the @Acknowledgement annotation. Chain of acknowledgment If we reuse this example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Outgoing ( \"source\" ) public Multi < String > generate () { return Multi . createFrom (). items ( \"Hello\" , \"from\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"sink\" ) public String process ( String in ) { return in . toUpperCase (); } @Incoming ( \"sink\" ) public void consume ( String processed ) { System . out . println ( processed ); } The framework automatically acknowledges the message received from the sink channel when the consume method returns. As a consequence, the message received by the process method is acknowledged, and so on. In other words, it creates a chain of acknowledgement - from the outbound channel to the inbound channel. When using connectors to receive and consume messages, the outbound connector acknowledges the messages when they are dispatched successfully to the broker. The acknowledgment chain would, as a result, acknowledges the inbound connector, which would be able to send an acknowledgment to the broker. This chain of acknowledgment is automatically implemented when processing payloads. Acknowledgment when using Messages When using Messages , the user controls the acknowledgment, and so the chain is not formed automatically. It gives you more flexibility about when and how the incoming messages are acknowledged. If you create a Message using the with method, is copy the acknowledgment function from the incoming message: 1 2 3 4 5 6 7 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < Integer > process ( Message < Integer > in ) { // The acknowledgement is forwarded, when the consumer // acknowledges the message, `in` will be acknowledged return in . withPayload ( in . getPayload () + 1 ); } To have more control over the acknowledgment, you can create a brand new Message and pass the acknowledgment function: 1 2 3 4 5 6 7 8 9 10 Message < String > message = Message . of ( \"hello\" , () -> { // called when the consumer acknowledges the message // return a CompletionStage completed when the // acknowledgment of the created message is // completed. // For immediate ack use: return CompletableFuture . completedFuture ( null ); }); However, you may need to create the acknowledgment chain, to acknowledge the incoming message: 1 2 3 4 5 6 7 8 9 10 11 12 13 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < Integer > processAndProduceNewMessage ( Message < Integer > in ) { // The acknowledgement is forwarded, when the consumer // acknowledges the message, `in` will be acknowledged return Message . of ( in . getPayload () + 1 , () -> { // Called when the consumer acknowledges the message // ... // Don't forget to acknowledge the incoming message: return in . ack (); }); } To trigger the acknowledgment of the incoming message, use the ack() method. It returns a CompletionStage , receiving null as value when the acknowledgment has completed. Acknowledgment when using streams When transforming streams of Message , the acknowledgment is delegated to the user. It means that it\u2019s up to the user to acknowledge the incoming messages: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Publisher < Message < String >> transform ( Multi < Message < String >> stream ) { return stream . map ( message -> message . withPayload ( message . getPayload (). toUpperCase ())); } In the previous example, we only generate a single message per incoming message so that we can use the with method. It becomes more sophisticated when grouping incoming messages or when each incoming message produces multiple messages. In the case of a stream of payloads, the default strategy acknowledges the incoming messages before being processed by the method (regardless of the outcome). 1 2 3 4 5 6 7 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Publisher < String > transformPayload ( Multi < String > stream ) { return stream // The incoming messages are already acknowledged . map ( String :: toUpperCase ); } Controlling acknowledgement The Acknowledgment annotation lets you customize the default strategy presented in the previous sections. The @Acknowledgement annotation takes a strategy as parameter. Reactive Messaging proposed 4 strategies: POST_PROCESSING - the acknowledgement of the incoming message is executed once the produced message is acknowledged. PRE_PROCESSING - the acknowledgement of the incoming message is executed before the message is processed by the method. MANUAL - the acknowledgement is doe by the user. NONE - No acknowledgment is performed, neither manually or automatically. It is recommended to use POST_PROCESSING as it guarantees that the full processing has completed before acknowledging the incoming message. However, sometimes it\u2019s not possible, and this strategy is not available if you manipulate streams of Messages or payloads. The PRE_PROCESSING strategy can be useful to acknowledge a message early in the process: 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Acknowledgment ( Acknowledgment . Strategy . PRE_PROCESSING ) public String process ( String input ) { // The message wrapping the payload is already acknowledged // The default would have waited the produced message to be // acknowledged return input . toUpperCase (); } It cuts the acknowledgment chain, meaning that the rest of the processing is not linked to the incoming message anymore. This strategy is the default strategy when manipulating streams of payloads. Refer to the signature list to determine which strategies are available for a specific method signature and what\u2019s the default strategy. Negative acknowledgement Messages can also be nacked , which indicates that the message was not processed correctly. The Message.nack method indicates failing processing (and supply the reason), and, as for successful acknowledgment, the nack is propagated through the chain of messages. If the message has been produced by a connector, this connector implements specific behavior when receiving a nack . It can fail (default), or ignore the failing, or implement a dead-letter queue mechanism. Refer to the connector documentation for further details about the available strategies. If the message is sent by an emitter using the send(P) method, the returned CompletionStage is completed exceptionally with the nack reason. 1 2 3 4 5 6 7 8 9 10 11 12 @Inject @Channel ( \"data\" ) Emitter < String > emitter ; public void emitPayload () { CompletionStage < Void > completionStage = emitter . send ( \"hello\" ); completionStage . whenComplete (( acked , nacked ) -> { if ( nacked != null ) { // the processing has failed } }); } Negative acknowledgment can be manual or automatic. If your method handles instances of Message and the acknowledgment strategy is MANUAL , you can nack a message explicitly. You must indicate the reason (an exception) when calling the nack method. As for successful acknowledgment, the nack returns a CompletionStage completed when the nack has been processed. If your method uses the POST_PROCESSING acknowledgment strategy, and the method fails (either by throwing an exception or by producing a failure), the message is automatically nacked with the caught exception: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Incoming ( \"data\" ) @Outgoing ( \"out\" ) public String process ( String s ) { if ( s . equalsIgnoreCase ( \"b\" )) { // Throwing an exception triggers a nack throw new IllegalArgumentException ( \"b\" ); } if ( s . equalsIgnoreCase ( \"e\" )) { // Returning null would skip the message (it will be acked) return null ; } return s . toUpperCase (); } @Incoming ( \"data\" ) @Outgoing ( \"out\" ) public Uni < String > processAsync ( String s ) { if ( s . equalsIgnoreCase ( \"a\" )) { // Returning a failing Uni triggers a nack return Uni . createFrom (). failure ( new Exception ( \"a\" )); } if ( s . equalsIgnoreCase ( \"b\" )) { // Throwing an exception triggers a nack throw new IllegalArgumentException ( \"b\" ); } if ( s . equalsIgnoreCase ( \"e\" )) { // Returning null would skip the message (it will be acked not nacked) return Uni . createFrom (). nullItem (); } if ( s . equalsIgnoreCase ( \"f\" )) { // returning `null` is invalid for method returning Unis, the message is nacked return null ; } return Uni . createFrom (). item ( s . toUpperCase ()); }","title":"Acknowledgement"},{"location":"concepts/acknowledgement/#acknowledgement","text":"Acknowledgment is an essential concept in messaging. A message is acknowledged when its processing or reception has been successful. It allows the broker to move to the next message. How acknowledgment is used, and the exact behavior in terms of retry and resilience depends on the broker. For example, for Kafka, it would commit the offset. For AMQP, it would inform the broker that the message has been accepted . Reactive Messaging supports acknowledgement. The default acknowledgement depends on the method signature. Also, the acknowledgement policy can be configured using the @Acknowledgement annotation.","title":"Acknowledgement"},{"location":"concepts/acknowledgement/#chain-of-acknowledgment","text":"If we reuse this example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Outgoing ( \"source\" ) public Multi < String > generate () { return Multi . createFrom (). items ( \"Hello\" , \"from\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"sink\" ) public String process ( String in ) { return in . toUpperCase (); } @Incoming ( \"sink\" ) public void consume ( String processed ) { System . out . println ( processed ); } The framework automatically acknowledges the message received from the sink channel when the consume method returns. As a consequence, the message received by the process method is acknowledged, and so on. In other words, it creates a chain of acknowledgement - from the outbound channel to the inbound channel. When using connectors to receive and consume messages, the outbound connector acknowledges the messages when they are dispatched successfully to the broker. The acknowledgment chain would, as a result, acknowledges the inbound connector, which would be able to send an acknowledgment to the broker. This chain of acknowledgment is automatically implemented when processing payloads.","title":"Chain of acknowledgment"},{"location":"concepts/acknowledgement/#acknowledgment-when-using-messages","text":"When using Messages , the user controls the acknowledgment, and so the chain is not formed automatically. It gives you more flexibility about when and how the incoming messages are acknowledged. If you create a Message using the with method, is copy the acknowledgment function from the incoming message: 1 2 3 4 5 6 7 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < Integer > process ( Message < Integer > in ) { // The acknowledgement is forwarded, when the consumer // acknowledges the message, `in` will be acknowledged return in . withPayload ( in . getPayload () + 1 ); } To have more control over the acknowledgment, you can create a brand new Message and pass the acknowledgment function: 1 2 3 4 5 6 7 8 9 10 Message < String > message = Message . of ( \"hello\" , () -> { // called when the consumer acknowledges the message // return a CompletionStage completed when the // acknowledgment of the created message is // completed. // For immediate ack use: return CompletableFuture . completedFuture ( null ); }); However, you may need to create the acknowledgment chain, to acknowledge the incoming message: 1 2 3 4 5 6 7 8 9 10 11 12 13 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < Integer > processAndProduceNewMessage ( Message < Integer > in ) { // The acknowledgement is forwarded, when the consumer // acknowledges the message, `in` will be acknowledged return Message . of ( in . getPayload () + 1 , () -> { // Called when the consumer acknowledges the message // ... // Don't forget to acknowledge the incoming message: return in . ack (); }); } To trigger the acknowledgment of the incoming message, use the ack() method. It returns a CompletionStage , receiving null as value when the acknowledgment has completed.","title":"Acknowledgment when using Messages"},{"location":"concepts/acknowledgement/#acknowledgment-when-using-streams","text":"When transforming streams of Message , the acknowledgment is delegated to the user. It means that it\u2019s up to the user to acknowledge the incoming messages: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Publisher < Message < String >> transform ( Multi < Message < String >> stream ) { return stream . map ( message -> message . withPayload ( message . getPayload (). toUpperCase ())); } In the previous example, we only generate a single message per incoming message so that we can use the with method. It becomes more sophisticated when grouping incoming messages or when each incoming message produces multiple messages. In the case of a stream of payloads, the default strategy acknowledges the incoming messages before being processed by the method (regardless of the outcome). 1 2 3 4 5 6 7 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Publisher < String > transformPayload ( Multi < String > stream ) { return stream // The incoming messages are already acknowledged . map ( String :: toUpperCase ); }","title":"Acknowledgment when using streams"},{"location":"concepts/acknowledgement/#controlling-acknowledgement","text":"The Acknowledgment annotation lets you customize the default strategy presented in the previous sections. The @Acknowledgement annotation takes a strategy as parameter. Reactive Messaging proposed 4 strategies: POST_PROCESSING - the acknowledgement of the incoming message is executed once the produced message is acknowledged. PRE_PROCESSING - the acknowledgement of the incoming message is executed before the message is processed by the method. MANUAL - the acknowledgement is doe by the user. NONE - No acknowledgment is performed, neither manually or automatically. It is recommended to use POST_PROCESSING as it guarantees that the full processing has completed before acknowledging the incoming message. However, sometimes it\u2019s not possible, and this strategy is not available if you manipulate streams of Messages or payloads. The PRE_PROCESSING strategy can be useful to acknowledge a message early in the process: 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Acknowledgment ( Acknowledgment . Strategy . PRE_PROCESSING ) public String process ( String input ) { // The message wrapping the payload is already acknowledged // The default would have waited the produced message to be // acknowledged return input . toUpperCase (); } It cuts the acknowledgment chain, meaning that the rest of the processing is not linked to the incoming message anymore. This strategy is the default strategy when manipulating streams of payloads. Refer to the signature list to determine which strategies are available for a specific method signature and what\u2019s the default strategy.","title":"Controlling acknowledgement"},{"location":"concepts/acknowledgement/#negative-acknowledgement","text":"Messages can also be nacked , which indicates that the message was not processed correctly. The Message.nack method indicates failing processing (and supply the reason), and, as for successful acknowledgment, the nack is propagated through the chain of messages. If the message has been produced by a connector, this connector implements specific behavior when receiving a nack . It can fail (default), or ignore the failing, or implement a dead-letter queue mechanism. Refer to the connector documentation for further details about the available strategies. If the message is sent by an emitter using the send(P) method, the returned CompletionStage is completed exceptionally with the nack reason. 1 2 3 4 5 6 7 8 9 10 11 12 @Inject @Channel ( \"data\" ) Emitter < String > emitter ; public void emitPayload () { CompletionStage < Void > completionStage = emitter . send ( \"hello\" ); completionStage . whenComplete (( acked , nacked ) -> { if ( nacked != null ) { // the processing has failed } }); } Negative acknowledgment can be manual or automatic. If your method handles instances of Message and the acknowledgment strategy is MANUAL , you can nack a message explicitly. You must indicate the reason (an exception) when calling the nack method. As for successful acknowledgment, the nack returns a CompletionStage completed when the nack has been processed. If your method uses the POST_PROCESSING acknowledgment strategy, and the method fails (either by throwing an exception or by producing a failure), the message is automatically nacked with the caught exception: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Incoming ( \"data\" ) @Outgoing ( \"out\" ) public String process ( String s ) { if ( s . equalsIgnoreCase ( \"b\" )) { // Throwing an exception triggers a nack throw new IllegalArgumentException ( \"b\" ); } if ( s . equalsIgnoreCase ( \"e\" )) { // Returning null would skip the message (it will be acked) return null ; } return s . toUpperCase (); } @Incoming ( \"data\" ) @Outgoing ( \"out\" ) public Uni < String > processAsync ( String s ) { if ( s . equalsIgnoreCase ( \"a\" )) { // Returning a failing Uni triggers a nack return Uni . createFrom (). failure ( new Exception ( \"a\" )); } if ( s . equalsIgnoreCase ( \"b\" )) { // Throwing an exception triggers a nack throw new IllegalArgumentException ( \"b\" ); } if ( s . equalsIgnoreCase ( \"e\" )) { // Returning null would skip the message (it will be acked not nacked) return Uni . createFrom (). nullItem (); } if ( s . equalsIgnoreCase ( \"f\" )) { // returning `null` is invalid for method returning Unis, the message is nacked return null ; } return Uni . createFrom (). item ( s . toUpperCase ()); }","title":"Negative acknowledgement"},{"location":"concepts/advanced-config/","text":"Advanced configuration Strict Binding Mode By default, SmallRye Reactive Messaging does not enforce whether all mediators are connected. It just prints a warning message. The strict mode fails the deployment if some \"incomings\" are not bound to \"outgoings\". To enable this mode, you can pass the -Dsmallrye-messaging-strict-binding=true via the command line, or you can set the smallrye-messaging-strict-binding attribute to true in the configuration: 1 smallrye-messaging-strict-binding=true Disabling channels You can disable a channel in the configuration by setting the enabled attribute to false : 1 2 mp.messaging.outgoing.dummy-sink.connector=dummy mp.messaging.outgoing.dummy-sink.enabled=false # Disable this channel SmallRye Reactive Messaging does not register disabled channels, so make sure the rest of the application does not rely on them. Publisher metrics SmallRye Reactive Messaging integrates MicroProfile Metrics and Micrometer for registering counter metrics (named mp.messaging.message.count ) of published messages per channel. Both MicroProfile and Micrometer publisher metrics are enabled by default if found on the classpath. They can be disabled with smallrye.messaging.metrics.mp.enabled and smallrye.messaging.metrics.micrometer.enabled properties respectively.","title":"Advanced Configuration"},{"location":"concepts/advanced-config/#advanced-configuration","text":"","title":"Advanced configuration"},{"location":"concepts/advanced-config/#strict-binding-mode","text":"By default, SmallRye Reactive Messaging does not enforce whether all mediators are connected. It just prints a warning message. The strict mode fails the deployment if some \"incomings\" are not bound to \"outgoings\". To enable this mode, you can pass the -Dsmallrye-messaging-strict-binding=true via the command line, or you can set the smallrye-messaging-strict-binding attribute to true in the configuration: 1 smallrye-messaging-strict-binding=true","title":"Strict Binding Mode"},{"location":"concepts/advanced-config/#disabling-channels","text":"You can disable a channel in the configuration by setting the enabled attribute to false : 1 2 mp.messaging.outgoing.dummy-sink.connector=dummy mp.messaging.outgoing.dummy-sink.enabled=false # Disable this channel SmallRye Reactive Messaging does not register disabled channels, so make sure the rest of the application does not rely on them.","title":"Disabling channels"},{"location":"concepts/advanced-config/#publisher-metrics","text":"SmallRye Reactive Messaging integrates MicroProfile Metrics and Micrometer for registering counter metrics (named mp.messaging.message.count ) of published messages per channel. Both MicroProfile and Micrometer publisher metrics are enabled by default if found on the classpath. They can be disabled with smallrye.messaging.metrics.mp.enabled and smallrye.messaging.metrics.micrometer.enabled properties respectively.","title":"Publisher metrics"},{"location":"concepts/blocking/","text":"@Blocking The io.smallrye.reactive.messaging.annotations.Blocking annotation can be used on a method annotated with @Incoming , or @Outgoing to indicate that the method should be executed on a worker pool: 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking public String process ( String s ) { return s . toUpperCase (); } If method execution does not need to be ordered, it can be indicated on the @Blocking annotation: 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking ( ordered = false ) public String process ( String s ) { return s . toUpperCase (); } When unordered, the invocation can happen concurrently. By default, use of @Blocking results in the method being executed in the Vert.x worker pool. If it\u2019s desired to execute methods on a custom worker pool, with specific concurrency needs, it can be defined on @Blocking : 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking ( \"my-custom-pool\" ) public String process ( String s ) { return s . toUpperCase (); } Specifying the concurrency for the above worker pool requires the following configuration property to be defined: 1 smallrye.messaging.worker.my-custom-pool.max-concurrency=3 Supported signatures @Blocking does not support every signature. The following table lists the supported ones. Shape Signature Comment Publisher @Outgoing(\"in\") @Blocking O generator() Invokes the generator from a worker thread. If ordered is set to false , the generator can be called concurrently. Publisher @Outgoing(\"in\") @Blocking Message<O> generator() Invokes the generator from a worker thread. If ordered is set to false , the generator can be called concurrently. Processor @Incoming(\"in\") @Outgoing(\"bar\") @Blocking O process(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Processor @Incoming(\"in\") @Outgoing(\"bar\") @Blocking Message<O> process(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking void consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking Uni<Void> consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking CompletionStage<Void> consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. When a method can be called concurrently, the max concurrency depends on the number of threads from the worker thread pool. Using io.smallrye.common.annotation.Blocking io.smallrye.common.annotation.Blocking is another annotation with the same semantic. io.smallrye.common.annotation.Blocking is used by multiple SmallRye projects and Quarkus. SmallRye Reactive Messaging also supports io.smallrye.common.annotation.Blocking . However, io.smallrye.common.annotation.Blocking does not allow configuring the ordering (it defaults to ordered=true ). When both annotations are used, io.smallrye.reactive.messaging.annotations.Blocking is preferred.","title":"Blocking Processing"},{"location":"concepts/blocking/#blocking","text":"The io.smallrye.reactive.messaging.annotations.Blocking annotation can be used on a method annotated with @Incoming , or @Outgoing to indicate that the method should be executed on a worker pool: 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking public String process ( String s ) { return s . toUpperCase (); } If method execution does not need to be ordered, it can be indicated on the @Blocking annotation: 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking ( ordered = false ) public String process ( String s ) { return s . toUpperCase (); } When unordered, the invocation can happen concurrently. By default, use of @Blocking results in the method being executed in the Vert.x worker pool. If it\u2019s desired to execute methods on a custom worker pool, with specific concurrency needs, it can be defined on @Blocking : 1 2 3 4 5 6 @Outgoing ( \"Y\" ) @Incoming ( \"X\" ) @Blocking ( \"my-custom-pool\" ) public String process ( String s ) { return s . toUpperCase (); } Specifying the concurrency for the above worker pool requires the following configuration property to be defined: 1 smallrye.messaging.worker.my-custom-pool.max-concurrency=3","title":"@Blocking"},{"location":"concepts/blocking/#supported-signatures","text":"@Blocking does not support every signature. The following table lists the supported ones. Shape Signature Comment Publisher @Outgoing(\"in\") @Blocking O generator() Invokes the generator from a worker thread. If ordered is set to false , the generator can be called concurrently. Publisher @Outgoing(\"in\") @Blocking Message<O> generator() Invokes the generator from a worker thread. If ordered is set to false , the generator can be called concurrently. Processor @Incoming(\"in\") @Outgoing(\"bar\") @Blocking O process(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Processor @Incoming(\"in\") @Outgoing(\"bar\") @Blocking Message<O> process(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking void consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking Uni<Void> consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. Subscriber @Incoming(\"in\") @Blocking CompletionStage<Void> consume(I in) Invokes the method on a worker thread. If ordered is set to false , the method can be called concurrently. When a method can be called concurrently, the max concurrency depends on the number of threads from the worker thread pool.","title":"Supported signatures"},{"location":"concepts/blocking/#using-iosmallryecommonannotationblocking","text":"io.smallrye.common.annotation.Blocking is another annotation with the same semantic. io.smallrye.common.annotation.Blocking is used by multiple SmallRye projects and Quarkus. SmallRye Reactive Messaging also supports io.smallrye.common.annotation.Blocking . However, io.smallrye.common.annotation.Blocking does not allow configuring the ordering (it defaults to ordered=true ). When both annotations are used, io.smallrye.reactive.messaging.annotations.Blocking is preferred.","title":"Using io.smallrye.common.annotation.Blocking"},{"location":"concepts/broadcast/","text":"Broadcast Experimental @Broadcast is an experimental feature. By default, messages transiting in a channel are only dispatched to a single consumer. Having multiple consumers is considered as an error, and is reported at deployment time. The Broadcast annotation changes this behavior and indicates that messages transiting in the channel are dispatched to all the consumers. @Broadcast must be used with the @Outgoing annotation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Broadcast public int increment ( int i ) { return i + 1 ; } @Incoming ( \"out\" ) public void consume1 ( int i ) { //... } @Incoming ( \"out\" ) public void consume2 ( int i ) { //... } In the previous example, both consumers get the messages. You can also control the number of consumers to wait before starting to dispatch the messages. This allows waiting for the complete graph to be woven: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Broadcast ( 2 ) public int increment ( int i ) { return i + 1 ; } @Incoming ( \"out\" ) public void consume1 ( int i ) { //... } @Incoming ( \"out\" ) public void consume2 ( int i ) { //... } Note Inbound connectors also support a broadcast attribute that allows broadcasting the messages to multiple downstream subscribers. Use with Emitter For details on how to use @Broadcast with Emitter see the documentation .","title":"Broadcast"},{"location":"concepts/broadcast/#broadcast","text":"Experimental @Broadcast is an experimental feature. By default, messages transiting in a channel are only dispatched to a single consumer. Having multiple consumers is considered as an error, and is reported at deployment time. The Broadcast annotation changes this behavior and indicates that messages transiting in the channel are dispatched to all the consumers. @Broadcast must be used with the @Outgoing annotation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Broadcast public int increment ( int i ) { return i + 1 ; } @Incoming ( \"out\" ) public void consume1 ( int i ) { //... } @Incoming ( \"out\" ) public void consume2 ( int i ) { //... } In the previous example, both consumers get the messages. You can also control the number of consumers to wait before starting to dispatch the messages. This allows waiting for the complete graph to be woven: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) @Broadcast ( 2 ) public int increment ( int i ) { return i + 1 ; } @Incoming ( \"out\" ) public void consume1 ( int i ) { //... } @Incoming ( \"out\" ) public void consume2 ( int i ) { //... } Note Inbound connectors also support a broadcast attribute that allows broadcasting the messages to multiple downstream subscribers.","title":"Broadcast"},{"location":"concepts/broadcast/#use-with-emitter","text":"For details on how to use @Broadcast with Emitter see the documentation .","title":"Use with Emitter"},{"location":"concepts/concepts/","text":"When dealing with event-driven or data streaming applications, there are a few concepts and vocabulary to introduce. Messages, Payload, Metadata A Message is an envelope around a payload . Your application is going to receive, process, and send Messages . Your application\u2019s logic can generate these Messages or receive them from a message broker. They can also be consumed by your application or sent to a message broker. An application can receive a message, process it and send a resulting message In Reactive Messaging, Message are represented by the Message interface. Each Message<T> contains a payload of type <T> . This payload can be retrieved using message.getPayload() : 1 2 String payload = message . getPayload (); Optional < MyMetadata > metadata = message . getMetadata ( MyMetadata . class ); As you can see in the previous snippet, Messages can also have metadata . Metadata is a way to extend messages with additional data. It can be metadata related to the message broker (like KafkaMessageMetadata ), or contain operational data (such as tracing metadata), or business-related data. Note When retrieving metadata, you get an Optional as it may not be present. Tip Metadata is also used to influence the outbound dispatching (how the message will be sent to the broker). Channels and Streams Inside your application, Messages transit on channel . A channel is a virtual destination identified by a name. The application is a set of channels SmallRye Reactive Messaging connects the component to the channel they read and to the channel they populate. The resulting structure is a stream: Messages flow between components through channels. What about Reactive Streams? You may wonder why Reactive Messaging has Reactive in the name. The Messaging part is kind of obvious. The Reactive part comes from the streams that are created by binding components. These streams are Reactive Streams . They follow the subscription and request protocol and implement back-pressure. It also means that Connectors are intended to use non-blocking IO to interact with the various message brokers. Connectors Your application is interacting with messaging brokers or event backbone using connectors . A connector is a piece of code that connects to a broker and: subscribe/poll/receive messages from the broker and propagate them to the application send/write/dispatch messages provided by the application to the broker Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. These collected messages are sent to the external broker. Connectors manages the communication between the application and the brokers Each connector is dedicated to a specific technology. For example, a Kafka Connector only deals with Kafka. You don\u2019t necessarily need a connector. When your application does not use connectors, everything happens in-memory , and the streams are created by chaining methods altogether. Each chain is still a reactive stream and enforces the back-pressure protocol. When you don\u2019t use connectors, you need to make sure the chain is complete, meaning it starts with a message source, and it ends with a sink. In other words, you need to generate messages from within the application (using a method with only @Outgoing , or an Emitter ) and consume the messages from within the application (using a method with only @Incoming or using an unmanaged stream).","title":"Introduction"},{"location":"concepts/concepts/#messages-payload-metadata","text":"A Message is an envelope around a payload . Your application is going to receive, process, and send Messages . Your application\u2019s logic can generate these Messages or receive them from a message broker. They can also be consumed by your application or sent to a message broker. An application can receive a message, process it and send a resulting message In Reactive Messaging, Message are represented by the Message interface. Each Message<T> contains a payload of type <T> . This payload can be retrieved using message.getPayload() : 1 2 String payload = message . getPayload (); Optional < MyMetadata > metadata = message . getMetadata ( MyMetadata . class ); As you can see in the previous snippet, Messages can also have metadata . Metadata is a way to extend messages with additional data. It can be metadata related to the message broker (like KafkaMessageMetadata ), or contain operational data (such as tracing metadata), or business-related data. Note When retrieving metadata, you get an Optional as it may not be present. Tip Metadata is also used to influence the outbound dispatching (how the message will be sent to the broker).","title":"Messages, Payload, Metadata"},{"location":"concepts/concepts/#channels-and-streams","text":"Inside your application, Messages transit on channel . A channel is a virtual destination identified by a name. The application is a set of channels SmallRye Reactive Messaging connects the component to the channel they read and to the channel they populate. The resulting structure is a stream: Messages flow between components through channels. What about Reactive Streams? You may wonder why Reactive Messaging has Reactive in the name. The Messaging part is kind of obvious. The Reactive part comes from the streams that are created by binding components. These streams are Reactive Streams . They follow the subscription and request protocol and implement back-pressure. It also means that Connectors are intended to use non-blocking IO to interact with the various message brokers.","title":"Channels and Streams"},{"location":"concepts/concepts/#connectors","text":"Your application is interacting with messaging brokers or event backbone using connectors . A connector is a piece of code that connects to a broker and: subscribe/poll/receive messages from the broker and propagate them to the application send/write/dispatch messages provided by the application to the broker Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. These collected messages are sent to the external broker. Connectors manages the communication between the application and the brokers Each connector is dedicated to a specific technology. For example, a Kafka Connector only deals with Kafka. You don\u2019t necessarily need a connector. When your application does not use connectors, everything happens in-memory , and the streams are created by chaining methods altogether. Each chain is still a reactive stream and enforces the back-pressure protocol. When you don\u2019t use connectors, you need to make sure the chain is complete, meaning it starts with a message source, and it ends with a sink. In other words, you need to generate messages from within the application (using a method with only @Outgoing , or an Emitter ) and consume the messages from within the application (using a method with only @Incoming or using an unmanaged stream).","title":"Connectors"},{"location":"concepts/connectors/","text":"Connectors Reactive Messaging can handle messages generated from within the application but also interact with remote brokers . Reactive Messaging Connectors interacts with these remote brokers to retrieve messages and send messages using various protocols and technology. Each connector handles to a specific technology. For example, a Kafka Connector is responsible for interacting with Kafka, while an MQTT Connector is responsible for MQTT interactions. Connector name Each connector has a name. This name is referenced by the application to indicate that this connector manages a specific channel. For example, the SmallRye Kafka Connector is named: smallrye-kafka . Inbound and Outbound connectors Connector can: retrieve messages from a remote broker (inbound) send messages to a remote broker (outbound) A connector can, of course, implement both directions. Inbound connectors are responsible for: Getting messages from the remote broker, Creating a Reactive Messaging Message associated with the retrieved message. Potentially associating technical metadata with the message. It includes unmarshalling the payload. Associating an acknowledgment callback to acknowledge the incoming message when the Reactive Messaging message is processed/acknowledged. Reactive matters The first step should follow the reactive streams principle: uses non-blocking technology, respects downstream requests. Outbound connectors are responsible for: Receiving Reactive Messaging Message and transform it into a structure understood by the remote broker. It includes marshaling the payload. If the Message contains outbound metadata (metadata set during the processing to influence the outbound structure and routing), taking them into account. Sending the message to the remote broker. Acknowledging the Reactive Messaging Message when the broker has accepted/acknowledged the message. Configuring connectors Applications need to configure the connector used by expressing which channel is managed by which connector. Non-mapped channels are local / in-memory. To configure connectors, you need to have an implementation of MicroProfile Config. If you don\u2019t have one, add an implementation of MicroProfile Config in your classpath , such as: 1 2 3 4 5 <dependency> <groupId> io.smallrye.config </groupId> <artifactId> smallrye-config </artifactId> <version> 3.1.2 </version> </dependency> Then edit the application configuration, generally src/main/resources/META-INF/microprofile-config.properties . The application configures the connector with a set of properties structured as follows: 1 mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value] For example: 1 2 3 4 mp.messaging.incoming.dummy-incoming-channel.connector=dummy mp.messaging.incoming.dummy-incoming-channel.attribute=value mp.messaging.outgoing.dummy-outgoing-channel.connector=dummy mp.messaging.outgoing.dummy-outgoing-channel.attribute=value You configure each channel (both incoming and outgoing) individually. The [incoming|outgoing] segment indicates the direction. an incoming channel consumes data from a message broker or something producing data. It\u2019s an inbound interaction. It relates to methods annotated with an @Incoming using the same channel name. an outgoing consumes data from the application and forwards it to a message broker or something consuming data. It\u2019s an outbound interaction. It relates to methods annotated with an @Outgoing using the same channel name. The [channel-name] is the name of the channel. If the channel name contains a . (dot), you would need to use \" (double-quote) around it. For example, to configure the dummy.incoming.channel channel, you would need: 1 2 mp.messaging.incoming.\"dummy.incoming.channel\".connector=dummy mp.messaging.incoming.\"dummy.incoming.channel\".attribute=value The [attribute]=[value] sets a specific connector attribute to the given value. Attributes depend on the used connector. So, refer to the connector documentation to check the supported attributes. The connector attribute must be set for each mapped channel and indicates the name of the connector responsible for the channel. Here is an example of a channel using an MQTT connector, consuming data from a MQTT broker, and a channel using a Kafka connector (writing data to Kafka): 1 2 3 4 5 6 7 8 9 10 11 12 # Configure the incoming health channel mp.messaging.incoming.health.topic = neo mp.messaging.incoming.health.connector = smallrye-mqtt mp.messaging.incoming.health.host = localhost mp.messaging.incoming.health.broadcast = true # Configure outgoing data channel mp.messaging.outgoing.data.connector = smallrye-kafka mp.messaging.outgoing.data.bootstrap.servers = localhost:9092 mp.messaging.outgoing.data.key.serializer = org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.data.value.serializer = io.vertx.kafka.client.serialization.JsonObjectSerializer mp.messaging.outgoing.data.acks = 1 Important To use a connector, you need to add it to your CLASSPATH . Generally, adding the dependency to your project is enough. Then, you need to know the connector\u2019s name and set the connector attribute for each channel managed by this connector. Connector attribute table In the connector documentation, you will find a table listing the attribute supported by the connector. Be aware that attributes for inbound and outbound interactions may be different. These tables contain the following entries: The name of the attribute, and potentially an alias . The name of the attribute is used with the mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value] syntax (the attribute segment). The alias (if set) is the name of a global MicroProfile Config property that avoids having to configure the attribute for each managed channel. For example, to set the location of your Kafka broker globally, you can use the kafka.bootstrap.servers alias. The description of the attribute, including the type. Whether that attribute is mandatory. If so, it fails the deployment if not set The default value, if any.","title":"Connectors"},{"location":"concepts/connectors/#connectors","text":"Reactive Messaging can handle messages generated from within the application but also interact with remote brokers . Reactive Messaging Connectors interacts with these remote brokers to retrieve messages and send messages using various protocols and technology. Each connector handles to a specific technology. For example, a Kafka Connector is responsible for interacting with Kafka, while an MQTT Connector is responsible for MQTT interactions.","title":"Connectors"},{"location":"concepts/connectors/#connector-name","text":"Each connector has a name. This name is referenced by the application to indicate that this connector manages a specific channel. For example, the SmallRye Kafka Connector is named: smallrye-kafka .","title":"Connector name"},{"location":"concepts/connectors/#inbound-and-outbound-connectors","text":"Connector can: retrieve messages from a remote broker (inbound) send messages to a remote broker (outbound) A connector can, of course, implement both directions. Inbound connectors are responsible for: Getting messages from the remote broker, Creating a Reactive Messaging Message associated with the retrieved message. Potentially associating technical metadata with the message. It includes unmarshalling the payload. Associating an acknowledgment callback to acknowledge the incoming message when the Reactive Messaging message is processed/acknowledged. Reactive matters The first step should follow the reactive streams principle: uses non-blocking technology, respects downstream requests. Outbound connectors are responsible for: Receiving Reactive Messaging Message and transform it into a structure understood by the remote broker. It includes marshaling the payload. If the Message contains outbound metadata (metadata set during the processing to influence the outbound structure and routing), taking them into account. Sending the message to the remote broker. Acknowledging the Reactive Messaging Message when the broker has accepted/acknowledged the message.","title":"Inbound and Outbound connectors"},{"location":"concepts/connectors/#configuring-connectors","text":"Applications need to configure the connector used by expressing which channel is managed by which connector. Non-mapped channels are local / in-memory. To configure connectors, you need to have an implementation of MicroProfile Config. If you don\u2019t have one, add an implementation of MicroProfile Config in your classpath , such as: 1 2 3 4 5 <dependency> <groupId> io.smallrye.config </groupId> <artifactId> smallrye-config </artifactId> <version> 3.1.2 </version> </dependency> Then edit the application configuration, generally src/main/resources/META-INF/microprofile-config.properties . The application configures the connector with a set of properties structured as follows: 1 mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value] For example: 1 2 3 4 mp.messaging.incoming.dummy-incoming-channel.connector=dummy mp.messaging.incoming.dummy-incoming-channel.attribute=value mp.messaging.outgoing.dummy-outgoing-channel.connector=dummy mp.messaging.outgoing.dummy-outgoing-channel.attribute=value You configure each channel (both incoming and outgoing) individually. The [incoming|outgoing] segment indicates the direction. an incoming channel consumes data from a message broker or something producing data. It\u2019s an inbound interaction. It relates to methods annotated with an @Incoming using the same channel name. an outgoing consumes data from the application and forwards it to a message broker or something consuming data. It\u2019s an outbound interaction. It relates to methods annotated with an @Outgoing using the same channel name. The [channel-name] is the name of the channel. If the channel name contains a . (dot), you would need to use \" (double-quote) around it. For example, to configure the dummy.incoming.channel channel, you would need: 1 2 mp.messaging.incoming.\"dummy.incoming.channel\".connector=dummy mp.messaging.incoming.\"dummy.incoming.channel\".attribute=value The [attribute]=[value] sets a specific connector attribute to the given value. Attributes depend on the used connector. So, refer to the connector documentation to check the supported attributes. The connector attribute must be set for each mapped channel and indicates the name of the connector responsible for the channel. Here is an example of a channel using an MQTT connector, consuming data from a MQTT broker, and a channel using a Kafka connector (writing data to Kafka): 1 2 3 4 5 6 7 8 9 10 11 12 # Configure the incoming health channel mp.messaging.incoming.health.topic = neo mp.messaging.incoming.health.connector = smallrye-mqtt mp.messaging.incoming.health.host = localhost mp.messaging.incoming.health.broadcast = true # Configure outgoing data channel mp.messaging.outgoing.data.connector = smallrye-kafka mp.messaging.outgoing.data.bootstrap.servers = localhost:9092 mp.messaging.outgoing.data.key.serializer = org.apache.kafka.common.serialization.StringSerializer mp.messaging.outgoing.data.value.serializer = io.vertx.kafka.client.serialization.JsonObjectSerializer mp.messaging.outgoing.data.acks = 1 Important To use a connector, you need to add it to your CLASSPATH . Generally, adding the dependency to your project is enough. Then, you need to know the connector\u2019s name and set the connector attribute for each channel managed by this connector.","title":"Configuring connectors"},{"location":"concepts/connectors/#connector-attribute-table","text":"In the connector documentation, you will find a table listing the attribute supported by the connector. Be aware that attributes for inbound and outbound interactions may be different. These tables contain the following entries: The name of the attribute, and potentially an alias . The name of the attribute is used with the mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value] syntax (the attribute segment). The alias (if set) is the name of a global MicroProfile Config property that avoids having to configure the attribute for each managed channel. For example, to set the location of your Kafka broker globally, you can use the kafka.bootstrap.servers alias. The description of the attribute, including the type. Whether that attribute is mandatory. If so, it fails the deployment if not set The default value, if any.","title":"Connector attribute table"},{"location":"concepts/converters/","text":"Message Converters SmallRye Reactive Messaging supports message converters , allowing to transform an incoming message into a version accepted by the method. If the incoming messages or payload does not match the invoked method\u2019s expectation, SmallRye Reactive Messaging looks for a suitable converter. If found, it converts the incoming message with this converter. Converters can have multiple purposes, but the main use case is about transforming the message\u2019s payload: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @ApplicationScoped public class MyConverter implements MessageConverter { @Override public boolean canConvert ( Message <?> in , Type target ) { // Checks whether this converter can be used to convert // the incoming message into a message containing a payload // of the type `target`. return in . getPayload (). getClass (). equals ( String . class ) && target . equals ( Person . class ); } @Override public Message <?> convert ( Message <?> in , Type target ) { // Convert the incoming message into the new message. // It's important to build the new message **from** // the received one. return in . withPayload ( new Person (( String ) in . getPayload ())); } } To provide a converter, implement a bean exposing the MessageConverter interface. The canConvert method is called during the lookup and verifies if it can handle the conversion. The target type is the expected payload type. If the converter returns true to canConvert , SmallRye Reactive Messaging calls the convert method to proceed to the conversion. The previous converter can be used in application like the following, to convert Message<String> to Message<Person> : 1 2 3 4 5 6 7 8 9 10 11 @Outgoing ( \"persons\" ) public Multi < String > source () { return Multi . createFrom (). items ( \"Neo\" , \"Morpheus\" , \"Trinity\" ); } // The messages need to be converted as they are emitted as Message<String> // and consumed as Message<Person> @Incoming ( \"persons\" ) public void consume ( Person p ) { // ... } Converters work for all supported method signatures. However, the signature must be well-formed to allow the extraction of the expected payload type. Wildcards and raw types do not support conversion. If the expected payload type cannot be extracted, or no converter fits, the message is passed as received. If multiple suitable converters are present, implementations should override the getPriority method returning the priority. The default priority is 100 . The converter lookup invokes converters with higher priority (from the least value to the greatest) first.","title":"Message Converters"},{"location":"concepts/converters/#message-converters","text":"SmallRye Reactive Messaging supports message converters , allowing to transform an incoming message into a version accepted by the method. If the incoming messages or payload does not match the invoked method\u2019s expectation, SmallRye Reactive Messaging looks for a suitable converter. If found, it converts the incoming message with this converter. Converters can have multiple purposes, but the main use case is about transforming the message\u2019s payload: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @ApplicationScoped public class MyConverter implements MessageConverter { @Override public boolean canConvert ( Message <?> in , Type target ) { // Checks whether this converter can be used to convert // the incoming message into a message containing a payload // of the type `target`. return in . getPayload (). getClass (). equals ( String . class ) && target . equals ( Person . class ); } @Override public Message <?> convert ( Message <?> in , Type target ) { // Convert the incoming message into the new message. // It's important to build the new message **from** // the received one. return in . withPayload ( new Person (( String ) in . getPayload ())); } } To provide a converter, implement a bean exposing the MessageConverter interface. The canConvert method is called during the lookup and verifies if it can handle the conversion. The target type is the expected payload type. If the converter returns true to canConvert , SmallRye Reactive Messaging calls the convert method to proceed to the conversion. The previous converter can be used in application like the following, to convert Message<String> to Message<Person> : 1 2 3 4 5 6 7 8 9 10 11 @Outgoing ( \"persons\" ) public Multi < String > source () { return Multi . createFrom (). items ( \"Neo\" , \"Morpheus\" , \"Trinity\" ); } // The messages need to be converted as they are emitted as Message<String> // and consumed as Message<Person> @Incoming ( \"persons\" ) public void consume ( Person p ) { // ... } Converters work for all supported method signatures. However, the signature must be well-formed to allow the extraction of the expected payload type. Wildcards and raw types do not support conversion. If the expected payload type cannot be extracted, or no converter fits, the message is passed as received. If multiple suitable converters are present, implementations should override the getPriority method returning the priority. The default priority is 100 . The converter lookup invokes converters with higher priority (from the least value to the greatest) first.","title":"Message Converters"},{"location":"concepts/decorators/","text":"Channel Decorators SmallRye Reactive Messaging supports decorating reactive streams of incoming and outgoing channels for implementing cross-cutting concerns such as monitoring, tracing or message interception. Two symmetrical APIs are proposed for decorating publisher and subscriber channels, PublisherDecorator and SubscriberDecorator respectively. Important @Incoming channels and channels bound to an outbound connector are both Subscriber s. Conversely @Outgoing channels and channels bound to an inbound connector are Publisher s. For example, to provide a decorator which counts consumed messages from incoming connector, implement a bean exposing the interface PublisherDecorator : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @ApplicationScoped public class ConsumedMessageDecorator implements PublisherDecorator { private final Map < String , AtomicLong > counters = new HashMap <> (); @Override public Multi <? extends Message <?>> decorate ( Multi <? extends Message <?>> publisher , String channelName , boolean isConnector ) { if ( isConnector ) { AtomicLong counter = new AtomicLong (); counters . put ( channelName , counter ); return publisher . onItem (). invoke ( counter :: incrementAndGet ); } else { return publisher ; } } @Override public int getPriority () { return 10 ; } public long getMessageCount ( String channel ) { return counters . get ( channel ). get (); } } Decorators' decorate method is called only once per channel at application deployment when graph wiring is taking place. Decorators are very powerful because they receive the stream of messages (Mutiny Multi<Message<?>> ) and potentially return a new stream of messages. Note that if a decorator is available it will be called on every channel. The decorate method receives the channel name and whether the channel is a connector or not as parameters. Decorators are called ordered from highest to lowest priority (from the least value to the greatest), obtained using the jakarta.enterprise.inject.spi.Prioritized#getPriority method. Note The SubscriberDecorator receive a list of channel names because @Incoming annotation is repeatable and consuming methods can be linked to multiple channels. Intercepting Outgoing Messages Decorators can be used to intercept and alter messages, both on incoming and outgoing channels. Smallrye Reactive Messaging provides a SubscriberDecorator by default to allow intercepting outgoing messages for a specific channel. To provide an outgoing interceptor implement a bean exposing the interface OutgoingInterceptor , qualified with a @Identifier with the channel name to intercept. Only one interceptor is allowed to be bound for interception per outgoing channel. If no interceptors are found with a @Identifier but a @Default one is available, it is used. When multiple interceptors are available, the bean with the highest priority is used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package interceptors ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.OutgoingInterceptor ; import io.smallrye.reactive.messaging.OutgoingMessageMetadata ; @Identifier ( \"channel-a\" ) @ApplicationScoped public class MyInterceptor implements OutgoingInterceptor { @Override public Message <?> onMessage ( Message <?> message ) { return message . withPayload ( \"changed \" + message . getPayload ()); } @Override public void onMessageAck ( Message <?> message ) { message . getMetadata ( OutgoingMessageMetadata . class ) . ifPresent ( m -> m . getResult ()); } @Override public void onMessageNack ( Message <?> message , Throwable failure ) { } } An OutgoingInterceptor can implement these three methods: Message<?> onMessage(Message<?> message) : Called before passing the message to the outgoing connector for transmission. The message can be altered by returning a new message from this method. void onMessageAck(Message<?> message) : Called after message acknowledgment. This callback can access OutgoingMessageMetadata which will hold the result of the message transmission to the broker, if supported by the connector. This is only supported by MQTT and Kafka connectors. void onMessageNack(Message<?> message, Throwable failure) : Called before message negative-acknowledgment. Note If you are willing to adapt an incoming message payload to fit a consuming method receiving type, you can use MessageConverter s.","title":"Channel Decorators"},{"location":"concepts/decorators/#channel-decorators","text":"SmallRye Reactive Messaging supports decorating reactive streams of incoming and outgoing channels for implementing cross-cutting concerns such as monitoring, tracing or message interception. Two symmetrical APIs are proposed for decorating publisher and subscriber channels, PublisherDecorator and SubscriberDecorator respectively. Important @Incoming channels and channels bound to an outbound connector are both Subscriber s. Conversely @Outgoing channels and channels bound to an inbound connector are Publisher s. For example, to provide a decorator which counts consumed messages from incoming connector, implement a bean exposing the interface PublisherDecorator : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @ApplicationScoped public class ConsumedMessageDecorator implements PublisherDecorator { private final Map < String , AtomicLong > counters = new HashMap <> (); @Override public Multi <? extends Message <?>> decorate ( Multi <? extends Message <?>> publisher , String channelName , boolean isConnector ) { if ( isConnector ) { AtomicLong counter = new AtomicLong (); counters . put ( channelName , counter ); return publisher . onItem (). invoke ( counter :: incrementAndGet ); } else { return publisher ; } } @Override public int getPriority () { return 10 ; } public long getMessageCount ( String channel ) { return counters . get ( channel ). get (); } } Decorators' decorate method is called only once per channel at application deployment when graph wiring is taking place. Decorators are very powerful because they receive the stream of messages (Mutiny Multi<Message<?>> ) and potentially return a new stream of messages. Note that if a decorator is available it will be called on every channel. The decorate method receives the channel name and whether the channel is a connector or not as parameters. Decorators are called ordered from highest to lowest priority (from the least value to the greatest), obtained using the jakarta.enterprise.inject.spi.Prioritized#getPriority method. Note The SubscriberDecorator receive a list of channel names because @Incoming annotation is repeatable and consuming methods can be linked to multiple channels.","title":"Channel Decorators"},{"location":"concepts/decorators/#intercepting-outgoing-messages","text":"Decorators can be used to intercept and alter messages, both on incoming and outgoing channels. Smallrye Reactive Messaging provides a SubscriberDecorator by default to allow intercepting outgoing messages for a specific channel. To provide an outgoing interceptor implement a bean exposing the interface OutgoingInterceptor , qualified with a @Identifier with the channel name to intercept. Only one interceptor is allowed to be bound for interception per outgoing channel. If no interceptors are found with a @Identifier but a @Default one is available, it is used. When multiple interceptors are available, the bean with the highest priority is used. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package interceptors ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.OutgoingInterceptor ; import io.smallrye.reactive.messaging.OutgoingMessageMetadata ; @Identifier ( \"channel-a\" ) @ApplicationScoped public class MyInterceptor implements OutgoingInterceptor { @Override public Message <?> onMessage ( Message <?> message ) { return message . withPayload ( \"changed \" + message . getPayload ()); } @Override public void onMessageAck ( Message <?> message ) { message . getMetadata ( OutgoingMessageMetadata . class ) . ifPresent ( m -> m . getResult ()); } @Override public void onMessageNack ( Message <?> message , Throwable failure ) { } } An OutgoingInterceptor can implement these three methods: Message<?> onMessage(Message<?> message) : Called before passing the message to the outgoing connector for transmission. The message can be altered by returning a new message from this method. void onMessageAck(Message<?> message) : Called after message acknowledgment. This callback can access OutgoingMessageMetadata which will hold the result of the message transmission to the broker, if supported by the connector. This is only supported by MQTT and Kafka connectors. void onMessageNack(Message<?> message, Throwable failure) : Called before message negative-acknowledgment. Note If you are willing to adapt an incoming message payload to fit a consuming method receiving type, you can use MessageConverter s.","title":"Intercepting Outgoing Messages"},{"location":"concepts/emitter/","text":"Emitter and Channels It is not rare to combine in a single application imperative parts (Jax-RS, regular CDI beans ) and reactive parts ( beans with @Incoming and @Outgoing annotations). In these case, it\u2019s often required to send messages from the imperative part to the reactive part. In other words, send messages to channels handled by reactive messaging and how can you retrieve messages. Emitter and @Channel To send things (payload or Message ) from imperative code to a specific channel you need to use: the org.eclipse.microprofile.reactive.messaging.Channel annotations the org.eclipse.microprofile.reactive.messaging.Emitter type The @Channel lets you indicate to which channel you are going to send your payloads or messages. The Emitter is the object to use to send these payloads or messages. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import org.eclipse.microprofile.reactive.messaging.Channel ; import org.eclipse.microprofile.reactive.messaging.Emitter ; @ApplicationScoped public class MyImperativeBean { @Inject @Channel ( \"prices\" ) Emitter < Double > emitter ; // ... public void send ( double d ) { emitter . send ( d ); } } The Emitter class takes a type parameter. It\u2019s the type of payload. Even if you want to send Messages , the type is the payload type. Important You must have a @Incoming(\"prices\") somewhere in your application (meaning a method consuming messages transiting on the channel prices ), or an outbound connector configured to manage the prices channel ( mp.messaging.outgoing.prices... ) Sending payloads Sending payloads is done as follows: 1 2 3 4 5 6 7 @Inject @Channel ( \"prices\" ) Emitter < Double > emitterForPrices ; public void send ( double d ) { emitterForPrices . send ( d ); } When sending a payload, the emitter returns a CompletionStage . This CompletionStage gets completed once the message created from the payload is acknowledged: 1 2 3 4 5 6 public void sendAndAwaitAcknowledgement ( double d ) { CompletionStage < Void > acked = emitterForPrices . send ( d ); // sending a payload returns a CompletionStage completed // when the message is acknowledged acked . toCompletableFuture (). join (); } If the processing fails, the CompletionStage gets completed exceptionally (with the reason of the nack). Sending messages You can also send Messages : 1 2 3 public void sendAsMessage ( double d ) { emitterForPrices . send ( Message . of ( d )); } When sending a Message , the emitter does not return a CompletionStage , but you can pass the ack/nack callback, and be called when the message is acked/nacked. 1 2 3 4 5 6 7 8 9 10 public void sendAsMessageWithAck ( double d ) { emitterForPrices . send ( Message . of ( d , () -> { // Called when the message is acknowledged. return CompletableFuture . completedFuture ( null ); }, reason -> { // Called when the message is acknowledged negatively. return CompletableFuture . completedFuture ( null ); })); } Sending messages also let you pass metadata. 1 2 3 4 5 6 7 8 9 10 11 12 public void sendAsMessageWithAckAndMetadata ( double d ) { MyMetadata metadata = new MyMetadata (); emitterForPrices . send ( Message . of ( d , Metadata . of ( metadata ), () -> { // Called when the message is acknowledged. return CompletableFuture . completedFuture ( null ); }, reason -> { // Called when the message is acknowledged negatively. return CompletableFuture . completedFuture ( null ); })); } Metadata can be used to propagate some context objects with the message. Overflow management When sending messages from imperative code to reactive code, you must be aware of back-pressure. Indeed, messages sent using the emitter and stored in a queue . If the consumer does not process the messages quickly enough, this queue can become a memory hog and you may even run out of memory. To control what need to happen when the queue becomes out of control, use the OnOverflow annotation. @OnOverflow lets you configure: the maximum size of the queue (default is 256) what needs to happen when this size is reached (fail, drop...) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Set the max size to 10 and fail if reached @OnOverflow ( value = OnOverflow . Strategy . BUFFER , bufferSize = 10 ) @Inject @Channel ( \"channel\" ) Emitter < String > emitterWithBuffer ; // [DANGER ZONE] no limit @OnOverflow ( OnOverflow . Strategy . UNBOUNDED_BUFFER ) @Inject @Channel ( \"channel\" ) Emitter < String > danger ; // Drop the new messages if the size is reached @OnOverflow ( OnOverflow . Strategy . DROP ) @Inject @Channel ( \"channel\" ) Emitter < String > dropping ; // Drop the previously sent messages if the size is reached @OnOverflow ( OnOverflow . Strategy . LATEST ) @Inject @Channel ( \"channel\" ) Emitter < String > dropOldMessages ; The supported strategies are: OnOverflow.Strategy.BUFFER - use a buffer to store the elements until they are consumed. If the buffer is full, a failure is propagated (and the thread using the emitted gets an exception) OnOverflow.Strategy.UNBOUNDED_BUFFER - use an unbounded buffer to store the elements OnOverflow.Strategy.DROP - drops the most recent value if the downstream can\u2019t keep up. It means that new value emitted by the emitter are ignored. OnOverflow.Strategy.FAIL - propagates a failure in case the downstream can\u2019t keep up. OnOverflow.Strategy.LATEST - keeps only the latest value, dropping any previous value if the downstream can\u2019t keep up. OnOverflow.Strategy.NONE - ignore the back-pressure signals letting the downstream consumer to implement a strategy. Defensive emission Having an emitter injected into your code does not guarantee that someone is ready to consume the message. For example, a subscriber may be connecting to a remote broker. If there are no subscribers, using the send method will throw an exception. The emitter.hasRequests() method indicates that a subscriber subscribes to the channel and requested items. So, you can wrap your emission with: 1 2 3 if ( emitter . hasRequests ()) { emitter . send ( \"hello\" ); } If you use the OnOverflow.Strategy.DROP , you can use the send method even with no subscribers nor demands. The message will be nacked immediately. Retrieving channels You can use the @Channel annotation to inject in your bean the underlying stream. Note that in this case, you will be responsible for the subscription: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Inject @Channel ( \"my-channel\" ) Multi < String > streamOfPayloads ; @Inject @Channel ( \"my-channel\" ) Multi < Message < String >> streamOfMessages ; @Inject @Channel ( \"my-channel\" ) Publisher < String > publisherOfPayloads ; @Inject @Channel ( \"my-channel\" ) Publisher < Message < String >> publisherOfMessages ; Important You must have a @Outgoing(\"my-channel\") somewhere in your application (meaning a method generating messages transiting on the channel my-channel ), or an inbound connector configured to manage the prices channel ( mp.messaging.incoming.prices... ) Injected channels merge all the matching outgoing - so if you have multiple @Outgoing(\"out\") , @Inject @Channel(\"out\") gets all the messages. If your injected channel receives payloads ( Multi<T> ), it acknowledges the message automatically, and support multiple subscribers. If you injected channel receives Message ( Multi<Message<T>> ), you will be responsible for the acknowledgement and broadcasting. Emitter and @Broadcast When using an Emitter , you can now @Broadcast what is emitted to all subscribers. Here is an example of emitting a price with two methods marked @Incoming to receive the broadcast: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Inject @Broadcast @Channel ( \"prices\" ) Emitter < Double > emitter ; public void emit ( double d ) { emitter . send ( d ); } @Incoming ( \"prices\" ) public void handle ( double d ) { // Handle the new price } @Incoming ( \"prices\" ) public void audit ( double d ) { // Audit the price change } For more details see @Broadcast documentation. Mutiny Emitter If you prefer to utilize Uni in all your code, there is now a MutinyEmitter that will return Uni<Void> instead of void . 1 2 3 4 5 6 7 @Inject @Channel ( \"prices\" ) MutinyEmitter < Double > emitter ; public Uni < Void > send ( double d ) { return emitter . send ( d ); } There\u2019s also the ability to block on sending the event to the emitter. It will only return from the method when the event is acknowledged, or nacked, by the receiver: 1 2 3 public void sendAwait ( double d ) { emitter . sendAndAwait ( d ); } And if you don\u2019t need to worry about the success or failure of sending an event, you can sendAndForget : 1 2 3 public Cancellable sendForget ( double d ) { return emitter . sendAndForget ( d ); } Custom Emitter Implementations Experimental Custom emitter implementations is an experimental feature. Emitter and MutinyEmitter are two implementations of the emitter concept, where imperative code in your application can send messages to Reactive Messaging channels. With EmitterFactory it is possible to provide custom implementations, and application facing emitter interfaces. In the following example, the injectable custom emitter interface is CustomEmitter , and it is implemented by CustomEmitterImpl : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public interface CustomEmitter < T > extends EmitterType { < M extends Message <? extends T >> void sendAndForget ( M msg ); } public static class CustomEmitterImpl < T > implements CustomEmitter < T > , MessagePublisherProvider < Object > { Publisher < Message <?>> publisher ; public CustomEmitterImpl ( EmitterConfiguration configuration , long defaultBufferSize ) { //... initialize emitter with configuration } @Override public Publisher < Message <?>> getPublisher () { return publisher ; } @Override public < M extends Message <? extends T >> void sendAndForget ( M msg ) { //... send to stream } } Note that CustomEmitter interface extends EmitterType , which is a marker interface for discovering custom emitter types. Also, CustomEmitterImpl implements the MessagePublisherProvider , which is used by the framework to transform this emitter to a channel. Then we need to provide an implementation of the EmitterFactory interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @EmitterFactoryFor ( CustomEmitter . class ) @ApplicationScoped public static class CustomEmitterFactory implements EmitterFactory < CustomEmitterImpl < Object >> { @Inject ChannelRegistry channelRegistry ; @Override public CustomEmitterImpl < Object > createEmitter ( EmitterConfiguration configuration , long defaultBufferSize ) { return new CustomEmitterImpl <> ( configuration , defaultBufferSize ); } @Produces @Channel ( \"\" ) // Stream name is ignored during type-safe resolution < T > CustomEmitter < T > produce ( InjectionPoint injectionPoint ) { String channelName = ChannelProducer . getChannelName ( injectionPoint ); return channelRegistry . getEmitter ( channelName , CustomEmitter . class ); } } The CustomEmitterFactory is a CDI managed bean, which implements the EmitterFactory . It is qualified with EmitterFactoryFor annotation which is configured with the emitter interface CustomEmitter that this factory provides. Smallrye Reactive Messaging discovers the emitter factory during the CDI deployment validation and verifies that custom emitters used by the application have corresponding emitter factories. It'll use the emitter factory to create the emitter implementation and will register the implementation into the ChannelRegistry . Note that the CustomEmitterFactory also uses the ChannelRegistry and provides the custom emitter with @Produces . Finally, the application can inject and use the CustomEmitter as a normal emitter channel: 1 2 3 4 5 6 7 8 9 10 11 @Inject @Channel ( \"custom-emitter-channel\" ) CustomEmitter < String > customEmitter ; //... public void emitMessage () { customEmitter . sendAndForget ( Message . of ( \"a\" )); customEmitter . sendAndForget ( Message . of ( \"b\" )); customEmitter . sendAndForget ( Message . of ( \"c\" )); }","title":"Emitters and Channel"},{"location":"concepts/emitter/#emitter-and-channels","text":"It is not rare to combine in a single application imperative parts (Jax-RS, regular CDI beans ) and reactive parts ( beans with @Incoming and @Outgoing annotations). In these case, it\u2019s often required to send messages from the imperative part to the reactive part. In other words, send messages to channels handled by reactive messaging and how can you retrieve messages.","title":"Emitter and Channels"},{"location":"concepts/emitter/#emitter-and-channel","text":"To send things (payload or Message ) from imperative code to a specific channel you need to use: the org.eclipse.microprofile.reactive.messaging.Channel annotations the org.eclipse.microprofile.reactive.messaging.Emitter type The @Channel lets you indicate to which channel you are going to send your payloads or messages. The Emitter is the object to use to send these payloads or messages. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import org.eclipse.microprofile.reactive.messaging.Channel ; import org.eclipse.microprofile.reactive.messaging.Emitter ; @ApplicationScoped public class MyImperativeBean { @Inject @Channel ( \"prices\" ) Emitter < Double > emitter ; // ... public void send ( double d ) { emitter . send ( d ); } } The Emitter class takes a type parameter. It\u2019s the type of payload. Even if you want to send Messages , the type is the payload type. Important You must have a @Incoming(\"prices\") somewhere in your application (meaning a method consuming messages transiting on the channel prices ), or an outbound connector configured to manage the prices channel ( mp.messaging.outgoing.prices... )","title":"Emitter and @Channel"},{"location":"concepts/emitter/#sending-payloads","text":"Sending payloads is done as follows: 1 2 3 4 5 6 7 @Inject @Channel ( \"prices\" ) Emitter < Double > emitterForPrices ; public void send ( double d ) { emitterForPrices . send ( d ); } When sending a payload, the emitter returns a CompletionStage . This CompletionStage gets completed once the message created from the payload is acknowledged: 1 2 3 4 5 6 public void sendAndAwaitAcknowledgement ( double d ) { CompletionStage < Void > acked = emitterForPrices . send ( d ); // sending a payload returns a CompletionStage completed // when the message is acknowledged acked . toCompletableFuture (). join (); } If the processing fails, the CompletionStage gets completed exceptionally (with the reason of the nack).","title":"Sending payloads"},{"location":"concepts/emitter/#sending-messages","text":"You can also send Messages : 1 2 3 public void sendAsMessage ( double d ) { emitterForPrices . send ( Message . of ( d )); } When sending a Message , the emitter does not return a CompletionStage , but you can pass the ack/nack callback, and be called when the message is acked/nacked. 1 2 3 4 5 6 7 8 9 10 public void sendAsMessageWithAck ( double d ) { emitterForPrices . send ( Message . of ( d , () -> { // Called when the message is acknowledged. return CompletableFuture . completedFuture ( null ); }, reason -> { // Called when the message is acknowledged negatively. return CompletableFuture . completedFuture ( null ); })); } Sending messages also let you pass metadata. 1 2 3 4 5 6 7 8 9 10 11 12 public void sendAsMessageWithAckAndMetadata ( double d ) { MyMetadata metadata = new MyMetadata (); emitterForPrices . send ( Message . of ( d , Metadata . of ( metadata ), () -> { // Called when the message is acknowledged. return CompletableFuture . completedFuture ( null ); }, reason -> { // Called when the message is acknowledged negatively. return CompletableFuture . completedFuture ( null ); })); } Metadata can be used to propagate some context objects with the message.","title":"Sending messages"},{"location":"concepts/emitter/#overflow-management","text":"When sending messages from imperative code to reactive code, you must be aware of back-pressure. Indeed, messages sent using the emitter and stored in a queue . If the consumer does not process the messages quickly enough, this queue can become a memory hog and you may even run out of memory. To control what need to happen when the queue becomes out of control, use the OnOverflow annotation. @OnOverflow lets you configure: the maximum size of the queue (default is 256) what needs to happen when this size is reached (fail, drop...) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Set the max size to 10 and fail if reached @OnOverflow ( value = OnOverflow . Strategy . BUFFER , bufferSize = 10 ) @Inject @Channel ( \"channel\" ) Emitter < String > emitterWithBuffer ; // [DANGER ZONE] no limit @OnOverflow ( OnOverflow . Strategy . UNBOUNDED_BUFFER ) @Inject @Channel ( \"channel\" ) Emitter < String > danger ; // Drop the new messages if the size is reached @OnOverflow ( OnOverflow . Strategy . DROP ) @Inject @Channel ( \"channel\" ) Emitter < String > dropping ; // Drop the previously sent messages if the size is reached @OnOverflow ( OnOverflow . Strategy . LATEST ) @Inject @Channel ( \"channel\" ) Emitter < String > dropOldMessages ; The supported strategies are: OnOverflow.Strategy.BUFFER - use a buffer to store the elements until they are consumed. If the buffer is full, a failure is propagated (and the thread using the emitted gets an exception) OnOverflow.Strategy.UNBOUNDED_BUFFER - use an unbounded buffer to store the elements OnOverflow.Strategy.DROP - drops the most recent value if the downstream can\u2019t keep up. It means that new value emitted by the emitter are ignored. OnOverflow.Strategy.FAIL - propagates a failure in case the downstream can\u2019t keep up. OnOverflow.Strategy.LATEST - keeps only the latest value, dropping any previous value if the downstream can\u2019t keep up. OnOverflow.Strategy.NONE - ignore the back-pressure signals letting the downstream consumer to implement a strategy.","title":"Overflow management"},{"location":"concepts/emitter/#defensive-emission","text":"Having an emitter injected into your code does not guarantee that someone is ready to consume the message. For example, a subscriber may be connecting to a remote broker. If there are no subscribers, using the send method will throw an exception. The emitter.hasRequests() method indicates that a subscriber subscribes to the channel and requested items. So, you can wrap your emission with: 1 2 3 if ( emitter . hasRequests ()) { emitter . send ( \"hello\" ); } If you use the OnOverflow.Strategy.DROP , you can use the send method even with no subscribers nor demands. The message will be nacked immediately.","title":"Defensive emission"},{"location":"concepts/emitter/#retrieving-channels","text":"You can use the @Channel annotation to inject in your bean the underlying stream. Note that in this case, you will be responsible for the subscription: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Inject @Channel ( \"my-channel\" ) Multi < String > streamOfPayloads ; @Inject @Channel ( \"my-channel\" ) Multi < Message < String >> streamOfMessages ; @Inject @Channel ( \"my-channel\" ) Publisher < String > publisherOfPayloads ; @Inject @Channel ( \"my-channel\" ) Publisher < Message < String >> publisherOfMessages ; Important You must have a @Outgoing(\"my-channel\") somewhere in your application (meaning a method generating messages transiting on the channel my-channel ), or an inbound connector configured to manage the prices channel ( mp.messaging.incoming.prices... ) Injected channels merge all the matching outgoing - so if you have multiple @Outgoing(\"out\") , @Inject @Channel(\"out\") gets all the messages. If your injected channel receives payloads ( Multi<T> ), it acknowledges the message automatically, and support multiple subscribers. If you injected channel receives Message ( Multi<Message<T>> ), you will be responsible for the acknowledgement and broadcasting.","title":"Retrieving channels"},{"location":"concepts/emitter/#emitter-and-broadcast","text":"When using an Emitter , you can now @Broadcast what is emitted to all subscribers. Here is an example of emitting a price with two methods marked @Incoming to receive the broadcast: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Inject @Broadcast @Channel ( \"prices\" ) Emitter < Double > emitter ; public void emit ( double d ) { emitter . send ( d ); } @Incoming ( \"prices\" ) public void handle ( double d ) { // Handle the new price } @Incoming ( \"prices\" ) public void audit ( double d ) { // Audit the price change } For more details see @Broadcast documentation.","title":"Emitter and @Broadcast"},{"location":"concepts/emitter/#mutiny-emitter","text":"If you prefer to utilize Uni in all your code, there is now a MutinyEmitter that will return Uni<Void> instead of void . 1 2 3 4 5 6 7 @Inject @Channel ( \"prices\" ) MutinyEmitter < Double > emitter ; public Uni < Void > send ( double d ) { return emitter . send ( d ); } There\u2019s also the ability to block on sending the event to the emitter. It will only return from the method when the event is acknowledged, or nacked, by the receiver: 1 2 3 public void sendAwait ( double d ) { emitter . sendAndAwait ( d ); } And if you don\u2019t need to worry about the success or failure of sending an event, you can sendAndForget : 1 2 3 public Cancellable sendForget ( double d ) { return emitter . sendAndForget ( d ); }","title":"Mutiny Emitter"},{"location":"concepts/emitter/#custom-emitter-implementations","text":"Experimental Custom emitter implementations is an experimental feature. Emitter and MutinyEmitter are two implementations of the emitter concept, where imperative code in your application can send messages to Reactive Messaging channels. With EmitterFactory it is possible to provide custom implementations, and application facing emitter interfaces. In the following example, the injectable custom emitter interface is CustomEmitter , and it is implemented by CustomEmitterImpl : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public interface CustomEmitter < T > extends EmitterType { < M extends Message <? extends T >> void sendAndForget ( M msg ); } public static class CustomEmitterImpl < T > implements CustomEmitter < T > , MessagePublisherProvider < Object > { Publisher < Message <?>> publisher ; public CustomEmitterImpl ( EmitterConfiguration configuration , long defaultBufferSize ) { //... initialize emitter with configuration } @Override public Publisher < Message <?>> getPublisher () { return publisher ; } @Override public < M extends Message <? extends T >> void sendAndForget ( M msg ) { //... send to stream } } Note that CustomEmitter interface extends EmitterType , which is a marker interface for discovering custom emitter types. Also, CustomEmitterImpl implements the MessagePublisherProvider , which is used by the framework to transform this emitter to a channel. Then we need to provide an implementation of the EmitterFactory interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @EmitterFactoryFor ( CustomEmitter . class ) @ApplicationScoped public static class CustomEmitterFactory implements EmitterFactory < CustomEmitterImpl < Object >> { @Inject ChannelRegistry channelRegistry ; @Override public CustomEmitterImpl < Object > createEmitter ( EmitterConfiguration configuration , long defaultBufferSize ) { return new CustomEmitterImpl <> ( configuration , defaultBufferSize ); } @Produces @Channel ( \"\" ) // Stream name is ignored during type-safe resolution < T > CustomEmitter < T > produce ( InjectionPoint injectionPoint ) { String channelName = ChannelProducer . getChannelName ( injectionPoint ); return channelRegistry . getEmitter ( channelName , CustomEmitter . class ); } } The CustomEmitterFactory is a CDI managed bean, which implements the EmitterFactory . It is qualified with EmitterFactoryFor annotation which is configured with the emitter interface CustomEmitter that this factory provides. Smallrye Reactive Messaging discovers the emitter factory during the CDI deployment validation and verifies that custom emitters used by the application have corresponding emitter factories. It'll use the emitter factory to create the emitter implementation and will register the implementation into the ChannelRegistry . Note that the CustomEmitterFactory also uses the ChannelRegistry and provides the custom emitter with @Produces . Finally, the application can inject and use the CustomEmitter as a normal emitter channel: 1 2 3 4 5 6 7 8 9 10 11 @Inject @Channel ( \"custom-emitter-channel\" ) CustomEmitter < String > customEmitter ; //... public void emitMessage () { customEmitter . sendAndForget ( Message . of ( \"a\" )); customEmitter . sendAndForget ( Message . of ( \"b\" )); customEmitter . sendAndForget ( Message . of ( \"c\" )); }","title":"Custom Emitter Implementations"},{"location":"concepts/incomings/","text":"Multiple Incoming Channels Experimental Multiple @Incomings is an experimental feature. The @Incoming annotation is repeatable. It means that the method receives the messages transiting on every listed channels, in no specific order: 1 2 3 4 5 6 @Incoming ( \"channel-1\" ) @Incoming ( \"channel-2\" ) public String process ( String s ) { // get messages from channel-1 and channel-2 return s . toUpperCase (); }","title":"@Incomings"},{"location":"concepts/incomings/#multiple-incoming-channels","text":"Experimental Multiple @Incomings is an experimental feature. The @Incoming annotation is repeatable. It means that the method receives the messages transiting on every listed channels, in no specific order: 1 2 3 4 5 6 @Incoming ( \"channel-1\" ) @Incoming ( \"channel-2\" ) public String process ( String s ) { // get messages from channel-1 and channel-2 return s . toUpperCase (); }","title":"Multiple Incoming Channels"},{"location":"concepts/logging/","text":"Logging SmallRye Reactive Messaging uses JBoss Logging as logging API. This section explains how to configure the loggers for various logging backends. Tip If you are developing SmallRye Reactive Messaging and wonder about how the logs are managed, it uses JBoss Logging Tools . Logging Backends SmallRye Reactive Messaging uses the JBoss Logging library to write messages to a log file. This library is a logging bridge that integrates different log frameworks. You can decide which of the following frameworks you want to use for your application: JBoss LogManager ( jboss ) Log4j 2 ( log4j2 ) Log4j 1 ( log4j ) Slf4j ( slf4j ) JDK logging ( jul ) You only need to add the chosen framework to the classpath, and the JBoss Logging library will pick it up. If there are multiple frameworks available on the classpath, it picks the first found (in the order from the list). Alternatively, you can set the org.jboss.logging.provider system property is one of the values given above. The concepts and log categories are the same for all frameworks. However, the format of the configuration file and the names of the log levels differ. Check the documentation of your logging library to find out which dependencies are required, the exact name of the log levels, and where the configuration should be written. Log Categories As all applications and frameworks, SmallRye Reactive Messaging writes log messages in different categories and log levels. The categories group messages from specific connectors, classes or components. The following table shows the essential log categories used by SmallRye Reactive Messaging: Category Description io.smallrye.reactive.messaging This category contains all the messages written by SmallRye Reactive Messaging. io.smallrye.reactive.messaging.provider This category contains all the messages generated by the core (provider). io.smallrye.reactive.messaging.kafka This category contains all the messages generated by the Kafka Connector. io.smallrye.reactive.messaging.amqp This category contains all the messages generated by the AMQP Connector. io.smallrye.reactive.messaging.jms This category contains all the messages generated by the JMS Connector. io.smallrye.reactive.messaging.mqtt This category contains all the messages generated by the MQTT (Client) Connector. io.smallrye.reactive.messaging.mqtt-server This category contains all the messages generated by the MQTT (Server) Connector. The names of the log levels are defined by your logging framework and determine the amount and granularity of the log messages. You can assign a log level to each category. If you do not specify a specific category\u2019s log level, it will inherit the level from its parent category. Thus, setting the log level of io.smallrye.reactive.messaging influences every loggers from SmallRye Reactive Messaging. Message Code Each message has an identifier code. They are all prefixed with SRMSG , followed with the numeric code. In the following output, the code is SRMSG00229 : 1 [2020-06-15 13:35:07] [INFO ] SRMSG00229: Channel manager initializing... Recommended logging configurations Development Log4J 1 log4j.properties 1 2 3 4 5 6 7 8 log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target = System.out log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern = %d{HH:mm:ss,SSS} %-5p [%c] - %m%n log4j.rootLogger = info, stdout log4j.logger.io.smallrye.reactive.messaging = info log4j.logger.org.jboss.weld = warn Log4J 2 log4j2.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <Configuration monitorInterval= \"60\" > <Properties> <Property name= \"log-path\" > PropertiesConfiguration </Property> </Properties> <Appenders> <Console name= \"Console-Appender\" target= \"SYSTEM*OUT\" > <PatternLayout> <pattern> [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n </pattern> > </PatternLayout> </Console> </Appenders> <Loggers> <Logger name= \"io.smallrye.reactive.messaging\" level= \"info\" additivity= \"false\" > <AppenderRef ref= \"Console-Appender\" /> </Logger> <Logger name= \"org.jboss.weld\" level= \"warn\" additivity= \"false\" > <AppenderRef ref= \"Console-Appender\" /> </Logger> <Root level= \"info\" > <AppenderRef ref= \"Console-Appender\" /> </Root> </Loggers> </Configuration> JDK (JUL) logging.properties 1 2 3 4 5 6 7 8 9 handlers = java.util.logging.ConsoleHandler java.util.logging.ConsoleHandler.level = FINEST java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format = [%1$tF %1$tT] [%4$-7s] %5$s %n .level = INFO io.smallrye.reactive.messaging.level = INFO org.jboss.weld.level = WARNING LogBack via SLF4J* logback.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <Pattern> %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n </Pattern> </encoder> </appender> <logger name= \"io.smallrye.reactive.messaging\" level= \"info\" additivity= \"false\" > <appender-ref ref= \"STDOUT\" /> </logger> <logger name= \"org.jboss.weld\" level= \"warn\" additivity= \"false\" > <appender-ref ref= \"STDOUT\" /> </logger> <root level= \"info\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration> Production Log4J 1 log4j.properties 1 2 3 4 5 6 7 8 log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target = System.out log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern = %d{HH:mm:ss,SSS} %-5p [%c] - %m%n log4j.rootLogger = info, stdout log4j.logger.io.smallrye.reactive.messaging = warn log4j.logger.org.jboss.weld = error Log4J 2 log4j2.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <Configuration monitorInterval= \"60\" > <Properties> <Property name= \"log-path\" > PropertiesConfiguration </Property> </Properties> <Appenders> <Console name= \"Console-Appender\" target= \"SYSTEM*OUT\" > <PatternLayout> <pattern> [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n </pattern> > </PatternLayout> </Console> </Appenders> <Loggers> <Logger name= \"io.smallrye.reactive.messaging\" level= \"warn\" additivity= \"false\" > <AppenderRef ref= \"Console-Appender\" /> </Logger> <Logger name= \"org.jboss.weld\" level= \"error\" additivity= \"false\" > <AppenderRef ref= \"Console-Appender\" /> </Logger> <Root level= \"info\" > <AppenderRef ref= \"Console-Appender\" /> </Root> </Loggers> </Configuration> JDK (JUL) logging.properties 1 2 3 4 5 6 7 8 9 handlers = java.util.logging.ConsoleHandler java.util.logging.ConsoleHandler.level = INFO java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter java.util.logging.SimpleFormatter.format = [%1$tF %1$tT] [%4$-7s] %5$s %n .level = INFO io.smallrye.reactive.messaging.level = WARNING org.jboss.weld.level = SEVERE logback.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <configuration> <appender name= \"STDOUT\" class= \"ch.qos.logback.core.ConsoleAppender\" > <encoder class= \"ch.qos.logback.classic.encoder.PatternLayoutEncoder\" > <Pattern> %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n </Pattern> </encoder> </appender> <logger name= \"io.smallrye.reactive.messaging\" level= \"warn\" additivity= \"false\" > <appender-ref ref= \"STDOUT\" /> </logger> <logger name= \"org.jboss.weld\" level= \"error\" additivity= \"false\" > <appender-ref ref= \"STDOUT\" /> </logger> <root level= \"info\" > <appender-ref ref= \"STDOUT\" /> </root> </configuration>","title":"Logging"},{"location":"concepts/logging/#logging","text":"SmallRye Reactive Messaging uses JBoss Logging as logging API. This section explains how to configure the loggers for various logging backends. Tip If you are developing SmallRye Reactive Messaging and wonder about how the logs are managed, it uses JBoss Logging Tools .","title":"Logging"},{"location":"concepts/logging/#logging-backends","text":"SmallRye Reactive Messaging uses the JBoss Logging library to write messages to a log file. This library is a logging bridge that integrates different log frameworks. You can decide which of the following frameworks you want to use for your application: JBoss LogManager ( jboss ) Log4j 2 ( log4j2 ) Log4j 1 ( log4j ) Slf4j ( slf4j ) JDK logging ( jul ) You only need to add the chosen framework to the classpath, and the JBoss Logging library will pick it up. If there are multiple frameworks available on the classpath, it picks the first found (in the order from the list). Alternatively, you can set the org.jboss.logging.provider system property is one of the values given above. The concepts and log categories are the same for all frameworks. However, the format of the configuration file and the names of the log levels differ. Check the documentation of your logging library to find out which dependencies are required, the exact name of the log levels, and where the configuration should be written.","title":"Logging Backends"},{"location":"concepts/logging/#log-categories","text":"As all applications and frameworks, SmallRye Reactive Messaging writes log messages in different categories and log levels. The categories group messages from specific connectors, classes or components. The following table shows the essential log categories used by SmallRye Reactive Messaging: Category Description io.smallrye.reactive.messaging This category contains all the messages written by SmallRye Reactive Messaging. io.smallrye.reactive.messaging.provider This category contains all the messages generated by the core (provider). io.smallrye.reactive.messaging.kafka This category contains all the messages generated by the Kafka Connector. io.smallrye.reactive.messaging.amqp This category contains all the messages generated by the AMQP Connector. io.smallrye.reactive.messaging.jms This category contains all the messages generated by the JMS Connector. io.smallrye.reactive.messaging.mqtt This category contains all the messages generated by the MQTT (Client) Connector. io.smallrye.reactive.messaging.mqtt-server This category contains all the messages generated by the MQTT (Server) Connector. The names of the log levels are defined by your logging framework and determine the amount and granularity of the log messages. You can assign a log level to each category. If you do not specify a specific category\u2019s log level, it will inherit the level from its parent category. Thus, setting the log level of io.smallrye.reactive.messaging influences every loggers from SmallRye Reactive Messaging.","title":"Log Categories"},{"location":"concepts/logging/#message-code","text":"Each message has an identifier code. They are all prefixed with SRMSG , followed with the numeric code. In the following output, the code is SRMSG00229 : 1 [2020-06-15 13:35:07] [INFO ] SRMSG00229: Channel manager initializing...","title":"Message Code"},{"location":"concepts/logging/#recommended-logging-configurations","text":"","title":"Recommended logging configurations"},{"location":"concepts/logging/#development","text":"","title":"Development"},{"location":"concepts/logging/#production","text":"","title":"Production"},{"location":"concepts/merge/","text":"Merge channels Experimental @Merge is an experimental feature. By default, messages transiting in a channel can arise from a single producer. Having multiple producers is considered erroneous and is reported at deployment time. The Merge annotation changes this behavior and indicates that a channel can have multiple producers. @Merge must be used with the @Incoming annotation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Incoming ( \"in1\" ) @Outgoing ( \"out\" ) public int increment ( int i ) { return i + 1 ; } @Incoming ( \"in2\" ) @Outgoing ( \"out\" ) public int multiply ( int i ) { return i * 2 ; } @Incoming ( \"out\" ) @Merge public void getAll ( int i ) { //... } In the previous example, the consumer gets all the messages (from both producers). The @Merge annotation allows configuring how the incoming messages (from the different producers) are merged into the channel. The mode attribute allows configuring this behavior: ONE picks a single producer, discarding the other producer; MERGE (default) gets all the messages as they come, without any defined order. Messages from different producers may be interleaved. CONCAT concatenates the producers. The messages from one producer are received until the messages from other producers are received. Note Outbound connectors also support a merge attribute that allows consuming the messages to multiple upstreams. It will dispatch all the received messages.","title":"Merge channels"},{"location":"concepts/merge/#merge-channels","text":"Experimental @Merge is an experimental feature. By default, messages transiting in a channel can arise from a single producer. Having multiple producers is considered erroneous and is reported at deployment time. The Merge annotation changes this behavior and indicates that a channel can have multiple producers. @Merge must be used with the @Incoming annotation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Incoming ( \"in1\" ) @Outgoing ( \"out\" ) public int increment ( int i ) { return i + 1 ; } @Incoming ( \"in2\" ) @Outgoing ( \"out\" ) public int multiply ( int i ) { return i * 2 ; } @Incoming ( \"out\" ) @Merge public void getAll ( int i ) { //... } In the previous example, the consumer gets all the messages (from both producers). The @Merge annotation allows configuring how the incoming messages (from the different producers) are merged into the channel. The mode attribute allows configuring this behavior: ONE picks a single producer, discarding the other producer; MERGE (default) gets all the messages as they come, without any defined order. Messages from different producers may be interleaved. CONCAT concatenates the producers. The messages from one producer are received until the messages from other producers are received. Note Outbound connectors also support a merge attribute that allows consuming the messages to multiple upstreams. It will dispatch all the received messages.","title":"Merge channels"},{"location":"concepts/message-context/","text":"Message Contexts Message context provides a way to propagate data along the processing of a message. It can be used to propagate message specific objects in an implicit manner and be able to retrieve them later, such as the user, session or transaction. Important Message contexts are only support by Kafka, AMQP, RabbitMQ and MQTT connectors. Note Message context support is an experimental and SmallRye only feature. What's a message context A message context is execution context on which a message is processed. Each stage of the processing is going to use the same execution context. Thus, it allows storing data which can later be restored. For example, you can imagine storing some authentication ( User in the following example) data in one part of your processing and restore it in a later stage. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Incoming ( \"data\" ) @Outgoing ( \"process\" ) public Message < String > process ( Message < String > input ) { // Extract some data from the message and store it in the context User user = ...; // Store the extracted data into the message context. ContextLocals . put ( \"user\" , user ); record input ; } @Incoming ( \"process\" ) @Outgoing ( \"after-process\" ) public String handle ( String payload ) { // You can retrieve the store data using User user = ContextLocals . get ( \"user\" , null ); // ... return payload ; } The Message context is also available when using blocking or asynchronous stages (stage returning Uni or CompletionStage ) The difference with metadata Message metadata can be used to provide a similar feature. However, it requires using Messages which can be inconvenient (need to handle the acknowledgement manually). Message Contexts provide a simpler API, closer to a Message CDI scope : you can save data, and restore it later. The implicit propagation avoid having to deal with Messages . Supported signatures Message context works with: methods consuming or producing Messages , Uni<Message<T>> and CompletionStage<Message<T>> methods consuming or producing payloads, Uni<Payload> and CompletionStage<Payload> . blocking and non-blocking methods However, message context are NOT enforced when using methods consuming or producing: Multi , Flow.Publisher , Publisher and PublisherBuilder Subscriber , Flow.Subscriber , and SubscriberBuilder Processor , Flow.Processor , and ProcessorBuilder Under the hood Under the hood, the message context feature uses Vert.x duplicated contexts . A duplicated context is a view of the \"root\" (event loop) context, which is restored at each stage of the message processing. Each time that a compatible connector receives a message from a broker, it creates a new duplicated context and attaches it to the message. So the context is stored in the metadata of the message. When the message is processed, SmallRye Reactive Messaging makes sure that this processing is executed on the stored duplicated context.","title":"Message Context"},{"location":"concepts/message-context/#message-contexts","text":"Message context provides a way to propagate data along the processing of a message. It can be used to propagate message specific objects in an implicit manner and be able to retrieve them later, such as the user, session or transaction. Important Message contexts are only support by Kafka, AMQP, RabbitMQ and MQTT connectors. Note Message context support is an experimental and SmallRye only feature.","title":"Message Contexts"},{"location":"concepts/message-context/#whats-a-message-context","text":"A message context is execution context on which a message is processed. Each stage of the processing is going to use the same execution context. Thus, it allows storing data which can later be restored. For example, you can imagine storing some authentication ( User in the following example) data in one part of your processing and restore it in a later stage. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Incoming ( \"data\" ) @Outgoing ( \"process\" ) public Message < String > process ( Message < String > input ) { // Extract some data from the message and store it in the context User user = ...; // Store the extracted data into the message context. ContextLocals . put ( \"user\" , user ); record input ; } @Incoming ( \"process\" ) @Outgoing ( \"after-process\" ) public String handle ( String payload ) { // You can retrieve the store data using User user = ContextLocals . get ( \"user\" , null ); // ... return payload ; } The Message context is also available when using blocking or asynchronous stages (stage returning Uni or CompletionStage )","title":"What's a message context"},{"location":"concepts/message-context/#the-difference-with-metadata","text":"Message metadata can be used to provide a similar feature. However, it requires using Messages which can be inconvenient (need to handle the acknowledgement manually). Message Contexts provide a simpler API, closer to a Message CDI scope : you can save data, and restore it later. The implicit propagation avoid having to deal with Messages .","title":"The difference with metadata"},{"location":"concepts/message-context/#supported-signatures","text":"Message context works with: methods consuming or producing Messages , Uni<Message<T>> and CompletionStage<Message<T>> methods consuming or producing payloads, Uni<Payload> and CompletionStage<Payload> . blocking and non-blocking methods However, message context are NOT enforced when using methods consuming or producing: Multi , Flow.Publisher , Publisher and PublisherBuilder Subscriber , Flow.Subscriber , and SubscriberBuilder Processor , Flow.Processor , and ProcessorBuilder","title":"Supported signatures"},{"location":"concepts/message-context/#under-the-hood","text":"Under the hood, the message context feature uses Vert.x duplicated contexts . A duplicated context is a view of the \"root\" (event loop) context, which is restored at each stage of the message processing. Each time that a compatible connector receives a message from a broker, it creates a new duplicated context and attaches it to the message. So the context is stored in the metadata of the message. When the message is processed, SmallRye Reactive Messaging makes sure that this processing is executed on the stored duplicated context.","title":"Under the hood"},{"location":"concepts/model/","text":"Development Model Reactive Messaging proposes a CDI-based programming model to implement event-driven applications. Following the CDI principles, beans are forming the main building block of your application. Reactive Messaging provides a set of annotations and types to implement beans that generate, consume or process messages. @Incoming and @Outgoing Reactive Messaging provides two main annotations: org.eclipse.microprofile.reactive.messaging.Incoming - indicates the consumed channel org.eclipse.microprofile.reactive.messaging.Outgoing - indicates the populated channel These annotations are used on methods : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package beans ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; @ApplicationScoped public class MessageProcessingBean { @Incoming ( \"consumed-channel\" ) @Outgoing ( \"populated-channel\" ) public Message < String > process ( Message < String > in ) { // Process the payload String payload = in . getPayload (). toUpperCase (); // Create a new message from `in` and just update the payload return in . withPayload ( payload ); } } Note Reactive Messaging beans can either be in the application scope ( @ApplicationScoped ) or dependent scope ( @Dependent ). Manipulating messages can be cumbersome. When you are only interested in the payload, you can use the following syntax: The following code is equivalent to the snippet from above: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package beans ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; @ApplicationScoped public class PayloadProcessingBean { @Incoming ( \"consumed-channel\" ) @Outgoing ( \"populated-channel\" ) public String process ( String in ) { return in . toUpperCase (); } } Important You should not call methods annotated with @Incoming and/or @Outgoing directly from your code. They are invoked by the framework. Having user code invoking them would not have the expected outcome. SmallRye Reactive Messaging automatically binds matching @Outgoing to @Incoming to form a chain: A chain of components If we consider the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Outgoing ( \"source\" ) public Multi < String > generate () { return Multi . createFrom (). items ( \"Hello\" , \"from\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"sink\" ) public String process ( String in ) { return in . toUpperCase (); } @Incoming ( \"sink\" ) public void consume ( String processed ) { System . out . println ( processed ); } It would generate the following chain: 1 generate --> [ source ] --> process --> [ sink ] --> consume Methods annotated with @Incoming or @Outgoing don\u2019t have to be in the same bean ( class ). You can distribute them among a set of beans. Remote interactions are also possible when using connectors. Methods annotated with: only @Outgoing are used to generate messages or payloads only @Incoming are used to consume messages or payloads both @Incoming and @Outgoing are used to process messages or payloads; or transform the stream Creating messages Messages are envelopes around payload. They are the vehicle. While manipulating payload is convenient, messages let you add metadata, handle acknowledgement... Creating Messages is done using the Message interface directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Create a simple message wrapping a payload Message < Price > m1 = Message . of ( price ); // Create a message with metadata Message < Price > m2 = Message . of ( price , Metadata . of ( new PriceMetadata ())); // Create a message with several metadata Message < Price > m3 = Message . of ( price , Metadata . of ( new PriceMetadata (), new MyMetadata ())); // Create a message with an acknowledgement callback Message < Price > m4 = Message . of ( price , () -> { // Called when the message is acknowledged by the next consumer. return CompletableFuture . completedFuture ( null ); }); // Create a message with both metadata and acknowledgement callback Message < Price > m5 = Message . of ( price , Metadata . of ( new PriceMetadata ()), () -> { // Called when the message is acknowledged by the next consumer. return CompletableFuture . completedFuture ( null ); }); You can also create new instance of Message from an existing one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Create a new message with a new payload but with the same metadata Message < Price > m1 = message . withPayload ( new Price ( 12.4 )); // Create a new message with a new payload and add another metadata Message < Price > m2 = message . withPayload ( new Price ( 15.0 )) . withMetadata ( Metadata . of ( new PriceMetadata ())); // Create a new message with a new payload and a custom acknowledgement Message < Price > m3 = message . withPayload ( new Price ( 15.0 )) . withAck (() -> // acknowledge the incoming message message . ack () . thenAccept ( x -> { // do something })); Acknowledgement? Acknowledgement is an important part of messaging systems. This will be covered in the acknowledgement section. Connector Metadata Most connectors are providing metadata to let you extract technical details about the message, but also customize the outbound dispatching. Generating Messages To produce messages to a channel, you need to use the @Outgoing annotation. This annotation takes a single parameter: the name of the populated channel. Generating messages synchronously You can generate messages synchronously. In this case, the method is called for every request from the downstream: 1 2 3 4 @Outgoing ( \"my-channel\" ) public Message < Integer > generateMessagesSynchronously () { return Message . of ( counter . getAndIncrement ()); } Requests? Reactive Messaging connects components to build a reactive stream. In a reactive stream, the emissions are controlled by the consumer (downstream) indicating to the publisher (upstream) how many items it can consume. With this protocol, the consumers are never flooded. Generating messages using CompletionStage You can also return a CompletionStage / CompletableFuture . In this case, Reactive Messaging waits until the CompletionStage gets completed before calling it again. For instance, this signature is useful to poll messages from a source using an asynchronous client: 1 2 3 4 5 @Outgoing ( \"my-channel\" ) public CompletionStage < Message < Price >> generateMessagesAsCompletionStage () { return asyncClient . poll () . thenApply ( Message :: of ); } Generating messages using Uni You can also return a Uni instance. In this case, Reactive Messaging waits until the Uni emits its item before calling it again. This signature is useful when integrating asynchronous clients providing a Mutiny API. 1 2 3 4 @Outgoing ( \"my-channel\" ) public Uni < Message < Integer >> generateMessagesAsync () { return Uni . createFrom (). item (() -> Message . of ( counter . getAndIncrement ())); } Generating Reactive Streams of messages Instead of producing the message one by one, you can return the stream directly. If you have a data source producing Reactive Streams Publisher (or sub-types, such as Multi ), this is the signature you are looking for: 1 2 3 4 public Flow . Publisher < Message < String >> generateMessageStream () { Multi < String > multi = reactiveClient . getStream (); return multi . map ( Message :: of ); } In this case, the method is called once to retrieve the Publisher . Generating Payloads Instead of Message , you can produce payloads . In this case, Reactive Messaging produces a simple message from the payload using Message.of . Generating payload synchronously You can produce payloads synchronously. The framework calls the method upon request and create Messages around the produced payloads. 1 2 3 4 @Outgoing ( \"my-channel\" ) public Integer generatePayloadsSynchronously () { return counter . getAndIncrement (); } Generating payload using CompletionStage You can also return CompletionStage or CompletableFuture . For example, if you have an asynchronous client returning CompletionStage , you can use it as follows, to poll the data one by one: 1 2 3 4 @Outgoing ( \"my-channel\" ) public CompletionStage < Price > generatePayloadsAsCompletionStage () { return asyncClient . poll (); } Generating payload by producing Unis You can also return a Uni if you have a client using Mutiny types: 1 2 3 4 @Outgoing ( \"my-channel\" ) public Uni < Integer > generatePayloadsAsync () { return Uni . createFrom (). item (() -> counter . getAndIncrement ()); } Generating Reactive Streams of payloads Finally, you can return a Publisher (or a sub-type such as a Multi ): 1 2 3 4 5 @Outgoing ( \"my-channel\" ) public Multi < String > generatePayloadsStream () { Multi < String > multi = reactiveClient . getStream (); return multi ; } In this case, Reactive Messaging calls the method only once to retrieve the Publisher . Consuming Messages To consume messages from a channel, you need to use the @Incoming annotation. This annotation takes a single parameter: the name of the consumed channel. Because Messages must be acknowledged, consuming messages requires returning asynchronous results that would complete when the incoming message get acknowledged. For example, you can receive the Message , process it and return the acknowledgement as result: 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public CompletionStage < Void > consumeMessage ( Message < Price > message ) { handle ( message . getPayload ()); return message . ack (); } ' You can also return a Uni if you need to implement more complicated processing: 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public Uni < Void > consumeMessageUni ( Message < Price > message ) { return Uni . createFrom (). item ( message ) . onItem (). invoke ( m -> handle ( m . getPayload ())) . onItem (). transformToUni ( x -> Uni . createFrom (). completionStage ( message . ack ())); } Consuming Payloads Unlike consuming messages, consuming payloads support both synchronous and asynchronous consumption. For example, you can consume a payload as follows: 1 2 3 4 @Incoming ( \"my-channel\" ) public void consumePayload ( Price payload ) { // do something } In this case, you don\u2019t need to deal with the acknowledgement yourself. The framework acknowledges the incoming message (that wrapped the payload) once your method returns successfully. If you need to achieve asynchronous actions, you can return a CompletionStage or a Uni : 1 2 3 4 5 @Incoming ( \"my-channel\" ) public CompletionStage < Void > consumePayloadCS ( Price payload ) { CompletionStage < Void > cs = handleAsync ( payload ); return cs ; } 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public Uni < Void > consumePayloadUni ( Price payload ) { return Uni . createFrom (). item ( payload ) . onItem (). invoke ( this :: handle ) . onItem (). ignore (). andContinueWithNull (); } In these 2 cases, the framework acknowledges the incoming message when the returned construct gets completed . Processing Messages You can process Message both synchronously or asynchronously. This later case is useful when you need to execute an asynchronous action during your processing such as invoking a remote service. Do process Messages synchronously uses: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < String > processMessage ( Message < Integer > in ) { return in . withPayload ( Integer . toString ( in . getPayload ())); } This method transforms the int payload to a String , and wraps it into a Message . '''important \"Using Message.withX methods\" You may be surprised by the usage of Message.withX methods. It allows metadata propagation as the metadata would be copied from the incoming message and so dispatched to the next method. You can also process Messages asynchronously: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public CompletionStage < Message < String >> processMessageCS ( Message < Integer > in ) { CompletionStage < String > cs = invokeService ( in . getPayload ()); return cs . thenApply ( in :: withPayload ); } Or using Mutiny: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < Message < String >> processMessageUni ( Message < String > in ) { return invokeService ( in . getPayload ()) . map ( in :: withPayload ); } In general, you want to create the new Message from the incoming one. It enables metadata propagation and post-acknowledgement. For this, use the withX method from the Message class returning a new Message instance but copy the content (metadata, ack/nack...). Processing payloads If you don\u2019t need to manipulate the envelope, you can process payload directly either synchronously or asynchronously: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public String processPayload ( int in ) { return Integer . toString ( in ); } 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public CompletionStage < String > processPayloadCS ( int in ) { return invokeService ( in ); } 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < String > processPayload ( String in ) { return invokeService ( in ); } What about metadata? With these methods, the metadata are automatically propagated. Processing streams The previous processing method were taking single Message or payload. Sometimes you need more advanced manipulation. For this, SmallRye Reactive Messaging lets you process the stream of Message or the stream of payloads directly: 1 2 3 4 5 6 7 8 9 10 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Multi < Message < String >> processMessageStream ( Multi < Message < Integer >> stream ) { return stream . onItem (). transformToUni ( message -> invokeService ( message . getPayload ()) . onFailure (). recoverWithItem ( \"fallback\" ) . onItem (). transform ( message :: withPayload )) . concatenate (); } 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Multi < String > processPayloadStream ( Multi < Integer > stream ) { return stream . onItem (). transformToUni ( payload -> invokeService ( payload ) . onFailure (). recoverWithItem ( \"fallback\" )) . concatenate (); } You can receive either a (Reactive Streams) Publisher , a PublisherBuilder or (Mutiny) Multi . You can return any subclass of Publisher or a Publisher directly. Important These signatures do not support metadata propagation. In the case of a stream of Message , you need to propagate the metadata manually. In the case of a stream of payload, propagation is not supported, and incoming metadata are lost.","title":"Development Model"},{"location":"concepts/model/#development-model","text":"Reactive Messaging proposes a CDI-based programming model to implement event-driven applications. Following the CDI principles, beans are forming the main building block of your application. Reactive Messaging provides a set of annotations and types to implement beans that generate, consume or process messages.","title":"Development Model"},{"location":"concepts/model/#incoming-and-outgoing","text":"Reactive Messaging provides two main annotations: org.eclipse.microprofile.reactive.messaging.Incoming - indicates the consumed channel org.eclipse.microprofile.reactive.messaging.Outgoing - indicates the populated channel These annotations are used on methods : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package beans ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; @ApplicationScoped public class MessageProcessingBean { @Incoming ( \"consumed-channel\" ) @Outgoing ( \"populated-channel\" ) public Message < String > process ( Message < String > in ) { // Process the payload String payload = in . getPayload (). toUpperCase (); // Create a new message from `in` and just update the payload return in . withPayload ( payload ); } } Note Reactive Messaging beans can either be in the application scope ( @ApplicationScoped ) or dependent scope ( @Dependent ). Manipulating messages can be cumbersome. When you are only interested in the payload, you can use the following syntax: The following code is equivalent to the snippet from above: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package beans ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; @ApplicationScoped public class PayloadProcessingBean { @Incoming ( \"consumed-channel\" ) @Outgoing ( \"populated-channel\" ) public String process ( String in ) { return in . toUpperCase (); } } Important You should not call methods annotated with @Incoming and/or @Outgoing directly from your code. They are invoked by the framework. Having user code invoking them would not have the expected outcome. SmallRye Reactive Messaging automatically binds matching @Outgoing to @Incoming to form a chain: A chain of components If we consider the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Outgoing ( \"source\" ) public Multi < String > generate () { return Multi . createFrom (). items ( \"Hello\" , \"from\" , \"reactive\" , \"messaging\" ); } @Incoming ( \"source\" ) @Outgoing ( \"sink\" ) public String process ( String in ) { return in . toUpperCase (); } @Incoming ( \"sink\" ) public void consume ( String processed ) { System . out . println ( processed ); } It would generate the following chain: 1 generate --> [ source ] --> process --> [ sink ] --> consume Methods annotated with @Incoming or @Outgoing don\u2019t have to be in the same bean ( class ). You can distribute them among a set of beans. Remote interactions are also possible when using connectors. Methods annotated with: only @Outgoing are used to generate messages or payloads only @Incoming are used to consume messages or payloads both @Incoming and @Outgoing are used to process messages or payloads; or transform the stream","title":"@Incoming and @Outgoing"},{"location":"concepts/model/#creating-messages","text":"Messages are envelopes around payload. They are the vehicle. While manipulating payload is convenient, messages let you add metadata, handle acknowledgement... Creating Messages is done using the Message interface directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Create a simple message wrapping a payload Message < Price > m1 = Message . of ( price ); // Create a message with metadata Message < Price > m2 = Message . of ( price , Metadata . of ( new PriceMetadata ())); // Create a message with several metadata Message < Price > m3 = Message . of ( price , Metadata . of ( new PriceMetadata (), new MyMetadata ())); // Create a message with an acknowledgement callback Message < Price > m4 = Message . of ( price , () -> { // Called when the message is acknowledged by the next consumer. return CompletableFuture . completedFuture ( null ); }); // Create a message with both metadata and acknowledgement callback Message < Price > m5 = Message . of ( price , Metadata . of ( new PriceMetadata ()), () -> { // Called when the message is acknowledged by the next consumer. return CompletableFuture . completedFuture ( null ); }); You can also create new instance of Message from an existing one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Create a new message with a new payload but with the same metadata Message < Price > m1 = message . withPayload ( new Price ( 12.4 )); // Create a new message with a new payload and add another metadata Message < Price > m2 = message . withPayload ( new Price ( 15.0 )) . withMetadata ( Metadata . of ( new PriceMetadata ())); // Create a new message with a new payload and a custom acknowledgement Message < Price > m3 = message . withPayload ( new Price ( 15.0 )) . withAck (() -> // acknowledge the incoming message message . ack () . thenAccept ( x -> { // do something })); Acknowledgement? Acknowledgement is an important part of messaging systems. This will be covered in the acknowledgement section. Connector Metadata Most connectors are providing metadata to let you extract technical details about the message, but also customize the outbound dispatching.","title":"Creating messages"},{"location":"concepts/model/#generating-messages","text":"To produce messages to a channel, you need to use the @Outgoing annotation. This annotation takes a single parameter: the name of the populated channel.","title":"Generating Messages"},{"location":"concepts/model/#generating-messages-synchronously","text":"You can generate messages synchronously. In this case, the method is called for every request from the downstream: 1 2 3 4 @Outgoing ( \"my-channel\" ) public Message < Integer > generateMessagesSynchronously () { return Message . of ( counter . getAndIncrement ()); } Requests? Reactive Messaging connects components to build a reactive stream. In a reactive stream, the emissions are controlled by the consumer (downstream) indicating to the publisher (upstream) how many items it can consume. With this protocol, the consumers are never flooded.","title":"Generating messages synchronously"},{"location":"concepts/model/#generating-messages-using-completionstage","text":"You can also return a CompletionStage / CompletableFuture . In this case, Reactive Messaging waits until the CompletionStage gets completed before calling it again. For instance, this signature is useful to poll messages from a source using an asynchronous client: 1 2 3 4 5 @Outgoing ( \"my-channel\" ) public CompletionStage < Message < Price >> generateMessagesAsCompletionStage () { return asyncClient . poll () . thenApply ( Message :: of ); }","title":"Generating messages using CompletionStage"},{"location":"concepts/model/#generating-messages-using-uni","text":"You can also return a Uni instance. In this case, Reactive Messaging waits until the Uni emits its item before calling it again. This signature is useful when integrating asynchronous clients providing a Mutiny API. 1 2 3 4 @Outgoing ( \"my-channel\" ) public Uni < Message < Integer >> generateMessagesAsync () { return Uni . createFrom (). item (() -> Message . of ( counter . getAndIncrement ())); }","title":"Generating messages using Uni"},{"location":"concepts/model/#generating-reactive-streams-of-messages","text":"Instead of producing the message one by one, you can return the stream directly. If you have a data source producing Reactive Streams Publisher (or sub-types, such as Multi ), this is the signature you are looking for: 1 2 3 4 public Flow . Publisher < Message < String >> generateMessageStream () { Multi < String > multi = reactiveClient . getStream (); return multi . map ( Message :: of ); } In this case, the method is called once to retrieve the Publisher .","title":"Generating Reactive Streams of messages"},{"location":"concepts/model/#generating-payloads","text":"Instead of Message , you can produce payloads . In this case, Reactive Messaging produces a simple message from the payload using Message.of .","title":"Generating Payloads"},{"location":"concepts/model/#generating-payload-synchronously","text":"You can produce payloads synchronously. The framework calls the method upon request and create Messages around the produced payloads. 1 2 3 4 @Outgoing ( \"my-channel\" ) public Integer generatePayloadsSynchronously () { return counter . getAndIncrement (); }","title":"Generating payload synchronously"},{"location":"concepts/model/#generating-payload-using-completionstage","text":"You can also return CompletionStage or CompletableFuture . For example, if you have an asynchronous client returning CompletionStage , you can use it as follows, to poll the data one by one: 1 2 3 4 @Outgoing ( \"my-channel\" ) public CompletionStage < Price > generatePayloadsAsCompletionStage () { return asyncClient . poll (); }","title":"Generating payload using CompletionStage"},{"location":"concepts/model/#generating-payload-by-producing-unis","text":"You can also return a Uni if you have a client using Mutiny types: 1 2 3 4 @Outgoing ( \"my-channel\" ) public Uni < Integer > generatePayloadsAsync () { return Uni . createFrom (). item (() -> counter . getAndIncrement ()); }","title":"Generating payload by producing Unis"},{"location":"concepts/model/#generating-reactive-streams-of-payloads","text":"Finally, you can return a Publisher (or a sub-type such as a Multi ): 1 2 3 4 5 @Outgoing ( \"my-channel\" ) public Multi < String > generatePayloadsStream () { Multi < String > multi = reactiveClient . getStream (); return multi ; } In this case, Reactive Messaging calls the method only once to retrieve the Publisher .","title":"Generating Reactive Streams of payloads"},{"location":"concepts/model/#consuming-messages","text":"To consume messages from a channel, you need to use the @Incoming annotation. This annotation takes a single parameter: the name of the consumed channel. Because Messages must be acknowledged, consuming messages requires returning asynchronous results that would complete when the incoming message get acknowledged. For example, you can receive the Message , process it and return the acknowledgement as result: 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public CompletionStage < Void > consumeMessage ( Message < Price > message ) { handle ( message . getPayload ()); return message . ack (); } ' You can also return a Uni if you need to implement more complicated processing: 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public Uni < Void > consumeMessageUni ( Message < Price > message ) { return Uni . createFrom (). item ( message ) . onItem (). invoke ( m -> handle ( m . getPayload ())) . onItem (). transformToUni ( x -> Uni . createFrom (). completionStage ( message . ack ())); }","title":"Consuming Messages"},{"location":"concepts/model/#consuming-payloads","text":"Unlike consuming messages, consuming payloads support both synchronous and asynchronous consumption. For example, you can consume a payload as follows: 1 2 3 4 @Incoming ( \"my-channel\" ) public void consumePayload ( Price payload ) { // do something } In this case, you don\u2019t need to deal with the acknowledgement yourself. The framework acknowledges the incoming message (that wrapped the payload) once your method returns successfully. If you need to achieve asynchronous actions, you can return a CompletionStage or a Uni : 1 2 3 4 5 @Incoming ( \"my-channel\" ) public CompletionStage < Void > consumePayloadCS ( Price payload ) { CompletionStage < Void > cs = handleAsync ( payload ); return cs ; } 1 2 3 4 5 6 @Incoming ( \"my-channel\" ) public Uni < Void > consumePayloadUni ( Price payload ) { return Uni . createFrom (). item ( payload ) . onItem (). invoke ( this :: handle ) . onItem (). ignore (). andContinueWithNull (); } In these 2 cases, the framework acknowledges the incoming message when the returned construct gets completed .","title":"Consuming Payloads"},{"location":"concepts/model/#processing-messages","text":"You can process Message both synchronously or asynchronously. This later case is useful when you need to execute an asynchronous action during your processing such as invoking a remote service. Do process Messages synchronously uses: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < String > processMessage ( Message < Integer > in ) { return in . withPayload ( Integer . toString ( in . getPayload ())); } This method transforms the int payload to a String , and wraps it into a Message . '''important \"Using Message.withX methods\" You may be surprised by the usage of Message.withX methods. It allows metadata propagation as the metadata would be copied from the incoming message and so dispatched to the next method. You can also process Messages asynchronously: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public CompletionStage < Message < String >> processMessageCS ( Message < Integer > in ) { CompletionStage < String > cs = invokeService ( in . getPayload ()); return cs . thenApply ( in :: withPayload ); } Or using Mutiny: 1 2 3 4 5 6 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < Message < String >> processMessageUni ( Message < String > in ) { return invokeService ( in . getPayload ()) . map ( in :: withPayload ); } In general, you want to create the new Message from the incoming one. It enables metadata propagation and post-acknowledgement. For this, use the withX method from the Message class returning a new Message instance but copy the content (metadata, ack/nack...).","title":"Processing Messages"},{"location":"concepts/model/#processing-payloads","text":"If you don\u2019t need to manipulate the envelope, you can process payload directly either synchronously or asynchronously: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public String processPayload ( int in ) { return Integer . toString ( in ); } 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public CompletionStage < String > processPayloadCS ( int in ) { return invokeService ( in ); } 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < String > processPayload ( String in ) { return invokeService ( in ); } What about metadata? With these methods, the metadata are automatically propagated.","title":"Processing payloads"},{"location":"concepts/model/#processing-streams","text":"The previous processing method were taking single Message or payload. Sometimes you need more advanced manipulation. For this, SmallRye Reactive Messaging lets you process the stream of Message or the stream of payloads directly: 1 2 3 4 5 6 7 8 9 10 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Multi < Message < String >> processMessageStream ( Multi < Message < Integer >> stream ) { return stream . onItem (). transformToUni ( message -> invokeService ( message . getPayload ()) . onFailure (). recoverWithItem ( \"fallback\" ) . onItem (). transform ( message :: withPayload )) . concatenate (); } 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Multi < String > processPayloadStream ( Multi < Integer > stream ) { return stream . onItem (). transformToUni ( payload -> invokeService ( payload ) . onFailure (). recoverWithItem ( \"fallback\" )) . concatenate (); } You can receive either a (Reactive Streams) Publisher , a PublisherBuilder or (Mutiny) Multi . You can return any subclass of Publisher or a Publisher directly. Important These signatures do not support metadata propagation. In the case of a stream of Message , you need to propagate the metadata manually. In the case of a stream of payload, propagation is not supported, and incoming metadata are lost.","title":"Processing streams"},{"location":"concepts/signatures/","text":"Supported signatures The following tables list the supported method signatures and indicate the various supported features. For instance, they indicate the default and available acknowledgement strategies (when applicable). Method signatures to generate data Signature Invocation time @Outgoing Publisher<Message<O>> method() ` Called once at assembly time @Outgoing Publisher<O> method() ` Called once at assembly time @Outgoing Multi<Message<O>> method() ` Called once at assembly time @Outgoing Multi<O> method() ` Called once at assembly time @Outgoing Flow.Publisher<Message<O>> method() ` Called once at assembly time @Outgoing Flow.Publisher<O> method() ` Called once at assembly time @Outgoing PublisherBuilder<Message<O>> method() ` Called once at assembly time @Outgoing PublisherBuilder<O> method() ` Called once at assembly time @Outgoing Message<O> method() ` Called for every downstream request, sequentially @Outgoing O method() ` Called for every downstream request, sequentially @Outgoing CompletionStage<Message<O>> method() ` Called for every downstream request, sequentially (After the completion of the last returned CompletionStage) @Outgoing CompletionStage<O> method() ` Called for every downstream request, , sequentially (After the completion of the last returned CompletionStage) @Outgoing Uni<Message<O>> method() ` Called for every downstream request, sequentially (After the completion of the last returned Uni) @Outgoing Uni<O> method() ` Called for every downstream request, , sequentially (After the completion of the last returned Uni) Method signatures to consume data Signature Invocation time Supported Acknowledgement Strategies @Incoming void method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING @Incoming CompletionStage<?> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING @Incoming CompletionStage<?> method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , PRE_PROCESSING, NONE @Incoming Uni<?> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING @Incoming Uni<?> method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , PRE_PROCESSING, NONE @Incoming Subscriber<Message<I>> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming Subscriber<I> method() Called once at assembly time POST_PROCESSING , NONE, PRE_PROCESSING @Incoming Flow.Subscriber<Message<I>> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming Flow.Subscriber<I> method() Called once at assembly time POST_PROCESSING , NONE, PRE_PROCESSING @Incoming SubscriberBuilder<Message<I>, ?> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming SubscriberBuilder<I, ?> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING Method signatures to process data Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation @Outgoing @Incoming Message<O> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming O method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming CompletionStage<Message<O>> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming CompletionStage<O> method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming Uni<Message<O>> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Uni<O> method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming Processor<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Processor<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Flow.Processor<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Flow.Processor<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming ProcessorBuilder<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming ProcessorBuilder<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Publisher<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Publisher<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming Multi<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Multi<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming Flow.Publisher<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Flow.Publisher<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming PublisherBuilder<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming PublisherBuilder<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic Method signatures to manipulate streams Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation @Outgoing @Incoming Publisher<Message<O>> method(Publisher<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Publisher<O> method(Publisher<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Multi<Message<O>> method(Multi<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Multi<O> method(Multi<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Flow.Publisher<Message<O>> method(Flow.Publisher<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Flow.Publisher<O> method(Flow.Publisher<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming PublisherBuilder<Message<O>> method(PublisherBuilder<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming PublisherBuilder<O> method(PublisherBuilder<I> pub) Called once at assembly time NONE, PRE_PROCESSING not supported Important When processing Message , it is often required to chain the incoming Message to enable post-processing acknowledgement and metadata propagation. Use the with (like withPayload ) methods from the incoming message, so it copies the metadata and ack/nack methods. It returns a new Message with the right content.","title":"Method Signatures"},{"location":"concepts/signatures/#supported-signatures","text":"The following tables list the supported method signatures and indicate the various supported features. For instance, they indicate the default and available acknowledgement strategies (when applicable).","title":"Supported signatures"},{"location":"concepts/signatures/#method-signatures-to-generate-data","text":"Signature Invocation time @Outgoing Publisher<Message<O>> method() ` Called once at assembly time @Outgoing Publisher<O> method() ` Called once at assembly time @Outgoing Multi<Message<O>> method() ` Called once at assembly time @Outgoing Multi<O> method() ` Called once at assembly time @Outgoing Flow.Publisher<Message<O>> method() ` Called once at assembly time @Outgoing Flow.Publisher<O> method() ` Called once at assembly time @Outgoing PublisherBuilder<Message<O>> method() ` Called once at assembly time @Outgoing PublisherBuilder<O> method() ` Called once at assembly time @Outgoing Message<O> method() ` Called for every downstream request, sequentially @Outgoing O method() ` Called for every downstream request, sequentially @Outgoing CompletionStage<Message<O>> method() ` Called for every downstream request, sequentially (After the completion of the last returned CompletionStage) @Outgoing CompletionStage<O> method() ` Called for every downstream request, , sequentially (After the completion of the last returned CompletionStage) @Outgoing Uni<Message<O>> method() ` Called for every downstream request, sequentially (After the completion of the last returned Uni) @Outgoing Uni<O> method() ` Called for every downstream request, , sequentially (After the completion of the last returned Uni)","title":"Method signatures to generate data"},{"location":"concepts/signatures/#method-signatures-to-consume-data","text":"Signature Invocation time Supported Acknowledgement Strategies @Incoming void method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING @Incoming CompletionStage<?> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING @Incoming CompletionStage<?> method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , PRE_PROCESSING, NONE @Incoming Uni<?> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING @Incoming Uni<?> method(I p) Called for every incoming payload (sequentially) POST_PROCESSING , PRE_PROCESSING, NONE @Incoming Subscriber<Message<I>> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming Subscriber<I> method() Called once at assembly time POST_PROCESSING , NONE, PRE_PROCESSING @Incoming Flow.Subscriber<Message<I>> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming Flow.Subscriber<I> method() Called once at assembly time POST_PROCESSING , NONE, PRE_PROCESSING @Incoming SubscriberBuilder<Message<I>, ?> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING @Incoming SubscriberBuilder<I, ?> method() Called once at assembly time MANUAL , POST_PROCESSING, NONE, PRE_PROCESSING","title":"Method signatures to consume data"},{"location":"concepts/signatures/#method-signatures-to-process-data","text":"Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation @Outgoing @Incoming Message<O> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming O method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming CompletionStage<Message<O>> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming CompletionStage<O> method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming Uni<Message<O>> method(Message<I> msg) Called for every incoming message (sequentially) MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Uni<O> method(I payload) Called for every incoming payload (sequentially) POST_PROCESSING , NONE, PRE_PROCESSING automatic @Outgoing @Incoming Processor<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Processor<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Flow.Processor<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Flow.Processor<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming ProcessorBuilder<Message<I>, Message<O>> method() Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming ProcessorBuilder<I, O> method() Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Publisher<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Publisher<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming Multi<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Multi<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming Flow.Publisher<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming Flow.Publisher<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic @Outgoing @Incoming PublisherBuilder<Message<O>> method(Message<I> msg) Called once at assembly time MANUAL , PRE_PROCESSING, NONE manual @Outgoing @Incoming PublisherBuilder<O> method(I payload) Called once at assembly time PRE_PROCESSING , NONE automatic","title":"Method signatures to process data"},{"location":"concepts/signatures/#method-signatures-to-manipulate-streams","text":"Signature Invocation time Supported Acknowledgement Strategies Metadata Propagation @Outgoing @Incoming Publisher<Message<O>> method(Publisher<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Publisher<O> method(Publisher<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Multi<Message<O>> method(Multi<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Multi<O> method(Multi<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming Flow.Publisher<Message<O>> method(Flow.Publisher<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming Flow.Publisher<O> method(Flow.Publisher<I> pub) Called once at assembly time PRE_PROCESSING , NONE not supported @Outgoing @Incoming PublisherBuilder<Message<O>> method(PublisherBuilder<Message<I>> pub) Called once at assembly time MANUAL , NONE, PRE_PROCESSING manual @Outgoing @Incoming PublisherBuilder<O> method(PublisherBuilder<I> pub) Called once at assembly time NONE, PRE_PROCESSING not supported Important When processing Message , it is often required to chain the incoming Message to enable post-processing acknowledgement and metadata propagation. Use the with (like withPayload ) methods from the incoming message, so it copies the metadata and ack/nack methods. It returns a new Message with the right content.","title":"Method signatures to manipulate streams"},{"location":"concepts/skipping/","text":"Skipping messages Sometimes you receive a message and don\u2019t want to produce an output message. To handle this, you have several choices: for method processing single message or payload, producing null would produce an ignored message (not forwarded) for method processing streams, you can generate an empty stream. Skipping a single item To skip a single message or payload, return null : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // Skip when processing payload synchronously - returning `null` @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public String processPayload ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { return null ; } return s . toUpperCase (); } // Skip when processing message synchronously - returning `null` @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < String > processMessage ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { m . ack (); return null ; } return m . withPayload ( s . toUpperCase ()); } // Skip when processing payload asynchronously - returning a `Uni` with a `null` value @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < String > processPayloadAsync ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { // Important, you must not return `null`, but a `null` content return Uni . createFrom (). nullItem (); } return Uni . createFrom (). item ( s . toUpperCase ()); } // Skip when processing message asynchronously - returning a `Uni` with a `null` value @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < Message < String >> processMessageAsync ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { m . ack (); return Uni . createFrom (). nullItem (); } return Uni . createFrom (). item ( m . withPayload ( s . toUpperCase ())); } Skipping in a stream To skip a message or payload when manipulating a stream, emit an empty Multi (or Publisher ): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Incoming ( \"in\" ) @Outgoing ( \"out-1\" ) public Multi < String > processPayload ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { return Multi . createFrom (). empty (); } return Multi . createFrom (). item ( s . toUpperCase ()); } @Incoming ( \"in\" ) @Outgoing ( \"out-2\" ) public Multi < Message < String >> processMessage ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { return Multi . createFrom (). empty (); } return Multi . createFrom (). item ( m . withPayload ( s . toUpperCase ())); } @Incoming ( \"in\" ) @Outgoing ( \"out-3\" ) public Multi < String > processPayloadStream ( Multi < String > stream ) { return stream . select (). where ( s -> ! s . equalsIgnoreCase ( \"skip\" )) . onItem (). transform ( String :: toUpperCase ); } @Incoming ( \"in\" ) @Outgoing ( \"out-4\" ) public Multi < Message < String >> processMessageStream ( Multi < Message < String >> stream ) { return stream . select (). where ( m -> ! m . getPayload (). equalsIgnoreCase ( \"skip\" )) . onItem (). transform ( m -> m . withPayload ( m . getPayload (). toUpperCase ())); }","title":"Skipping Messages"},{"location":"concepts/skipping/#skipping-messages","text":"Sometimes you receive a message and don\u2019t want to produce an output message. To handle this, you have several choices: for method processing single message or payload, producing null would produce an ignored message (not forwarded) for method processing streams, you can generate an empty stream.","title":"Skipping messages"},{"location":"concepts/skipping/#skipping-a-single-item","text":"To skip a single message or payload, return null : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // Skip when processing payload synchronously - returning `null` @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public String processPayload ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { return null ; } return s . toUpperCase (); } // Skip when processing message synchronously - returning `null` @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Message < String > processMessage ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { m . ack (); return null ; } return m . withPayload ( s . toUpperCase ()); } // Skip when processing payload asynchronously - returning a `Uni` with a `null` value @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < String > processPayloadAsync ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { // Important, you must not return `null`, but a `null` content return Uni . createFrom (). nullItem (); } return Uni . createFrom (). item ( s . toUpperCase ()); } // Skip when processing message asynchronously - returning a `Uni` with a `null` value @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Uni < Message < String >> processMessageAsync ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { m . ack (); return Uni . createFrom (). nullItem (); } return Uni . createFrom (). item ( m . withPayload ( s . toUpperCase ())); }","title":"Skipping a single item"},{"location":"concepts/skipping/#skipping-in-a-stream","text":"To skip a message or payload when manipulating a stream, emit an empty Multi (or Publisher ): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Incoming ( \"in\" ) @Outgoing ( \"out-1\" ) public Multi < String > processPayload ( String s ) { if ( s . equalsIgnoreCase ( \"skip\" )) { return Multi . createFrom (). empty (); } return Multi . createFrom (). item ( s . toUpperCase ()); } @Incoming ( \"in\" ) @Outgoing ( \"out-2\" ) public Multi < Message < String >> processMessage ( Message < String > m ) { String s = m . getPayload (); if ( s . equalsIgnoreCase ( \"skip\" )) { return Multi . createFrom (). empty (); } return Multi . createFrom (). item ( m . withPayload ( s . toUpperCase ())); } @Incoming ( \"in\" ) @Outgoing ( \"out-3\" ) public Multi < String > processPayloadStream ( Multi < String > stream ) { return stream . select (). where ( s -> ! s . equalsIgnoreCase ( \"skip\" )) . onItem (). transform ( String :: toUpperCase ); } @Incoming ( \"in\" ) @Outgoing ( \"out-4\" ) public Multi < Message < String >> processMessageStream ( Multi < Message < String >> stream ) { return stream . select (). where ( m -> ! m . getPayload (). equalsIgnoreCase ( \"skip\" )) . onItem (). transform ( m -> m . withPayload ( m . getPayload (). toUpperCase ())); }","title":"Skipping in a stream"},{"location":"concepts/testing/","text":"Testing your application It\u2019s not rare to have to test your application but deploying the infrastructure can be cumbersome. While Docker or Test Containers have improved the testing experience, you may want to mock this infrastructure. SmallRye Reactive Messaging proposes an in-memory connector for this exact purpose. It allows switching the connector used for a channel with an in-memory connector. This in-memory connector provides a way to send messages to incoming channels, or check the received messages for outgoing channels. To use the in-memory connector, you need to add the following dependency to your project: 1 2 3 4 5 6 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-in-memory </artifactId> <version> 4.3.0 </version> <scope> test </scope> </dependency> Then, in a test, you can do something like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package testing ; import jakarta.enterprise.inject.Any ; import jakarta.inject.Inject ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.smallrye.reactive.messaging.memory.InMemoryConnector ; import io.smallrye.reactive.messaging.memory.InMemorySink ; import io.smallrye.reactive.messaging.memory.InMemorySource ; public class MyTest { // 1. Switch the channels to the in-memory connector: @BeforeAll public static void switchMyChannels () { InMemoryConnector . switchIncomingChannelsToInMemory ( \"prices\" ); InMemoryConnector . switchOutgoingChannelsToInMemory ( \"processed-prices\" ); } // 2. Don't forget to reset the channel after the tests: @AfterAll public static void revertMyChannels () { InMemoryConnector . clear (); } // 3. Inject the in-memory connector in your test, // or use the bean manager to retrieve the instance @Inject @Any InMemoryConnector connector ; @Test void test () { // 4. Retrieves the in-memory source to send message InMemorySource < Integer > prices = connector . source ( \"prices\" ); // 5. Retrieves the in-memory sink to check what is received InMemorySink < Integer > results = connector . sink ( \"processed-prices\" ); // 6. Send fake messages: prices . send ( 1 ); prices . send ( 2 ); prices . send ( 3 ); // 7. Check you have received the expected messages Assertions . assertEquals ( 3 , results . received (). size ()); } } When switching a channel to the in-memory connector, all the configuration properties are ignored. Important This connector has been designed for testing purpose only. The switch methods return Map<String, String> instances containing the set properties. While these system properties are already set, you can retrieve them and pass them around, for example if you need to start an external process with these properties: 1 2 3 4 5 6 7 8 9 10 public Map < String , String > start () { Map < String , String > env = new HashMap <> (); env . putAll ( InMemoryConnector . switchIncomingChannelsToInMemory ( \"prices\" )); env . putAll ( InMemoryConnector . switchOutgoingChannelsToInMemory ( \"my-data-stream\" )); return env ; } public void stop () { InMemoryConnector . clear (); } Note The in-memory connector support the broadcast and merge attributes. So, if your connector is configured with broadcast: true , the connector broadcasts the messages to all the channel consumers. If your connector is configured with merge:true , the connector receives all the messages sent to the mapped channel even when coming from multiple producers.","title":"Testing"},{"location":"concepts/testing/#testing-your-application","text":"It\u2019s not rare to have to test your application but deploying the infrastructure can be cumbersome. While Docker or Test Containers have improved the testing experience, you may want to mock this infrastructure. SmallRye Reactive Messaging proposes an in-memory connector for this exact purpose. It allows switching the connector used for a channel with an in-memory connector. This in-memory connector provides a way to send messages to incoming channels, or check the received messages for outgoing channels. To use the in-memory connector, you need to add the following dependency to your project: 1 2 3 4 5 6 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-in-memory </artifactId> <version> 4.3.0 </version> <scope> test </scope> </dependency> Then, in a test, you can do something like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package testing ; import jakarta.enterprise.inject.Any ; import jakarta.inject.Inject ; import org.junit.jupiter.api.AfterAll ; import org.junit.jupiter.api.Assertions ; import org.junit.jupiter.api.BeforeAll ; import org.junit.jupiter.api.Test ; import io.smallrye.reactive.messaging.memory.InMemoryConnector ; import io.smallrye.reactive.messaging.memory.InMemorySink ; import io.smallrye.reactive.messaging.memory.InMemorySource ; public class MyTest { // 1. Switch the channels to the in-memory connector: @BeforeAll public static void switchMyChannels () { InMemoryConnector . switchIncomingChannelsToInMemory ( \"prices\" ); InMemoryConnector . switchOutgoingChannelsToInMemory ( \"processed-prices\" ); } // 2. Don't forget to reset the channel after the tests: @AfterAll public static void revertMyChannels () { InMemoryConnector . clear (); } // 3. Inject the in-memory connector in your test, // or use the bean manager to retrieve the instance @Inject @Any InMemoryConnector connector ; @Test void test () { // 4. Retrieves the in-memory source to send message InMemorySource < Integer > prices = connector . source ( \"prices\" ); // 5. Retrieves the in-memory sink to check what is received InMemorySink < Integer > results = connector . sink ( \"processed-prices\" ); // 6. Send fake messages: prices . send ( 1 ); prices . send ( 2 ); prices . send ( 3 ); // 7. Check you have received the expected messages Assertions . assertEquals ( 3 , results . received (). size ()); } } When switching a channel to the in-memory connector, all the configuration properties are ignored. Important This connector has been designed for testing purpose only. The switch methods return Map<String, String> instances containing the set properties. While these system properties are already set, you can retrieve them and pass them around, for example if you need to start an external process with these properties: 1 2 3 4 5 6 7 8 9 10 public Map < String , String > start () { Map < String , String > env = new HashMap <> (); env . putAll ( InMemoryConnector . switchIncomingChannelsToInMemory ( \"prices\" )); env . putAll ( InMemoryConnector . switchOutgoingChannelsToInMemory ( \"my-data-stream\" )); return env ; } public void stop () { InMemoryConnector . clear (); } Note The in-memory connector support the broadcast and merge attributes. So, if your connector is configured with broadcast: true , the connector broadcasts the messages to all the channel consumers. If your connector is configured with merge:true , the connector receives all the messages sent to the mapped channel even when coming from multiple producers.","title":"Testing your application"},{"location":"jms/advanced-jms/","text":"Advanced configuration Underlying thread pool Lots of JMS operations are blocking and so not cannot be done on the caller thread. For this reason, these blocking operations are executed on a worker thread. You can configure the thread pool providing these worker threads using the following MicroProfile Config properties: smallrye.jms.threads.max-pool-size - the max number of threads (Defaults to 10) smallrye.jms.threads.ttl - the ttl of the created threads (Defaults to 60 seconds) Selecting the ConnectionFactory The JMS Connector requires a jakarta.jms.ConnectionFactory to be exposed as a CDI bean. The connector looks for a jakarta.jms.ConnectionFactory and delegate the interaction with the JMS server to this factory. In case you have several connection factories, you can use the @Identifier qualifier on your factory to specify the name. Then, in the channel configuration, configure the name as follows: 1 2 3 4 5 6 # Configure the connector globally mp.messaging.connector.smallrye-jms.connection-factory-name = my-factory-name # Configure a specific incoming channel mp.messaging.incoming.my-channel.connection-factory-name = my-factory-name # Configure a specific outgoing channel mp.messaging.outgoing.my-channel.connection-factory-name = my-factory-name","title":"Advanced configuration"},{"location":"jms/advanced-jms/#advanced-configuration","text":"","title":"Advanced configuration"},{"location":"jms/advanced-jms/#underlying-thread-pool","text":"Lots of JMS operations are blocking and so not cannot be done on the caller thread. For this reason, these blocking operations are executed on a worker thread. You can configure the thread pool providing these worker threads using the following MicroProfile Config properties: smallrye.jms.threads.max-pool-size - the max number of threads (Defaults to 10) smallrye.jms.threads.ttl - the ttl of the created threads (Defaults to 60 seconds)","title":"Underlying thread pool"},{"location":"jms/advanced-jms/#selecting-the-connectionfactory","text":"The JMS Connector requires a jakarta.jms.ConnectionFactory to be exposed as a CDI bean. The connector looks for a jakarta.jms.ConnectionFactory and delegate the interaction with the JMS server to this factory. In case you have several connection factories, you can use the @Identifier qualifier on your factory to specify the name. Then, in the channel configuration, configure the name as follows: 1 2 3 4 5 6 # Configure the connector globally mp.messaging.connector.smallrye-jms.connection-factory-name = my-factory-name # Configure a specific incoming channel mp.messaging.incoming.my-channel.connection-factory-name = my-factory-name # Configure a specific outgoing channel mp.messaging.outgoing.my-channel.connection-factory-name = my-factory-name","title":"Selecting the ConnectionFactory"},{"location":"jms/jms/","text":"The JMS connector The JMS connector adds support for Jakarta Messaging to Reactive Messaging. It is designed to integrate with JakartaEE applications that are sending or receiving Jakarta Messaging Messages. Jakarta Messaging is a Java Message Oriented Middleware API for sending messages between two or more clients. It is a programming model to handle the producer-consumer messaging problem. It is a messaging standard that allows application components based on Jakarta EE to create, send, receive, and read messages. It allows the communication between different components of a distributed application to be loosely coupled, reliable, and asynchronous. Using the JMS connector To you the JMS Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-jms </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-jms . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-jms # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-jms The JMS Connector requires a jakarta.jms.ConnectionFactory to be exposed (as CDI bean). The connector looks for a jakarta.jms.ConnectionFactory and delegate the interaction with the JMS server to this factory. In other words, it creates the JMS connection and context using this factory. So, in order to use this connector you would need to expose a jakarta.jms.ConnectionFactory : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import jakarta.enterprise.context.ApplicationScoped ; import jakarta.enterprise.inject.Produces ; import jakarta.jms.ConnectionFactory ; import org.apache.activemq.artemis.jms.client.ActiveMQJMSConnectionFactory ; @ApplicationScoped public class ConnectionFactoryBean { @Produces ConnectionFactory factory () { return new ActiveMQJMSConnectionFactory ( \"tcp://localhost:61616\" , null , null ); } } The factory class may depend on your JMS connector/server.","title":"The JMS connector"},{"location":"jms/jms/#the-jms-connector","text":"The JMS connector adds support for Jakarta Messaging to Reactive Messaging. It is designed to integrate with JakartaEE applications that are sending or receiving Jakarta Messaging Messages. Jakarta Messaging is a Java Message Oriented Middleware API for sending messages between two or more clients. It is a programming model to handle the producer-consumer messaging problem. It is a messaging standard that allows application components based on Jakarta EE to create, send, receive, and read messages. It allows the communication between different components of a distributed application to be loosely coupled, reliable, and asynchronous.","title":"The JMS connector"},{"location":"jms/jms/#using-the-jms-connector","text":"To you the JMS Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-jms </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-jms . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-jms # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-jms The JMS Connector requires a jakarta.jms.ConnectionFactory to be exposed (as CDI bean). The connector looks for a jakarta.jms.ConnectionFactory and delegate the interaction with the JMS server to this factory. In other words, it creates the JMS connection and context using this factory. So, in order to use this connector you would need to expose a jakarta.jms.ConnectionFactory : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import jakarta.enterprise.context.ApplicationScoped ; import jakarta.enterprise.inject.Produces ; import jakarta.jms.ConnectionFactory ; import org.apache.activemq.artemis.jms.client.ActiveMQJMSConnectionFactory ; @ApplicationScoped public class ConnectionFactoryBean { @Produces ConnectionFactory factory () { return new ActiveMQJMSConnectionFactory ( \"tcp://localhost:61616\" , null , null ); } } The factory class may depend on your JMS connector/server.","title":"Using the JMS connector"},{"location":"jms/receiving-jms-messages/","text":"Receiving messages from JMS The JMS Connector retrieves JMS Message and maps each of them into Reactive Messaging Messages . Example Let\u2019s imagine you have a jakarta.jms.ConnectionFactory bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector. Configure your application to receive JMS messages on the prices channel as follows: 1 mp.messaging.incoming.prices.connector = smallrye-jms Note You don\u2019t need to set the destination. By default, it uses the channel name ( prices ). You can configure the destination attribute to override it. Note By default the connector uses a queue . You can configure it to use a topic by setting destination-type=topic . Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package jms.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class JmsPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package jms.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class JmsPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message return price . ack (); } } Deserialization The content of the incoming JMS message is mapped to a Java object. By default it extracts the JMS Message body as a java.lang.Object . This can be changed by setting, in the incoming JMS Message: The _classname property the JMSType The value must be a fully qualified class name. The connector then load the associated class. Note The connector loads the associated Class using the TCCL and if not found, the classloader used to load the connector. If the target type is a primitive type ort String , the resulting message contains the mapped payload. If the target type is a class, the object is built using included JSON deserializer (JSON-B and Jackson provided OOB, for more details see Serde ), from the JMSType . If not, the default behavior is used (Java deserialization). Inbound Metadata Messages coming from JMS contains an instance of io.smallrye.reactive.messaging.jms.IncomingJmsMessageMetadata in the metadata. 1 2 3 4 5 6 Optional < IncomingJmsMessageMetadata > metadata = incoming . getMetadata ( IncomingJmsMessageMetadata . class ); metadata . ifPresent ( meta -> { long expiration = meta . getExpiration (); Destination destination = meta . getDestination (); String value = meta . getStringProperty ( \"my-property\" ); }); Acknowledgement When the Reactive Messaging Message gets acknowledged, the associated JMS Message is acknowledged. As JMS acknowledgement is blocking, this acknowledgement is delegated to a worker thread. Configuration Reference Attribute ( alias ) Description Type Mandatory Default broadcast Whether or not the JMS message should be dispatched to multiple consumers boolean false false client-id The client id String false connection-factory-name The name of the JMS connection factory ( jakarta.jms.ConnectionFactory ) to be used. If not set, it uses any exposed JMS connection factory String false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either queue or topic string false queue durable Set to true to use a durable subscription boolean false false no-local Enable or disable local delivery boolean false false password The password to connect to to the JMS server String false selector The JMS selector String false session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false AUTO_ACKNOWLEDGE username The username to connect to to the JMS server String false","title":"Receiving JMS messages"},{"location":"jms/receiving-jms-messages/#receiving-messages-from-jms","text":"The JMS Connector retrieves JMS Message and maps each of them into Reactive Messaging Messages .","title":"Receiving messages from JMS"},{"location":"jms/receiving-jms-messages/#example","text":"Let\u2019s imagine you have a jakarta.jms.ConnectionFactory bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector. Configure your application to receive JMS messages on the prices channel as follows: 1 mp.messaging.incoming.prices.connector = smallrye-jms Note You don\u2019t need to set the destination. By default, it uses the channel name ( prices ). You can configure the destination attribute to override it. Note By default the connector uses a queue . You can configure it to use a topic by setting destination-type=topic . Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package jms.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class JmsPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package jms.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class JmsPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message return price . ack (); } }","title":"Example"},{"location":"jms/receiving-jms-messages/#deserialization","text":"The content of the incoming JMS message is mapped to a Java object. By default it extracts the JMS Message body as a java.lang.Object . This can be changed by setting, in the incoming JMS Message: The _classname property the JMSType The value must be a fully qualified class name. The connector then load the associated class. Note The connector loads the associated Class using the TCCL and if not found, the classloader used to load the connector. If the target type is a primitive type ort String , the resulting message contains the mapped payload. If the target type is a class, the object is built using included JSON deserializer (JSON-B and Jackson provided OOB, for more details see Serde ), from the JMSType . If not, the default behavior is used (Java deserialization).","title":"Deserialization"},{"location":"jms/receiving-jms-messages/#inbound-metadata","text":"Messages coming from JMS contains an instance of io.smallrye.reactive.messaging.jms.IncomingJmsMessageMetadata in the metadata. 1 2 3 4 5 6 Optional < IncomingJmsMessageMetadata > metadata = incoming . getMetadata ( IncomingJmsMessageMetadata . class ); metadata . ifPresent ( meta -> { long expiration = meta . getExpiration (); Destination destination = meta . getDestination (); String value = meta . getStringProperty ( \"my-property\" ); });","title":"Inbound Metadata"},{"location":"jms/receiving-jms-messages/#acknowledgement","text":"When the Reactive Messaging Message gets acknowledged, the associated JMS Message is acknowledged. As JMS acknowledgement is blocking, this acknowledgement is delegated to a worker thread.","title":"Acknowledgement"},{"location":"jms/receiving-jms-messages/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default broadcast Whether or not the JMS message should be dispatched to multiple consumers boolean false false client-id The client id String false connection-factory-name The name of the JMS connection factory ( jakarta.jms.ConnectionFactory ) to be used. If not set, it uses any exposed JMS connection factory String false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either queue or topic string false queue durable Set to true to use a durable subscription boolean false false no-local Enable or disable local delivery boolean false false password The password to connect to to the JMS server String false selector The JMS selector String false session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false AUTO_ACKNOWLEDGE username The username to connect to to the JMS server String false","title":"Configuration Reference"},{"location":"jms/sending-jms-messages/","text":"Sending messages to JMS The JMS Connector can send Reactive Messaging Messages as JMS Message. Example Let\u2019s imagine you have a jakarta.jms.ConnectionFactory bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector. Configure your application to write the messages from the prices channel into a JMS Message as follows: 1 mp.messaging.outgoing.prices.connector = smallrye-jms Note You don\u2019t need to set the destination. By default, it uses the channel name ( prices ). You can configure the destination attribute to override it. Note By default the connector uses a queue . You can configure it to use a topic by setting destination-type=topic . Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package jms.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class JmsPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package jms.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class JmsPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } } Serialization The connector serializes the incoming message payload into the body of the outgoing JMS Message. If the payload is a String or a primitive type, the payload is encoded as String and the JMSType is set to the target class. The _classname property is also set. The JMS Message is a TextMessage . If the payload is a byte[] , it\u2019s passed as byte[] in a JMS BytesMessage . Otherwise, the payload is encoded using included JSON serializer (JSON-B and Jackson provided OOB, for more details see Serde ). The JMSType is set to the target class. The _classname property is also set. The JMS Message is a TextMessage . For example, the following code serialize the produced Person using JSON-B. 1 2 3 4 5 6 @Incoming ( \"...\" ) @Outgoing ( \"my-channel\" ) public Person sendToJms (...) { // ... return new Person ( \"bob\" , 42 ); } It requires that the Person class can be serialized to JSON. The classname is passed in the JMSType property and _classname property. Outbound Metadata When sending Messages , you can add an instance of OutgoingJmsMessageMetadata to influence how the message is going to be written to JMS. 1 2 3 4 5 6 7 OutgoingJmsMessageMetadata metadata = OutgoingJmsMessageMetadata . builder () . withProperties ( JmsProperties . builder (). with ( \"some-property\" , \"some-value\" ). build ()) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); The metadata allow adding properties but also override the destination. Acknowledgement Once the JMS message is sent to the JMS server, the message is acknowledged. Sending a JMS message is a blocking operation. So, sending is done on a worker thread. Configuration Reference Attribute ( alias ) Description Type Mandatory Default client-id The client id String false connection-factory-name The name of the JMS connection factory ( jakarta.jms.ConnectionFactory ) to be used. If not set, it uses any exposed JMS connection factory String false correlation-id The JMS Message correlation id string false delivery-delay The delivery delay long false delivery-mode The delivery mode. Either persistent or non_persistent string false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either queue or topic string false queue disable-message-id Omit the message id in the outbound JMS message boolean false disable-message-timestamp Omit the message timestamp in the outbound JMS message boolean false merge Whether the connector should allow multiple upstreams boolean false false password The password to connect to to the JMS server String false priority The JMS Message priority int false reply-to The reply to destination if any string false reply-to-destination-type The type of destination for the response. It can be either queue or topic string false queue session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false AUTO_ACKNOWLEDGE ttl The JMS Message time-to-live long false username The username to connect to to the JMS server String false","title":"Sending JMS messages"},{"location":"jms/sending-jms-messages/#sending-messages-to-jms","text":"The JMS Connector can send Reactive Messaging Messages as JMS Message.","title":"Sending messages to JMS"},{"location":"jms/sending-jms-messages/#example","text":"Let\u2019s imagine you have a jakarta.jms.ConnectionFactory bean exposed and connected to your JMS server. Don\u2019t forget that it\u2019s required to use the JMS connector. Configure your application to write the messages from the prices channel into a JMS Message as follows: 1 mp.messaging.outgoing.prices.connector = smallrye-jms Note You don\u2019t need to set the destination. By default, it uses the channel name ( prices ). You can configure the destination attribute to override it. Note By default the connector uses a queue . You can configure it to use a topic by setting destination-type=topic . Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package jms.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class JmsPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package jms.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class JmsPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } }","title":"Example"},{"location":"jms/sending-jms-messages/#serialization","text":"The connector serializes the incoming message payload into the body of the outgoing JMS Message. If the payload is a String or a primitive type, the payload is encoded as String and the JMSType is set to the target class. The _classname property is also set. The JMS Message is a TextMessage . If the payload is a byte[] , it\u2019s passed as byte[] in a JMS BytesMessage . Otherwise, the payload is encoded using included JSON serializer (JSON-B and Jackson provided OOB, for more details see Serde ). The JMSType is set to the target class. The _classname property is also set. The JMS Message is a TextMessage . For example, the following code serialize the produced Person using JSON-B. 1 2 3 4 5 6 @Incoming ( \"...\" ) @Outgoing ( \"my-channel\" ) public Person sendToJms (...) { // ... return new Person ( \"bob\" , 42 ); } It requires that the Person class can be serialized to JSON. The classname is passed in the JMSType property and _classname property.","title":"Serialization"},{"location":"jms/sending-jms-messages/#outbound-metadata","text":"When sending Messages , you can add an instance of OutgoingJmsMessageMetadata to influence how the message is going to be written to JMS. 1 2 3 4 5 6 7 OutgoingJmsMessageMetadata metadata = OutgoingJmsMessageMetadata . builder () . withProperties ( JmsProperties . builder (). with ( \"some-property\" , \"some-value\" ). build ()) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); The metadata allow adding properties but also override the destination.","title":"Outbound Metadata"},{"location":"jms/sending-jms-messages/#acknowledgement","text":"Once the JMS message is sent to the JMS server, the message is acknowledged. Sending a JMS message is a blocking operation. So, sending is done on a worker thread.","title":"Acknowledgement"},{"location":"jms/sending-jms-messages/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default client-id The client id String false connection-factory-name The name of the JMS connection factory ( jakarta.jms.ConnectionFactory ) to be used. If not set, it uses any exposed JMS connection factory String false correlation-id The JMS Message correlation id string false delivery-delay The delivery delay long false delivery-mode The delivery mode. Either persistent or non_persistent string false destination The name of the JMS destination. If not set the name of the channel is used String false destination-type The type of destination. It can be either queue or topic string false queue disable-message-id Omit the message id in the outbound JMS message boolean false disable-message-timestamp Omit the message timestamp in the outbound JMS message boolean false merge Whether the connector should allow multiple upstreams boolean false false password The password to connect to to the JMS server String false priority The JMS Message priority int false reply-to The reply to destination if any string false reply-to-destination-type The type of destination for the response. It can be either queue or topic string false queue session-mode The session mode. Accepted values are AUTO_ACKNOWLEDGE, SESSION_TRANSACTED, CLIENT_ACKNOWLEDGE, DUPS_OK_ACKNOWLEDGE String false AUTO_ACKNOWLEDGE ttl The JMS Message time-to-live long false username The username to connect to to the JMS server String false","title":"Configuration Reference"},{"location":"kafka/avro-configuration/","text":"Using Apache Avro serializer/deserializer If you are using Apache Avro serializer/deserializer, please note the following configuration properties. For Confluent Schema Registry Confluent Avro library is io.confluent:kafka-avro-serializer . Note that this library is not available in Maven Central, you need to use the Confluent Maven repository . Consumer Property Recommended value value.deserializer io.confluent.kafka.serializers.KafkaAvroDeserializer schema.registry.url http://{your_host}:{your_port}/ specific.avro.reader true Example: 1 2 3 mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer mp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.avro.reader=true Producer Property Recommended value value.serializer io.confluent.kafka.serializers.KafkaAvroSerializer schema.registry.url http://{your_host}:{your_port}/ Example: 1 2 mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer mp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/ For Apicurio Registry 1.x Apicurio Registry 1.x Avro library is io.apicurio:apicurio-registry-utils-serde . The configuration properties listed here are meant to be used with the Apicurio Registry 1.x client library and Apicurio Registry 1.x server. Consumer Property Recommended value value.deserializer io.apicurio.registry.utils.serde.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/api apicurio.registry.avro-datum-provider io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider apicurio.registry.use-specific-avro-reader true Example: 1 2 3 4 mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.utils.serde.AvroKafkaDeserializer mp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api mp.messaging.incoming.[channel].apicurio.registry.avro-datum-provider=io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider mp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true Producer Property Recommended value value.serializer io.apicurio.registry.utils.serde.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/api To automatically register schemas with the registry, add: Property Value apicurio.registry.global-id io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy Example: 1 2 3 mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.utils.serde.AvroKafkaSerializer mp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api mp.messaging.outgoing.[channel].apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy For Apicurio Registry 2.x Apicurio Registry 2.x Avro library is io.apicurio:apicurio-registry-serdes-avro-serde . The configuration properties listed here are meant to be used with the Apicurio Registry 2.x client library and Apicurio Registry 2.x server. Consumer Property Recommended value value.deserializer io.apicurio.registry.serde.avro.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 apicurio.registry.use-specific-avro-reader true Example: 1 2 3 mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.serde.avro.AvroKafkaDeserializer mp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2 mp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true Producer Property Recommended value value.serializer io.apicurio.registry.serde.avro.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 To automatically register schemas with the registry, add: Property Value apicurio.registry.auto-register true Example: 1 2 3 mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.serde.avro.AvroKafkaSerializer mp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2 mp.messaging.outgoing.[channel].apicurio.registry.auto-register=true","title":"Using Avro"},{"location":"kafka/avro-configuration/#using-apache-avro-serializerdeserializer","text":"If you are using Apache Avro serializer/deserializer, please note the following configuration properties.","title":"Using Apache Avro serializer/deserializer"},{"location":"kafka/avro-configuration/#for-confluent-schema-registry","text":"Confluent Avro library is io.confluent:kafka-avro-serializer . Note that this library is not available in Maven Central, you need to use the Confluent Maven repository .","title":"For Confluent Schema Registry"},{"location":"kafka/avro-configuration/#consumer","text":"Property Recommended value value.deserializer io.confluent.kafka.serializers.KafkaAvroDeserializer schema.registry.url http://{your_host}:{your_port}/ specific.avro.reader true Example: 1 2 3 mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer mp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.avro.reader=true","title":"Consumer"},{"location":"kafka/avro-configuration/#producer","text":"Property Recommended value value.serializer io.confluent.kafka.serializers.KafkaAvroSerializer schema.registry.url http://{your_host}:{your_port}/ Example: 1 2 mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer mp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/","title":"Producer"},{"location":"kafka/avro-configuration/#for-apicurio-registry-1x","text":"Apicurio Registry 1.x Avro library is io.apicurio:apicurio-registry-utils-serde . The configuration properties listed here are meant to be used with the Apicurio Registry 1.x client library and Apicurio Registry 1.x server.","title":"For Apicurio Registry 1.x"},{"location":"kafka/avro-configuration/#consumer_1","text":"Property Recommended value value.deserializer io.apicurio.registry.utils.serde.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/api apicurio.registry.avro-datum-provider io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider apicurio.registry.use-specific-avro-reader true Example: 1 2 3 4 mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.utils.serde.AvroKafkaDeserializer mp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api mp.messaging.incoming.[channel].apicurio.registry.avro-datum-provider=io.apicurio.registry.utils.serde.avro.DefaultAvroDatumProvider mp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true","title":"Consumer"},{"location":"kafka/avro-configuration/#producer_1","text":"Property Recommended value value.serializer io.apicurio.registry.utils.serde.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/api To automatically register schemas with the registry, add: Property Value apicurio.registry.global-id io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy Example: 1 2 3 mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.utils.serde.AvroKafkaSerializer mp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/api mp.messaging.outgoing.[channel].apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy","title":"Producer"},{"location":"kafka/avro-configuration/#for-apicurio-registry-2x","text":"Apicurio Registry 2.x Avro library is io.apicurio:apicurio-registry-serdes-avro-serde . The configuration properties listed here are meant to be used with the Apicurio Registry 2.x client library and Apicurio Registry 2.x server.","title":"For Apicurio Registry 2.x"},{"location":"kafka/avro-configuration/#consumer_2","text":"Property Recommended value value.deserializer io.apicurio.registry.serde.avro.AvroKafkaDeserializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 apicurio.registry.use-specific-avro-reader true Example: 1 2 3 mp.messaging.incoming.[channel].value.deserializer=io.apicurio.registry.serde.avro.AvroKafkaDeserializer mp.messaging.incoming.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2 mp.messaging.incoming.[channel].apicurio.registry.use-specific-avro-reader=true","title":"Consumer"},{"location":"kafka/avro-configuration/#producer_2","text":"Property Recommended value value.serializer io.apicurio.registry.serde.avro.AvroKafkaSerializer apicurio.registry.url http://{your_host}:{your_port}/apis/registry/v2 To automatically register schemas with the registry, add: Property Value apicurio.registry.auto-register true Example: 1 2 3 mp.messaging.outgoing.[channel].value.serializer=io.apicurio.registry.serde.avro.AvroKafkaSerializer mp.messaging.outgoing.[channel].apicurio.registry.url=http://{your_host}:{your_port}/apis/registry/v2 mp.messaging.outgoing.[channel].apicurio.registry.auto-register=true","title":"Producer"},{"location":"kafka/client-service/","text":"KafkaClientService For advanced use cases, SmallRye Reactive Messaging provides a bean of type KafkaClientService that you can inject: 1 2 @Inject KafkaClientService kafka ; From there, you can obtain an io.smallrye.reactive.messaging.kafka.KafkaProducer and an io.smallrye.reactive.messaging.kafka.KafkaConsumer . KafkaProducer and KafkaConsumer expose a non-blocking API on top of the Kafka client API. They also mediate access to the threads that SmallRye Reactive Messaging uses to run all Kafka operations: the polling thread , used for consuming records from Kafka topics, and the sending thread , used for producing records to Kafka topics. (Just to be clear: each channel has its own polling thread and sending thread.) The reason why SmallRye Reactive Messaging uses a special thread to run the poll loop should be obvious: the Consumer API is blocking. The Producer API, on the other hand, is documented to be non-blocking. However, in present versions, Kafka doesn\u2019t guarantee that in all cases; see KAFKA-3539 for more details. That is why SmallRye Reactive Messaging uses a dedicated thread to run the send operations as well. Sometimes, SmallRye Reactive Messaging provides direct access to the Kafka Producer or Consumer . For example, a KafkaConsumerRebalanceListener methods are always invoked on the polling thread, so they give you direct access to Consumer . In such case, you should use the Producer / Consumer API directly, instead of the KafkaProducer / KafkaConsumer API.","title":"Accessing the client"},{"location":"kafka/client-service/#kafkaclientservice","text":"For advanced use cases, SmallRye Reactive Messaging provides a bean of type KafkaClientService that you can inject: 1 2 @Inject KafkaClientService kafka ; From there, you can obtain an io.smallrye.reactive.messaging.kafka.KafkaProducer and an io.smallrye.reactive.messaging.kafka.KafkaConsumer . KafkaProducer and KafkaConsumer expose a non-blocking API on top of the Kafka client API. They also mediate access to the threads that SmallRye Reactive Messaging uses to run all Kafka operations: the polling thread , used for consuming records from Kafka topics, and the sending thread , used for producing records to Kafka topics. (Just to be clear: each channel has its own polling thread and sending thread.) The reason why SmallRye Reactive Messaging uses a special thread to run the poll loop should be obvious: the Consumer API is blocking. The Producer API, on the other hand, is documented to be non-blocking. However, in present versions, Kafka doesn\u2019t guarantee that in all cases; see KAFKA-3539 for more details. That is why SmallRye Reactive Messaging uses a dedicated thread to run the send operations as well. Sometimes, SmallRye Reactive Messaging provides direct access to the Kafka Producer or Consumer . For example, a KafkaConsumerRebalanceListener methods are always invoked on the polling thread, so they give you direct access to Consumer . In such case, you should use the Producer / Consumer API directly, instead of the KafkaProducer / KafkaConsumer API.","title":"KafkaClientService"},{"location":"kafka/consumer-rebalance-listener/","text":"Consumer Rebalance Listener To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface, make the implementing class a bean, and add the @Identifier qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset. The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the partitionsAssigned callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the partitionsRevoked and partitionsAssigned callbacks again, so you can implement custom logic. Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier. When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes. If the rebalance listener handles offset commit on behalf of the user (using the ignore commit strategy), the rebalance listener must commit the offset synchronously in the partitionsRevoked callback. We also recommend applying the same logic when the application stops. Unlike the ConsumerRebalanceListener from Apache Kafka, the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener methods pass the Kafka Consumer and the set of topics/partitions. Example In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface and is annotated with @Identifier . We then must configure our inbound connector to use this named bean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package kafka.inbound ; import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.logging.Logger ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.Consumer ; import org.apache.kafka.clients.consumer.OffsetAndTimestamp ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener ; @ApplicationScoped @Identifier ( \"rebalanced-example.rebalancer\" ) public class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener { private static final Logger LOGGER = Logger . getLogger ( KafkaRebalancedConsumerRebalanceListener . class . getName ()); /** * When receiving a list of partitions will search for the earliest offset within 10 minutes * and seek the consumer to it. * * @param consumer underlying consumer * @param partitions set of assigned topic partitions */ @Override public void onPartitionsAssigned ( Consumer <? , ?> consumer , Collection < org . apache . kafka . common . TopicPartition > partitions ) { long now = System . currentTimeMillis (); long shouldStartAt = now - 600_000L ; //10 minute ago Map < org . apache . kafka . common . TopicPartition , Long > request = new HashMap <> (); for ( org . apache . kafka . common . TopicPartition partition : partitions ) { LOGGER . info ( \"Assigned \" + partition ); request . put ( partition , shouldStartAt ); } Map < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > offsets = consumer . offsetsForTimes ( request ); for ( Map . Entry < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > position : offsets . entrySet ()) { long target = position . getValue () == null ? 0 L : position . getValue (). offset (); LOGGER . info ( \"Seeking position \" + target + \" for \" + position . getKey ()); consumer . seek ( position . getKey (), target ); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package kafka.inbound ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Acknowledgment ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; @ApplicationScoped public class KafkaRebalancedConsumer { @Incoming ( \"rebalanced-example\" ) @Acknowledgment ( Acknowledgment . Strategy . NONE ) public CompletionStage < Void > consume ( IncomingKafkaRecord < Integer , String > message ) { // We don't need to ACK messages because in this example we set offset during consumer re-balance return CompletableFuture . completedFuture ( null ); } } To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name: mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer Or have the listener\u2019s name be the same as the group id: mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer Setting the consumer rebalance listener\u2019s name takes precedence over using the group id.","title":"Rebalance Listeners"},{"location":"kafka/consumer-rebalance-listener/#consumer-rebalance-listener","text":"To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface, make the implementing class a bean, and add the @Identifier qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset. The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the partitionsAssigned callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the partitionsRevoked and partitionsAssigned callbacks again, so you can implement custom logic. Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier. When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes. If the rebalance listener handles offset commit on behalf of the user (using the ignore commit strategy), the rebalance listener must commit the offset synchronously in the partitionsRevoked callback. We also recommend applying the same logic when the application stops. Unlike the ConsumerRebalanceListener from Apache Kafka, the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener methods pass the Kafka Consumer and the set of topics/partitions.","title":"Consumer Rebalance Listener"},{"location":"kafka/consumer-rebalance-listener/#example","text":"In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface and is annotated with @Identifier . We then must configure our inbound connector to use this named bean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package kafka.inbound ; import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.logging.Logger ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.Consumer ; import org.apache.kafka.clients.consumer.OffsetAndTimestamp ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener ; @ApplicationScoped @Identifier ( \"rebalanced-example.rebalancer\" ) public class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener { private static final Logger LOGGER = Logger . getLogger ( KafkaRebalancedConsumerRebalanceListener . class . getName ()); /** * When receiving a list of partitions will search for the earliest offset within 10 minutes * and seek the consumer to it. * * @param consumer underlying consumer * @param partitions set of assigned topic partitions */ @Override public void onPartitionsAssigned ( Consumer <? , ?> consumer , Collection < org . apache . kafka . common . TopicPartition > partitions ) { long now = System . currentTimeMillis (); long shouldStartAt = now - 600_000L ; //10 minute ago Map < org . apache . kafka . common . TopicPartition , Long > request = new HashMap <> (); for ( org . apache . kafka . common . TopicPartition partition : partitions ) { LOGGER . info ( \"Assigned \" + partition ); request . put ( partition , shouldStartAt ); } Map < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > offsets = consumer . offsetsForTimes ( request ); for ( Map . Entry < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > position : offsets . entrySet ()) { long target = position . getValue () == null ? 0 L : position . getValue (). offset (); LOGGER . info ( \"Seeking position \" + target + \" for \" + position . getKey ()); consumer . seek ( position . getKey (), target ); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package kafka.inbound ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Acknowledgment ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; @ApplicationScoped public class KafkaRebalancedConsumer { @Incoming ( \"rebalanced-example\" ) @Acknowledgment ( Acknowledgment . Strategy . NONE ) public CompletionStage < Void > consume ( IncomingKafkaRecord < Integer , String > message ) { // We don't need to ACK messages because in this example we set offset during consumer re-balance return CompletableFuture . completedFuture ( null ); } } To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name: mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer Or have the listener\u2019s name be the same as the group id: mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer Setting the consumer rebalance listener\u2019s name takes precedence over using the group id.","title":"Example"},{"location":"kafka/default-configuration/","text":"Retrieving Kafka default configuration If your application/runtime exposes as a CDI bean of type Map<String, Object with the identifier default-kafka-broker , this configuration is used to establish the connection with the Kafka broker. For example, you can imagine exposing this map as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Produces @ApplicationScoped @Identifier ( \"default-kafka-broker\" ) public Map < String , Object > createKafkaRuntimeConfig () { Map < String , Object > properties = new HashMap <> (); StreamSupport . stream ( config . getPropertyNames (). spliterator (), false ) . map ( String :: toLowerCase ) . filter ( name -> name . startsWith ( \"kafka\" )) . distinct () . sorted () . forEach ( name -> { final String key = name . substring ( \"kafka\" . length () + 1 ). toLowerCase (). replaceAll ( \"[^a-z0-9.]\" , \".\" ); final String value = config . getOptionalValue ( name , String . class ). orElse ( \"\" ); properties . put ( key , value ); }); return properties ; } This previous example would extract all the configuration keys from MicroProfile Config starting with kafka . Quarkus Starting with Quarkus 1.5, a map corresponding to the previous example is automatically provided. In addition to this default configuration, you can configure the name of the Map producer using the kafka-configuration attribute: 1 2 mp.messaging.incoming.my-channel.connector = smallrye-kafka mp.messaging.incoming.my-channel.kafka-configuration = my-configuration In this case, the connector looks for the Map associated with the my-configuration name. If kafka-configuration is not set, an optional lookup for a Map exposed with the channel name ( my-channel in the previous example) is done. Important If kafka-configuration is set and no Map can be found, the deployment fails. Attribute values are resolved as follows: if the attribute is set directly on the channel configuration ( mp.messaging.incoming.my-channel.attribute=value ), this value is used if the attribute is not set on the channel, the connector looks for a Map with the channel name or the configured kafka-configuration (if set) and the value is retrieved from that Map If the resolved Map does not contain the value the default Map is used (exposed with the default-kafka-broker name)","title":"Customizing Default Kafka Configuration"},{"location":"kafka/default-configuration/#retrieving-kafka-default-configuration","text":"If your application/runtime exposes as a CDI bean of type Map<String, Object with the identifier default-kafka-broker , this configuration is used to establish the connection with the Kafka broker. For example, you can imagine exposing this map as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Produces @ApplicationScoped @Identifier ( \"default-kafka-broker\" ) public Map < String , Object > createKafkaRuntimeConfig () { Map < String , Object > properties = new HashMap <> (); StreamSupport . stream ( config . getPropertyNames (). spliterator (), false ) . map ( String :: toLowerCase ) . filter ( name -> name . startsWith ( \"kafka\" )) . distinct () . sorted () . forEach ( name -> { final String key = name . substring ( \"kafka\" . length () + 1 ). toLowerCase (). replaceAll ( \"[^a-z0-9.]\" , \".\" ); final String value = config . getOptionalValue ( name , String . class ). orElse ( \"\" ); properties . put ( key , value ); }); return properties ; } This previous example would extract all the configuration keys from MicroProfile Config starting with kafka . Quarkus Starting with Quarkus 1.5, a map corresponding to the previous example is automatically provided. In addition to this default configuration, you can configure the name of the Map producer using the kafka-configuration attribute: 1 2 mp.messaging.incoming.my-channel.connector = smallrye-kafka mp.messaging.incoming.my-channel.kafka-configuration = my-configuration In this case, the connector looks for the Map associated with the my-configuration name. If kafka-configuration is not set, an optional lookup for a Map exposed with the channel name ( my-channel in the previous example) is done. Important If kafka-configuration is set and no Map can be found, the deployment fails. Attribute values are resolved as follows: if the attribute is set directly on the channel configuration ( mp.messaging.incoming.my-channel.attribute=value ), this value is used if the attribute is not set on the channel, the connector looks for a Map with the channel name or the configured kafka-configuration (if set) and the value is retrieved from that Map If the resolved Map does not contain the value the default Map is used (exposed with the default-kafka-broker name)","title":"Retrieving Kafka default configuration"},{"location":"kafka/health/","text":"Health reporting The Kafka connector reports the readiness and liveness of each channel managed by the connector. Note To disable health reporting, set the health-enabled attribute for the channel to false . Readiness On the inbound side, two strategies are available to check the readiness of the application. The default strategy verifies that we have at least one active connection with the broker. This strategy is lightweight. You can also enable another strategy by setting the health-readiness-topic-verification attribute to true . In this case, the check verifies that: the broker is available the Kafka topic is created (available in the broker). no failures have been caught With this second strategy, if you consume multiple topics using the topics attribute, the readiness check verifies that all the consumed topics are available. If you use a pattern (using the pattern attribute), the readiness check verifies that at least one existing topic matches the pattern. On the outbound side (writing records to Kafka), two strategies are also offered. The default strategy just verifies that the producer has at least one active connection with the broker. You can also enable another strategy by setting the health-readiness-topic-verification attribute to true . In this case, teh check verifies that the broker is available the Kafka topic is created (available in the broker). With this second strategy, the readiness check uses a Kafka Admin Client to retrieve the existing topics. Retrieving the topics can be a lengthy operation. You can configure a timeout using the health-readiness-timeout attribute. The default timeout is set to 2 seconds. Also, you can disable the readiness checks altogether by setting health-readiness-enabled to false . Liveness On the inbound side (receiving records from Kafka), the liveness check verifies that: no failures have been caught the client is connected to the broker On the outbound side (writing records to Kafka), the liveness check verifies that: no failures have been caught Note that a message processing failures nacks the message which is then handled by the failure-strategy. It the responsibility of the failure-strategy to report the failure and influence the outcome of the liveness checks. The fail failure strategy reports the failure and so the liveness check will report the failure.","title":"Health Checks"},{"location":"kafka/health/#health-reporting","text":"The Kafka connector reports the readiness and liveness of each channel managed by the connector. Note To disable health reporting, set the health-enabled attribute for the channel to false .","title":"Health reporting"},{"location":"kafka/health/#readiness","text":"On the inbound side, two strategies are available to check the readiness of the application. The default strategy verifies that we have at least one active connection with the broker. This strategy is lightweight. You can also enable another strategy by setting the health-readiness-topic-verification attribute to true . In this case, the check verifies that: the broker is available the Kafka topic is created (available in the broker). no failures have been caught With this second strategy, if you consume multiple topics using the topics attribute, the readiness check verifies that all the consumed topics are available. If you use a pattern (using the pattern attribute), the readiness check verifies that at least one existing topic matches the pattern. On the outbound side (writing records to Kafka), two strategies are also offered. The default strategy just verifies that the producer has at least one active connection with the broker. You can also enable another strategy by setting the health-readiness-topic-verification attribute to true . In this case, teh check verifies that the broker is available the Kafka topic is created (available in the broker). With this second strategy, the readiness check uses a Kafka Admin Client to retrieve the existing topics. Retrieving the topics can be a lengthy operation. You can configure a timeout using the health-readiness-timeout attribute. The default timeout is set to 2 seconds. Also, you can disable the readiness checks altogether by setting health-readiness-enabled to false .","title":"Readiness"},{"location":"kafka/health/#liveness","text":"On the inbound side (receiving records from Kafka), the liveness check verifies that: no failures have been caught the client is connected to the broker On the outbound side (writing records to Kafka), the liveness check verifies that: no failures have been caught Note that a message processing failures nacks the message which is then handled by the failure-strategy. It the responsibility of the failure-strategy to report the failure and influence the outcome of the liveness checks. The fail failure strategy reports the failure and so the liveness check will report the failure.","title":"Liveness"},{"location":"kafka/kafka/","text":"Apache Kafka Connector The Kafka connector adds support for Kafka to Reactive Messaging. With it you can receive Kafka Records as well as write message into Kafka. Apache Kafka is a popular distributed streaming platform. It lets you: Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. The Kafka cluster stores streams of records in categories called topics . Each record consists of a key , a value , and a timestamp . For more details about Kafka, check the documentation . Using the Kafka Connector To use the Kafka Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-kafka </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-kafka . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-kafka # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-kafka","title":"Apache Kafka Connector"},{"location":"kafka/kafka/#apache-kafka-connector","text":"The Kafka connector adds support for Kafka to Reactive Messaging. With it you can receive Kafka Records as well as write message into Kafka. Apache Kafka is a popular distributed streaming platform. It lets you: Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. The Kafka cluster stores streams of records in categories called topics . Each record consists of a key , a value , and a timestamp . For more details about Kafka, check the documentation .","title":"Apache Kafka Connector"},{"location":"kafka/kafka/#using-the-kafka-connector","text":"To use the Kafka Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-kafka </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-kafka . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-kafka # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-kafka","title":"Using the Kafka Connector"},{"location":"kafka/kerberos/","text":"Kerberos authentication When using Kerberos authentication, you need to configure the connector with: the security protocol set to SASL_PLAINTEXT the SASL mechanism set to GSSAPI the Jaas config configured with Krb5LoginModule the Kerberos service name The following snippet provides an example: 1 2 3 4 5 kafka.bootstrap.servers = ip-192-168-0-207.us-east-2.compute.internal:9094 kafka.sasl.mechanism = GSSAPI kafka.security.protocol = SASL_PLAINTEXT kafka.sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true refreshKrb5Config=true useKeyTab=true storeKey=true keyTab=\"file:/opt/kafka/krb5/kafka-producer.keytab\" principal=\"kafka-producer/ip-192-168-0-207.us-east-2.compute.internal@INTERNAL\"; kafka.sasl.kerberos.service.name = kafka","title":"Kerberos authentication"},{"location":"kafka/kerberos/#kerberos-authentication","text":"When using Kerberos authentication, you need to configure the connector with: the security protocol set to SASL_PLAINTEXT the SASL mechanism set to GSSAPI the Jaas config configured with Krb5LoginModule the Kerberos service name The following snippet provides an example: 1 2 3 4 5 kafka.bootstrap.servers = ip-192-168-0-207.us-east-2.compute.internal:9094 kafka.sasl.mechanism = GSSAPI kafka.security.protocol = SASL_PLAINTEXT kafka.sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true refreshKrb5Config=true useKeyTab=true storeKey=true keyTab=\"file:/opt/kafka/krb5/kafka-producer.keytab\" principal=\"kafka-producer/ip-192-168-0-207.us-east-2.compute.internal@INTERNAL\"; kafka.sasl.kerberos.service.name = kafka","title":"Kerberos authentication"},{"location":"kafka/protobuf-configuration/","text":"Using Google Protobuf serializer/deserializer If you are using Protocol Buffers serializer/deserializer, please note the following configuration properties. For Confluent Schema Registry Confluent protobuf library is io.confluent:kafka-protobuf-serializer . Note that this library is not available in Maven Central, you need to use the Confluent Maven repository . Consumer Property Recommended value value.deserializer io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer schema.registry.url http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.protobuf.value.type your.package.DomainObjectKey$Key mp.messaging.incoming.[channel].specific.protobuf.key.type your.package.DomainObjectValue$Value Example: 1 2 3 4 mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer mp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.protobuf.value.type=your.package.DomainObjectKey$Key mp.messaging.incoming.[channel].specific.protobuf.key.type=your.package.DomainObjectValue$Value Producer Property Recommended value value.serializer io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer schema.registry.url http://{your_host}:{your_port}/ Example: 1 2 mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer mp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/","title":"Using Protobuf"},{"location":"kafka/protobuf-configuration/#using-google-protobuf-serializerdeserializer","text":"If you are using Protocol Buffers serializer/deserializer, please note the following configuration properties.","title":"Using Google Protobuf serializer/deserializer"},{"location":"kafka/protobuf-configuration/#for-confluent-schema-registry","text":"Confluent protobuf library is io.confluent:kafka-protobuf-serializer . Note that this library is not available in Maven Central, you need to use the Confluent Maven repository .","title":"For Confluent Schema Registry"},{"location":"kafka/protobuf-configuration/#consumer","text":"Property Recommended value value.deserializer io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer schema.registry.url http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.protobuf.value.type your.package.DomainObjectKey$Key mp.messaging.incoming.[channel].specific.protobuf.key.type your.package.DomainObjectValue$Value Example: 1 2 3 4 mp.messaging.incoming.[channel].value.deserializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer mp.messaging.incoming.[channel].schema.registry.url=http://{your_host}:{your_port}/ mp.messaging.incoming.[channel].specific.protobuf.value.type=your.package.DomainObjectKey$Key mp.messaging.incoming.[channel].specific.protobuf.key.type=your.package.DomainObjectValue$Value","title":"Consumer"},{"location":"kafka/protobuf-configuration/#producer","text":"Property Recommended value value.serializer io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer schema.registry.url http://{your_host}:{your_port}/ Example: 1 2 mp.messaging.outgoing.[channel].value.serializer=io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer mp.messaging.outgoing.[channel].schema.registry.url=http://{your_host}:{your_port}/","title":"Producer"},{"location":"kafka/receiving-kafka-records/","text":"Receiving Kafka Records The Kafka Connector retrieves Kafka Records from Kafka Brokers and maps each of them to Reactive Messaging Messages . Example Let\u2019s imagine you have a Kafka broker running, and accessible using the kafka:9092 address (by default it would use localhost:9092 ). Configure your application to receive Kafka records from a Kafka topic on the prices channel as follows: 1 2 3 4 5 kafka.bootstrap.servers = kafka:9092 # <1> mp.messaging.incoming.prices.connector = smallrye-kafka # <2> mp.messaging.incoming.prices.value.deserializer = org.apache.kafka.common.serialization.DoubleDeserializer # <3> mp.messaging.incoming.prices.broadcast = true # <4> Configure the broker location. You can configure it globally or per channel Configure the connector to manage the prices channel Sets the (Kafka) deserializer to read the record\u2019s value Make sure that we can receive from more than one consumer (see KafkaPriceConsumer and KafkaPriceMessageConsumer below) Note You don\u2019t need to set the Kafka topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package kafka.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class KafkaPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package kafka.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class KafkaPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message (commit the offset) return price . ack (); } } Deserialization The deserialization is handled by the underlying Kafka Client. You need to configure the: mp.messaging.incoming.[channel-name].value.deserializer to configure the value deserializer (mandatory) mp.messaging.incoming.[channel-name].key.deserializer to configure the key deserializer (optional, default to String ) If you want to use a custom deserializer, add it to your CLASSPATH and configure the associate attribute. In addition, the Kafka Connector also provides a set of message converters . So you can receive payloads representing records from Kafka using: Record - a pair key/value ConsumerRecord - a structure representing the record with all its metadata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Incoming ( \"topic-a\" ) public void consume ( Record < String , String > record ) { String key = record . key (); // Can be `null` if the incoming record has no key String value = record . value (); // Can be `null` if the incoming record has no value } @Incoming ( \"topic-b\" ) public void consume ( ConsumerRecord < String , String > record ) { String key = record . key (); // Can be `null` if the incoming record has no key String value = record . value (); // Can be `null` if the incoming record has no value String topic = record . topic (); int partition = record . partition (); // ... } Inbound Metadata Messages coming from Kafka contains an instance of IncomingKafkaRecordMetadata in the metadata. It provides the key, topic, partitions, headers and so on: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 IncomingKafkaRecordMetadata < String , Double > metadata = incoming . getMetadata ( IncomingKafkaRecordMetadata . class ) . orElse ( null ); if ( metadata != null ) { // The topic String topic = metadata . getTopic (); // The key String key = metadata . getKey (); // The timestamp Instant timestamp = metadata . getTimestamp (); // The underlying record ConsumerRecord < String , Double > record = metadata . getRecord (); // ... } Acknowledgement When a message produced from a Kafka record is acknowledged, the connector invokes a commit strategy . These strategies decide when the consumer offset for a specific topic/partition is committed. Committing an offset indicates that all previous records have been processed. It is also the position where the application would restart the processing after a crash recovery or a restart. Committing every offset has performance penalties as Kafka offset management can be slow. However, not committing the offset often enough may lead to message duplication if the application crashes between two commits. The Kafka connector supports three strategies: throttled keeps track of received messages and commit to the next offset after the latest acked message in sequence. This strategy guarantees at-least-once delivery even if the channel performs asynchronous processing. The connector tracks the received records and periodically (period specified by auto.commit.interval.ms (default: 5000)) commits the highest consecutive offset. The connector will be marked as unhealthy if a message associated with a record is not acknowledged in throttled.unprocessed-record-max-age.ms (default: 60000). Indeed, this strategy cannot commit the offset as soon as a single record processing fails (see failure-strategy to configure what happens on failing processing). If throttled.unprocessed-record-max-age.ms is set to less than or equal to 0, it does not perform any health check verification. Such a setting might lead to running out of memory if there are poison pill messages. This strategy is the default if enable.auto.commit is not explicitly set to true . checkpoint allows persisting consumer offsets on a \"state store\", instead of committing them back to the Kafka broker. Using the CheckpointMetadata API, consumer code can persist a processing state with the offset to mark the progress of a consumer. When the processing continues from a previously persisted offset, it seeks the Kafka consumer to that offset and also restores the persisted state, continuing the stateful processing from where it left off. The checkpoint strategy holds locally the processing state associated with the latest offset, and persists it periodically to the state store (period specified by auto.commit.interval.ms (default: 5000)). The connector will be marked as unhealthy if no processing state is persisted to the state store in checkpoint.unsynced-state-max-age.ms (default: 10000). Using the CheckpointMetadata API the user code can force to persist the state on message ack. If checkpoint.unsynced-state-max-age.ms is set to less than or equal to 0, it does not perform any health check verification. For more information, see Stateful processing with Checkpointing latest commits the record offset received by the Kafka consumer as soon as the associated message is acknowledged (if the offset is higher than the previously committed offset). This strategy provides at-least-once delivery if the channel processes the message without performing any asynchronous processing. This strategy should not be used on high-load as offset commit is expensive. However, it reduces the risk of duplicates. ignore performs no commit. This strategy is the default strategy when the consumer is explicitly configured with enable.auto.commit to true . It delegates the offset commit to the Kafka client. When enable.auto.commit is true this strategy DOES NOT guarantee at-least-once delivery. However, if the processing failed between two commits, messages received after the commit and before the failure will be re-processed. Important The Kafka connector disables the Kafka auto commit if not explicitly enabled. This behavior differs from the traditional Kafka consumer. If high-throughout is important for you, and not limited by the downstream, we recommend to either: Use the throttled policy or set enable.auto.commit to true and annotate the consuming method with @Acknowledgment(Acknowledgment.Strategy.NONE) Failure Management If a message produced from a Kafka record is nacked , a failure strategy is applied. The Kafka connector supports 3 strategies: fail - fail the application, no more records will be processed. (default) The offset of the record that has not been processed correctly is not committed. ignore - the failure is logged, but the processing continue. The offset of the record that has not been processed correctly is committed. dead-letter-queue - the offset of the record that has not been processed correctly is committed, but the record is written to a (Kafka) dead letter queue topic. The strategy is selected using the failure-strategy attribute. In the case of dead-letter-queue , you can configure the following attributes: dead-letter-queue.topic : the topic to use to write the records not processed correctly, default is dead-letter-topic-$channel , with $channel being the name of the channel. dead-letter-queue.producer-client-id : the client id used by the kafka producer when sending records to dead letter queue topic. If not specified it will default to kafka-dead-letter-topic-producer-$client-id , with $client-id being the value obtained from consumer client id. dead-letter-queue.key.serializer : the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer. dead-letter-queue.value.serializer : the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer. The record written on the dead letter topic contains the original record\u2019s headers, as well as a set of additional headers about the original record: dead-letter-reason - the reason of the failure (the Throwable passed to nack() ) dead-letter-cause - the cause of the failure (the getCause() of the Throwable passed to nack() ), if any dead-letter-topic - the original topic of the record dead-letter-partition - the original partition of the record (integer mapped to String) dead-letter-offset - the original offset of the record (long mapped to String) When using dead-letter-queue , it is also possible to change some metadata of the record that is sent to the dead letter topic. To do that, use the Message.nack(Throwable, Metadata) method: 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) public CompletionStage < Void > consume ( KafkaRecord < String , String > message ) { return message . nack ( new Exception ( \"Failed!\" ), Metadata . of ( OutgoingKafkaRecordMetadata . builder () . withKey ( \"failed-record\" ) . withHeaders ( new RecordHeaders () . add ( \"my-header\" , \"my-header-value\" . getBytes ( StandardCharsets . UTF_8 ))) . build ())); } The Metadata may contain an instance of OutgoingKafkaRecordMetadata . If the instance is present, the following properties will be used: key; if not present, the original record\u2019s key will be used topic; if not present, the configured dead letter topic will be used partition; if not present, partition will be assigned automatically headers; combined with the original record\u2019s headers, as well as the dead-letter-* headers described above Custom commit and failure strategies In addition to provided strategies, it is possible to implement custom commit and failure strategies and configure Kafka channels with them. For example, for a custom commit strategy, implement the KafkaCommitHandler interface, and provide a managed bean implementing the KafkaCommitHandler.Factory interface, identified using @Identifier qualifier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 package kafka.inbound ; import java.util.Collection ; import java.util.function.BiConsumer ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.common.TopicPartition ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; import io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration ; import io.smallrye.reactive.messaging.kafka.KafkaConsumer ; import io.smallrye.reactive.messaging.kafka.commit.KafkaCommitHandler ; import io.vertx.mutiny.core.Vertx ; public class KafkaCustomCommit implements KafkaCommitHandler { @Override public < K , V > Uni < Void > handle ( IncomingKafkaRecord < K , V > record ) { // called on message ack return Uni . createFrom (). voidItem (); } @Override public < K , V > Uni < IncomingKafkaRecord < K , V >> received ( IncomingKafkaRecord < K , V > record ) { // called before message processing return Uni . createFrom (). item ( record ); } @Override public void terminate ( boolean graceful ) { // called on channel shutdown } @Override public void partitionsAssigned ( Collection < TopicPartition > partitions ) { // called on partitions assignment } @Override public void partitionsRevoked ( Collection < TopicPartition > partitions ) { // called on partitions revoked } @ApplicationScoped @Identifier ( \"custom\" ) public static class Factory implements KafkaCommitHandler . Factory { @Override public KafkaCommitHandler create ( KafkaConnectorIncomingConfiguration config , Vertx vertx , KafkaConsumer <? , ?> consumer , BiConsumer < Throwable , Boolean > reportFailure ) { return new KafkaCustomCommit ( /* ... */ ); } } } Finally, to use the custom commit strategy, set the commit-strategy attribute to the identifier of the commit handler factory: mp.messaging.incoming.$channel.commit-strategy=custom . Similarly, custom failure strategies can be configured using failure-strategy attribute. Note If the custom strategy implementation inherits ContextHolder class it can access the Vert.x event-loop context created for the Kafka consumer Retrying processing You can combine Reactive Messaging with SmallRye Fault Tolerance , and retry processing when it fails: 1 2 3 4 5 6 @Incoming ( \"kafka\" ) @Outgoing ( \"processed\" ) @Retry ( delay = 10 , maxRetries = 5 ) public String process ( String v ) { // ... retry if this method throws an exception } You can configure the delay, the number of retries, the jitter... If your method returns a Uni , you need to add the @NonBlocking annotation: 1 2 3 4 5 6 7 @Incoming ( \"kafka\" ) @Outgoing ( \"processed\" ) @Retry ( delay = 10 , maxRetries = 5 ) @NonBlocking public Uni < String > process ( String v ) { // ... retry if this method throws an exception or the returned Uni produce a failure } The incoming messages are acknowledged only once the processing completes successfully. So, it commits the offset after the successful processing. If after the retries the processing still failed, the message is nacked and the failure strategy is applied. You can also use @Retry on methods only consuming incoming messages: 1 2 3 4 5 @Incoming ( \"kafka\" ) @Retry ( delay = 10 , maxRetries = 5 ) public void consume ( String v ) { // ... retry if this method throws an exception } Handling deserialization failures Because deserialization happens before creating a Message , the failure strategy presented above cannot be applied. However, when a deserialization failure occurs, you can intercept it and provide a fallback value. To achieve this, create a CDI bean implementing the DeserializationFailureHandler interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ApplicationScoped @Identifier ( \"failure-retry\" ) // Set the name of the failure handler public class MyDeserializationFailureHandler implements DeserializationFailureHandler < JsonObject > { // Specify the expected type @Override public JsonObject decorateDeserialization ( Uni < JsonObject > deserialization , String topic , boolean isKey , String deserializer , byte [] data , Headers headers ) { return deserialization . onFailure (). retry (). atMost ( 3 ) . await (). atMost ( Duration . ofMillis ( 200 )); } } The bean must be exposed with the @Identifier qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute: mp.messaging.incoming.$channel.key-deserialization-failure-handler : name of the bean handling deserialization failures happening for the record\u2019s key mp.messaging.incoming.$channel.value-deserialization-failure-handler : name of the bean handling deserialization failures happening for the record\u2019s value, The handler is called with the deserialization action as a Uni<T> , the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers augmented with headers describing the failure (which ease the write to a dead letter). On the deserialization Uni failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the deserialized object. Alternatively, the handler can only implement handleDeserializationFailure method and provide a fallback value, which may be null . If you don\u2019t configure a deserialization failure handlers and a deserialization failure happens, the application is marked unhealthy. You can also ignore the failure, which will log the exception and produce a null value. To enable this behavior, set the mp.messaging.incoming.$channel.fail-on-deserialization-failure attribute to false . Receiving Cloud Events The Kafka connector supports Cloud Events . When the connector detects a structured or binary Cloud Events, it adds a IncomingKafkaCloudEventMetadata in the metadata of the Message. IncomingKafkaCloudEventMetadata contains the various (mandatory and optional) Cloud Event attributes. If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata. Binary Cloud Events For binary Cloud Events, all mandatory Cloud Event attributes must be set in the record header, prefixed by ce_ (as mandated by the protocol binding ). The connector considers headers starting with the ce_ prefix but not listed in the specification as extensions. You can access them using the getExtension method from IncomingKafkaCloudEventMetadata . You can retrieve them as String . The datacontenttype attribute is mapped to the content-type header of the record. The partitionkey attribute is mapped to the record\u2019s key, if any. Note that all headers are read as UTF-8. With binary Cloud Events, the record\u2019s key and value can use any deserializer. Structured Cloud Events For structured Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value. Structured Cloud Event must set the content-type header of the record to application/cloudevents or prefix the value with application/cloudevents such as: application/cloudevents+json; charset=UTF-8 . To receive structured Cloud Events, your value deserializer must be: org.apache.kafka.common.serialization.StringDeserializer org.apache.kafka.common.serialization.ByteArrayDeserializer io.vertx.kafka.client.serialization.JsonObjectDeserializer As mentioned previously, the value must be a valid JSON object containing at least all the mandatory Cloud Events attributes. If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event data . The partitionkey attribute is mapped to the record\u2019s key if any. Consumer Rebalance Listener To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface, make the implementing class a bean, and add the @Identifier qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset. The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the partitionsAssigned callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the partitionsRevoked and partitionsAssigned callbacks again, so you can implement custom logic. Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier. When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes. If the rebalance listener handles offset commit on behalf of the user (using the ignore commit strategy), the rebalance listener must commit the offset synchronously in the partitionsRevoked callback. We also recommend applying the same logic when the application stops. Unlike the ConsumerRebalanceListener from Apache Kafka, the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener methods pass the Kafka Consumer and the set of topics/partitions. Example In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface and is annotated with @Identifier . We then must configure our inbound connector to use this named bean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package kafka.inbound ; import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.logging.Logger ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.Consumer ; import org.apache.kafka.clients.consumer.OffsetAndTimestamp ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener ; @ApplicationScoped @Identifier ( \"rebalanced-example.rebalancer\" ) public class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener { private static final Logger LOGGER = Logger . getLogger ( KafkaRebalancedConsumerRebalanceListener . class . getName ()); /** * When receiving a list of partitions will search for the earliest offset within 10 minutes * and seek the consumer to it. * * @param consumer underlying consumer * @param partitions set of assigned topic partitions */ @Override public void onPartitionsAssigned ( Consumer <? , ?> consumer , Collection < org . apache . kafka . common . TopicPartition > partitions ) { long now = System . currentTimeMillis (); long shouldStartAt = now - 600_000L ; //10 minute ago Map < org . apache . kafka . common . TopicPartition , Long > request = new HashMap <> (); for ( org . apache . kafka . common . TopicPartition partition : partitions ) { LOGGER . info ( \"Assigned \" + partition ); request . put ( partition , shouldStartAt ); } Map < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > offsets = consumer . offsetsForTimes ( request ); for ( Map . Entry < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > position : offsets . entrySet ()) { long target = position . getValue () == null ? 0 L : position . getValue (). offset (); LOGGER . info ( \"Seeking position \" + target + \" for \" + position . getKey ()); consumer . seek ( position . getKey (), target ); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package kafka.inbound ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Acknowledgment ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; @ApplicationScoped public class KafkaRebalancedConsumer { @Incoming ( \"rebalanced-example\" ) @Acknowledgment ( Acknowledgment . Strategy . NONE ) public CompletionStage < Void > consume ( IncomingKafkaRecord < Integer , String > message ) { // We don't need to ACK messages because in this example we set offset during consumer re-balance return CompletableFuture . completedFuture ( null ); } } To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name: mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer Or have the listener\u2019s name be the same as the group id: mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer Setting the consumer rebalance listener\u2019s name takes precedence over using the group id. Receiving Kafka Records in Batches By default, incoming methods receive each Kafka record individually. Under the hood, Kafka consumer clients poll the broker constantly and receive records in batches, presented inside the ConsumerRecords container. In batch mode, your application can receive all the records returned by the consumer poll in one go. To achieve this you need to set mp.messaging.incoming.$channel.batch=true and specify a compatible container type to receive all the data: 1 2 3 4 5 6 @Incoming ( \"prices\" ) public void consume ( List < Double > prices ) { for ( double price : prices ) { // process price } } The incoming method can also receive Message<List<Payload> , KafkaBatchRecords<Payload> ConsumerRecords<Key, Payload> types, They give access to record details such as offset or timestamp : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Incoming ( \"prices\" ) public CompletionStage < Void > consumeMessage ( KafkaRecordBatch < String , Double > records ) { for ( KafkaRecord < String , Double > record : records ) { record . getMetadata ( IncomingKafkaRecordMetadata . class ). ifPresent ( metadata -> { int partition = metadata . getPartition (); long offset = metadata . getOffset (); Instant timestamp = metadata . getTimestamp (); //... process messages }); } // ack will commit the latest offsets (per partition) of the batch. return records . ack (); } @Incoming ( \"prices\" ) public void consumeRecords ( ConsumerRecords < String , Double > records ) { for ( TopicPartition partition : records . partitions ()) { for ( ConsumerRecord < String , Double > record : records . records ( partition )) { //... process messages } } } Note that the successful processing of the incoming record batch will commit the latest offsets for each partition received inside the batch. The configured commit strategy will be applied for these records only. Conversely, if the processing throws an exception, all messages are nacked , applying the failure strategy for all the records inside the batch. Stateful processing with Checkpointing Experimental Checkpointing is experimental, and APIs and features are subject to change in the future. The checkpoint commit strategy allows for a Kafka incoming channel to manage topic-partition offsets, not by committing on the Kafka broker, but by persisting consumers' advancement on a state store . In addition to that, if the consumer builds an internal state as a result of consumed records, the topic-partition offset persisted to the state store can be associated with a processing state , saving the local state to the persistent store. When a consumer restarts or consumer group instances scale, i.e. when new partitions get assigned to the consumer, the checkpointing works by resuming the processing from the latest offset and its saved state. The @Incoming channel consumer code can manipulate the processing state through the CheckpointMetadata API: 1 2 3 4 5 6 7 8 9 10 11 12 13 @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( KafkaRecord < String , Double > record ) { // Get the `CheckpointMetadata` from the incoming message CheckpointMetadata < Double > checkpoint = CheckpointMetadata . fromMessage ( record ); // `CheckpointMetadata` allows transforming the processing state // Applies the given function, starting from the value `0.0` when no previous state exists checkpoint . transform ( 0.0 , current -> current + record . getPayload (), /* persistOnAck */ true ); // `persistOnAck` flag set to true, ack will persist the processing state // associated with the latest offset (per partition). return record . ack (); } The transform method allows applying a transformation function to the current state, producing a changed state and registering it locally for checkpointing. By default, the local state is synced (persisted) to the state store periodically, period specified by auto.commit.interval.ms , (default: 5000). If persistOnAck flag is given, the latest state is persisted to the state store eagerly on message acknowledgment. The setNext method works similarly directly setting the latest state. The checkpoint commit strategy tracks when a processing state is last persisted for each topic-partition. If an outstanding state change can not be persisted for checkpoint.unsynced-state-max-age.ms (default: 10000), the channel is marked unhealthy. Where and how processing states are persisted is decided by the state store implementation. This can be configured on the incoming channel using checkpoint.state-store configuration property, using the state store factory identifier name. The serialization of state objects depends on the state store implementation. In order to instruct state stores for serialization can require configuring the type name of state objects using checkpoint.state-type property. In order to keep Smallrye Reactive Messaging free of persistence-related dependencies, this library includes only a default state store named file . It is based on Vert.x Filesystem API and stores the processing state in Json formatted files, in a local directory configured by the checkpoint.file.state-dir property. State files follow the naming scheme [consumer-group-id]:[topic]:[partition] . Implementing State Stores State store implementations are required to implement CheckpointStateStore interface, and provide a managed bean implementing CheckpointStateStore.Factory , identified with @Identifier bean qualifier indicating the name of the state-store. The factory bean identifier indicates the name to configure on checkpoint.state-store config property. The factory is discovered as a CDI managed bean and state store is created once per channel: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 package kafka.inbound ; import java.util.Collection ; import java.util.Map ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.ConsumerConfig ; import org.apache.kafka.common.TopicPartition ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration ; import io.smallrye.reactive.messaging.kafka.KafkaConsumer ; import io.smallrye.reactive.messaging.kafka.commit.CheckpointStateStore ; import io.smallrye.reactive.messaging.kafka.commit.ProcessingState ; import io.vertx.mutiny.core.Vertx ; public class MyCheckpointStateStore implements CheckpointStateStore { private final String consumerGroupId ; private final Class <?> stateType ; public MyCheckpointStateStore ( String consumerGroupId , Class <?> stateType ) { this . consumerGroupId = consumerGroupId ; this . stateType = stateType ; } /** * Can be used with * {@code mp.reactive.messaging.incoming.my-channel.commit-strategy=checkpoint} * {@code mp.reactive.messaging.incoming.my-channel.checkpoint.state-store=my-store} */ @ApplicationScoped @Identifier ( \"my-store\" ) public static class Factory implements CheckpointStateStore . Factory { @Override public CheckpointStateStore create ( KafkaConnectorIncomingConfiguration config , Vertx vertx , KafkaConsumer <? , ?> consumer , Class <?> stateType /* if configured, otherwise null */ ) { String consumerGroupId = ( String ) consumer . configuration (). get ( ConsumerConfig . GROUP_ID_CONFIG ); return new MyCheckpointStateStore ( consumerGroupId , stateType ); } } @Override public Uni < Map < TopicPartition , ProcessingState <?>>> fetchProcessingState ( Collection < TopicPartition > partitions ) { // Called on Vert.x event loop // Return a Uni completing with the map of topic-partition to processing state // The Uni will be subscribed also on Vert.x event loop return Uni . createFrom (). nullItem (); } @Override public Uni < Void > persistProcessingState ( Map < TopicPartition , ProcessingState <?>> state ) { // Called on Vert.x event loop // Return a Uni completing with void when the given states are persisted // The Uni will be subscribed also on Vert.x event loop return Uni . createFrom (). voidItem (); } @Override public void close () { /* Called when channel is closing, no-op by default */ } } The checkpoint commit strategy calls the state store in following events: fetchProcessingState : on partitions assigned, to seek the consumer to the latest offset. persistProcessingState on partitions revoked, to persist the state of last processed record. persistProcessingState on message acknowledgement, if a new state is set during the processing and persistOnAck flag is set. persistProcessingState on auto.commit.interval.ms intervals, if a new state is set during processing. persistProcessingState on channel shutdown. close on channel shutdown. Configuration Reference Attribute ( alias ) Description Type Mandatory Default auto.offset.reset What to do when there is no initial offset in Kafka.Accepted values are earliest, latest and none string false latest batch Whether the Kafka records are consumed in batch. The channel injection point must consume a compatible type, such as List<Payload> or KafkaRecordBatch<Payload> . boolean false false bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false localhost:9092 broadcast Whether the Kafka records should be dispatched to multiple consumer boolean false false checkpoint.state-store While using the checkpoint commit-strategy, the name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.StateStore.Factory to specify the state store implementation. string false checkpoint.state-type While using the checkpoint commit-strategy, the fully qualified type name of the state object to persist in the state store. When provided, it can be used by the state store implementation to help persisting the processing state object. string false checkpoint.unsynced-state-max-age.ms While using the checkpoint commit-strategy, specify the max age in milliseconds that the processing state must be persisted before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false 10000 client-id-prefix Prefix for Kafka client client.id attribute. If defined configured or generated client.id will be prefixed with the given value. string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true commit-strategy Specify the commit strategy to apply when a message produced from a record is acknowledged. Values can be latest , ignore or throttled . If enable.auto.commit is true then the default is ignore otherwise it is throttled string false consumer-rebalance-listener.name The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener . If set, this rebalance listener is applied to the consumer. string false dead-letter-queue.key.serializer When the failure-strategy is set to dead-letter-queue indicates the key serializer to use. If not set the serializer associated to the key deserializer is used string false dead-letter-queue.producer-client-id When the failure-strategy is set to dead-letter-queue indicates what client id the generated producer should use. Defaults is kafka-dead-letter-topic-producer-$client-id string false dead-letter-queue.topic When the failure-strategy is set to dead-letter-queue indicates on which topic the record is sent. Defaults is dead-letter-topic-$channel string false dead-letter-queue.value.serializer When the failure-strategy is set to dead-letter-queue indicates the value serializer to use. If not set the serializer associated to the value deserializer is used string false enable.auto.commit If enabled, consumer's offset will be periodically committed in the background by the underlying Kafka client, ignoring the actual processing outcome of the records. It is recommended to NOT enable this setting and let Reactive Messaging handles the commit. boolean false false fail-on-deserialization-failure When no deserialization failure handler is set and a deserialization failure happens, report the failure and mark the application as unhealthy. If set to false and a deserialization failure happens, a null value is forwarded. boolean false true failure-strategy Specify the failure strategy to apply when a message produced from a record is acknowledged negatively (nack). Values can be fail (default), ignore , or dead-letter-queue string false fail fetch.min.bytes The minimum amount of data the server should return for a fetch request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. int false 1 graceful-shutdown Whether or not a graceful shutdown should be attempted when the application terminates. boolean false true group.id A unique string that identifies the consumer group the application belongs to. If not set, a unique, generated id is used string false health-enabled Whether health reporting is enabled (default) or disabled boolean false true health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false true health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false false health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false 2000 kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key-deserialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler . If set, deserialization failure happening when deserializing keys are delegated to this handler which may retry or provide a fallback value. string false key.deserializer The deserializer classname used to deserialize the record's key string false org.apache.kafka.common.serialization.StringDeserializer lazy-client Whether Kafka client is created lazily or eagerly. boolean false false max-queue-size-factor Multiplier factor to determine maximum number of records queued for processing, using max.poll.records * max-queue-size-factor . Defaults to 2. In batch mode max.poll.records is considered 1 . int false 2 partitions The number of partitions to be consumed concurrently. The connector creates the specified amount of Kafka consumers. It should match the number of partition of the targeted topic int false 1 pattern Indicate that the topic property is a regular expression. Must be used with the topic property. Cannot be used with the topics property boolean false false pause-if-no-requests Whether the polling must be paused when the application does not request items and resume when it does. This allows implementing back-pressure based on the application capacity. Note that polling is not stopped, but will not retrieve any records when paused. boolean false true poll-timeout The polling timeout in milliseconds. When polling records, the poll will wait at most that duration before returning records. Default is 1000ms int false 1000 requests When partitions is greater than 1, this attribute allows configuring how many records are requested by each consumers every time. int false 128 retry Whether or not the connection to the broker is re-attempted in case of failure boolean false true retry-attempts The maximum number of reconnection before failing. -1 means infinite retry int false -1 retry-max-wait The max delay (in seconds) between 2 reconnects int false 30 throttled.unprocessed-record-max-age.ms While using the throttled commit-strategy, specify the max age in milliseconds that an unprocessed message can be before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false 60000 topic The consumed / populated Kafka topic. If neither this property nor the topics properties are set, the channel name is used string false topics A comma-separating list of topics to be consumed. Cannot be used with the topic or pattern properties string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true value-deserialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler . If set, deserialization failure happening when deserializing values are delegated to this handler which may retry or provide a fallback value. string false value.deserializer The deserializer classname used to deserialize the record's value string true You can also pass any property supported by the underlying Kafka consumer . For example, to configure the max.poll.records property, use: 1 mp.messaging.incoming.[channel].max.poll.records = 1000 Some consumer client properties are configured to sensible default values: If not set, reconnect.backoff.max.ms is set to 10000 to avoid high load on disconnection. If not set, key.deserializer is set to org.apache.kafka.common.serialization.StringDeserializer . The consumer client.id is configured according to the number of clients to create using mp.messaging.incoming.[channel].partitions property. If a client.id is provided, it is used as-is or suffixed with client index if partitions property is set. If a client.id is not provided, it is generated as kafka-consumer-[channel][-index] .","title":"Receiving records"},{"location":"kafka/receiving-kafka-records/#receiving-kafka-records","text":"The Kafka Connector retrieves Kafka Records from Kafka Brokers and maps each of them to Reactive Messaging Messages .","title":"Receiving Kafka Records"},{"location":"kafka/receiving-kafka-records/#example","text":"Let\u2019s imagine you have a Kafka broker running, and accessible using the kafka:9092 address (by default it would use localhost:9092 ). Configure your application to receive Kafka records from a Kafka topic on the prices channel as follows: 1 2 3 4 5 kafka.bootstrap.servers = kafka:9092 # <1> mp.messaging.incoming.prices.connector = smallrye-kafka # <2> mp.messaging.incoming.prices.value.deserializer = org.apache.kafka.common.serialization.DoubleDeserializer # <3> mp.messaging.incoming.prices.broadcast = true # <4> Configure the broker location. You can configure it globally or per channel Configure the connector to manage the prices channel Sets the (Kafka) deserializer to read the record\u2019s value Make sure that we can receive from more than one consumer (see KafkaPriceConsumer and KafkaPriceMessageConsumer below) Note You don\u2019t need to set the Kafka topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. Then, your application receives Message<Double> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package kafka.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class KafkaPriceConsumer { @Incoming ( \"prices\" ) public void consume ( double price ) { // process your price. } } Or, you can retrieve the Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package kafka.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class KafkaPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < Double > price ) { // process your price. // Acknowledge the incoming message (commit the offset) return price . ack (); } }","title":"Example"},{"location":"kafka/receiving-kafka-records/#deserialization","text":"The deserialization is handled by the underlying Kafka Client. You need to configure the: mp.messaging.incoming.[channel-name].value.deserializer to configure the value deserializer (mandatory) mp.messaging.incoming.[channel-name].key.deserializer to configure the key deserializer (optional, default to String ) If you want to use a custom deserializer, add it to your CLASSPATH and configure the associate attribute. In addition, the Kafka Connector also provides a set of message converters . So you can receive payloads representing records from Kafka using: Record - a pair key/value ConsumerRecord - a structure representing the record with all its metadata 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Incoming ( \"topic-a\" ) public void consume ( Record < String , String > record ) { String key = record . key (); // Can be `null` if the incoming record has no key String value = record . value (); // Can be `null` if the incoming record has no value } @Incoming ( \"topic-b\" ) public void consume ( ConsumerRecord < String , String > record ) { String key = record . key (); // Can be `null` if the incoming record has no key String value = record . value (); // Can be `null` if the incoming record has no value String topic = record . topic (); int partition = record . partition (); // ... }","title":"Deserialization"},{"location":"kafka/receiving-kafka-records/#inbound-metadata","text":"Messages coming from Kafka contains an instance of IncomingKafkaRecordMetadata in the metadata. It provides the key, topic, partitions, headers and so on: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 IncomingKafkaRecordMetadata < String , Double > metadata = incoming . getMetadata ( IncomingKafkaRecordMetadata . class ) . orElse ( null ); if ( metadata != null ) { // The topic String topic = metadata . getTopic (); // The key String key = metadata . getKey (); // The timestamp Instant timestamp = metadata . getTimestamp (); // The underlying record ConsumerRecord < String , Double > record = metadata . getRecord (); // ... }","title":"Inbound Metadata"},{"location":"kafka/receiving-kafka-records/#acknowledgement","text":"When a message produced from a Kafka record is acknowledged, the connector invokes a commit strategy . These strategies decide when the consumer offset for a specific topic/partition is committed. Committing an offset indicates that all previous records have been processed. It is also the position where the application would restart the processing after a crash recovery or a restart. Committing every offset has performance penalties as Kafka offset management can be slow. However, not committing the offset often enough may lead to message duplication if the application crashes between two commits. The Kafka connector supports three strategies: throttled keeps track of received messages and commit to the next offset after the latest acked message in sequence. This strategy guarantees at-least-once delivery even if the channel performs asynchronous processing. The connector tracks the received records and periodically (period specified by auto.commit.interval.ms (default: 5000)) commits the highest consecutive offset. The connector will be marked as unhealthy if a message associated with a record is not acknowledged in throttled.unprocessed-record-max-age.ms (default: 60000). Indeed, this strategy cannot commit the offset as soon as a single record processing fails (see failure-strategy to configure what happens on failing processing). If throttled.unprocessed-record-max-age.ms is set to less than or equal to 0, it does not perform any health check verification. Such a setting might lead to running out of memory if there are poison pill messages. This strategy is the default if enable.auto.commit is not explicitly set to true . checkpoint allows persisting consumer offsets on a \"state store\", instead of committing them back to the Kafka broker. Using the CheckpointMetadata API, consumer code can persist a processing state with the offset to mark the progress of a consumer. When the processing continues from a previously persisted offset, it seeks the Kafka consumer to that offset and also restores the persisted state, continuing the stateful processing from where it left off. The checkpoint strategy holds locally the processing state associated with the latest offset, and persists it periodically to the state store (period specified by auto.commit.interval.ms (default: 5000)). The connector will be marked as unhealthy if no processing state is persisted to the state store in checkpoint.unsynced-state-max-age.ms (default: 10000). Using the CheckpointMetadata API the user code can force to persist the state on message ack. If checkpoint.unsynced-state-max-age.ms is set to less than or equal to 0, it does not perform any health check verification. For more information, see Stateful processing with Checkpointing latest commits the record offset received by the Kafka consumer as soon as the associated message is acknowledged (if the offset is higher than the previously committed offset). This strategy provides at-least-once delivery if the channel processes the message without performing any asynchronous processing. This strategy should not be used on high-load as offset commit is expensive. However, it reduces the risk of duplicates. ignore performs no commit. This strategy is the default strategy when the consumer is explicitly configured with enable.auto.commit to true . It delegates the offset commit to the Kafka client. When enable.auto.commit is true this strategy DOES NOT guarantee at-least-once delivery. However, if the processing failed between two commits, messages received after the commit and before the failure will be re-processed. Important The Kafka connector disables the Kafka auto commit if not explicitly enabled. This behavior differs from the traditional Kafka consumer. If high-throughout is important for you, and not limited by the downstream, we recommend to either: Use the throttled policy or set enable.auto.commit to true and annotate the consuming method with @Acknowledgment(Acknowledgment.Strategy.NONE)","title":"Acknowledgement"},{"location":"kafka/receiving-kafka-records/#failure-management","text":"If a message produced from a Kafka record is nacked , a failure strategy is applied. The Kafka connector supports 3 strategies: fail - fail the application, no more records will be processed. (default) The offset of the record that has not been processed correctly is not committed. ignore - the failure is logged, but the processing continue. The offset of the record that has not been processed correctly is committed. dead-letter-queue - the offset of the record that has not been processed correctly is committed, but the record is written to a (Kafka) dead letter queue topic. The strategy is selected using the failure-strategy attribute. In the case of dead-letter-queue , you can configure the following attributes: dead-letter-queue.topic : the topic to use to write the records not processed correctly, default is dead-letter-topic-$channel , with $channel being the name of the channel. dead-letter-queue.producer-client-id : the client id used by the kafka producer when sending records to dead letter queue topic. If not specified it will default to kafka-dead-letter-topic-producer-$client-id , with $client-id being the value obtained from consumer client id. dead-letter-queue.key.serializer : the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer. dead-letter-queue.value.serializer : the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer. The record written on the dead letter topic contains the original record\u2019s headers, as well as a set of additional headers about the original record: dead-letter-reason - the reason of the failure (the Throwable passed to nack() ) dead-letter-cause - the cause of the failure (the getCause() of the Throwable passed to nack() ), if any dead-letter-topic - the original topic of the record dead-letter-partition - the original partition of the record (integer mapped to String) dead-letter-offset - the original offset of the record (long mapped to String) When using dead-letter-queue , it is also possible to change some metadata of the record that is sent to the dead letter topic. To do that, use the Message.nack(Throwable, Metadata) method: 1 2 3 4 5 6 7 8 9 @Incoming ( \"in\" ) public CompletionStage < Void > consume ( KafkaRecord < String , String > message ) { return message . nack ( new Exception ( \"Failed!\" ), Metadata . of ( OutgoingKafkaRecordMetadata . builder () . withKey ( \"failed-record\" ) . withHeaders ( new RecordHeaders () . add ( \"my-header\" , \"my-header-value\" . getBytes ( StandardCharsets . UTF_8 ))) . build ())); } The Metadata may contain an instance of OutgoingKafkaRecordMetadata . If the instance is present, the following properties will be used: key; if not present, the original record\u2019s key will be used topic; if not present, the configured dead letter topic will be used partition; if not present, partition will be assigned automatically headers; combined with the original record\u2019s headers, as well as the dead-letter-* headers described above","title":"Failure Management"},{"location":"kafka/receiving-kafka-records/#custom-commit-and-failure-strategies","text":"In addition to provided strategies, it is possible to implement custom commit and failure strategies and configure Kafka channels with them. For example, for a custom commit strategy, implement the KafkaCommitHandler interface, and provide a managed bean implementing the KafkaCommitHandler.Factory interface, identified using @Identifier qualifier. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 package kafka.inbound ; import java.util.Collection ; import java.util.function.BiConsumer ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.common.TopicPartition ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; import io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration ; import io.smallrye.reactive.messaging.kafka.KafkaConsumer ; import io.smallrye.reactive.messaging.kafka.commit.KafkaCommitHandler ; import io.vertx.mutiny.core.Vertx ; public class KafkaCustomCommit implements KafkaCommitHandler { @Override public < K , V > Uni < Void > handle ( IncomingKafkaRecord < K , V > record ) { // called on message ack return Uni . createFrom (). voidItem (); } @Override public < K , V > Uni < IncomingKafkaRecord < K , V >> received ( IncomingKafkaRecord < K , V > record ) { // called before message processing return Uni . createFrom (). item ( record ); } @Override public void terminate ( boolean graceful ) { // called on channel shutdown } @Override public void partitionsAssigned ( Collection < TopicPartition > partitions ) { // called on partitions assignment } @Override public void partitionsRevoked ( Collection < TopicPartition > partitions ) { // called on partitions revoked } @ApplicationScoped @Identifier ( \"custom\" ) public static class Factory implements KafkaCommitHandler . Factory { @Override public KafkaCommitHandler create ( KafkaConnectorIncomingConfiguration config , Vertx vertx , KafkaConsumer <? , ?> consumer , BiConsumer < Throwable , Boolean > reportFailure ) { return new KafkaCustomCommit ( /* ... */ ); } } } Finally, to use the custom commit strategy, set the commit-strategy attribute to the identifier of the commit handler factory: mp.messaging.incoming.$channel.commit-strategy=custom . Similarly, custom failure strategies can be configured using failure-strategy attribute. Note If the custom strategy implementation inherits ContextHolder class it can access the Vert.x event-loop context created for the Kafka consumer","title":"Custom commit and failure strategies"},{"location":"kafka/receiving-kafka-records/#retrying-processing","text":"You can combine Reactive Messaging with SmallRye Fault Tolerance , and retry processing when it fails: 1 2 3 4 5 6 @Incoming ( \"kafka\" ) @Outgoing ( \"processed\" ) @Retry ( delay = 10 , maxRetries = 5 ) public String process ( String v ) { // ... retry if this method throws an exception } You can configure the delay, the number of retries, the jitter... If your method returns a Uni , you need to add the @NonBlocking annotation: 1 2 3 4 5 6 7 @Incoming ( \"kafka\" ) @Outgoing ( \"processed\" ) @Retry ( delay = 10 , maxRetries = 5 ) @NonBlocking public Uni < String > process ( String v ) { // ... retry if this method throws an exception or the returned Uni produce a failure } The incoming messages are acknowledged only once the processing completes successfully. So, it commits the offset after the successful processing. If after the retries the processing still failed, the message is nacked and the failure strategy is applied. You can also use @Retry on methods only consuming incoming messages: 1 2 3 4 5 @Incoming ( \"kafka\" ) @Retry ( delay = 10 , maxRetries = 5 ) public void consume ( String v ) { // ... retry if this method throws an exception }","title":"Retrying processing"},{"location":"kafka/receiving-kafka-records/#handling-deserialization-failures","text":"Because deserialization happens before creating a Message , the failure strategy presented above cannot be applied. However, when a deserialization failure occurs, you can intercept it and provide a fallback value. To achieve this, create a CDI bean implementing the DeserializationFailureHandler interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ApplicationScoped @Identifier ( \"failure-retry\" ) // Set the name of the failure handler public class MyDeserializationFailureHandler implements DeserializationFailureHandler < JsonObject > { // Specify the expected type @Override public JsonObject decorateDeserialization ( Uni < JsonObject > deserialization , String topic , boolean isKey , String deserializer , byte [] data , Headers headers ) { return deserialization . onFailure (). retry (). atMost ( 3 ) . await (). atMost ( Duration . ofMillis ( 200 )); } } The bean must be exposed with the @Identifier qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute: mp.messaging.incoming.$channel.key-deserialization-failure-handler : name of the bean handling deserialization failures happening for the record\u2019s key mp.messaging.incoming.$channel.value-deserialization-failure-handler : name of the bean handling deserialization failures happening for the record\u2019s value, The handler is called with the deserialization action as a Uni<T> , the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers augmented with headers describing the failure (which ease the write to a dead letter). On the deserialization Uni failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the deserialized object. Alternatively, the handler can only implement handleDeserializationFailure method and provide a fallback value, which may be null . If you don\u2019t configure a deserialization failure handlers and a deserialization failure happens, the application is marked unhealthy. You can also ignore the failure, which will log the exception and produce a null value. To enable this behavior, set the mp.messaging.incoming.$channel.fail-on-deserialization-failure attribute to false .","title":"Handling deserialization failures"},{"location":"kafka/receiving-kafka-records/#receiving-cloud-events","text":"The Kafka connector supports Cloud Events . When the connector detects a structured or binary Cloud Events, it adds a IncomingKafkaCloudEventMetadata in the metadata of the Message. IncomingKafkaCloudEventMetadata contains the various (mandatory and optional) Cloud Event attributes. If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.","title":"Receiving Cloud Events"},{"location":"kafka/receiving-kafka-records/#binary-cloud-events","text":"For binary Cloud Events, all mandatory Cloud Event attributes must be set in the record header, prefixed by ce_ (as mandated by the protocol binding ). The connector considers headers starting with the ce_ prefix but not listed in the specification as extensions. You can access them using the getExtension method from IncomingKafkaCloudEventMetadata . You can retrieve them as String . The datacontenttype attribute is mapped to the content-type header of the record. The partitionkey attribute is mapped to the record\u2019s key, if any. Note that all headers are read as UTF-8. With binary Cloud Events, the record\u2019s key and value can use any deserializer.","title":"Binary Cloud Events"},{"location":"kafka/receiving-kafka-records/#structured-cloud-events","text":"For structured Cloud Events, the event is encoded in the record\u2019s value. Only JSON is supported, so your event must be encoded as JSON in the record\u2019s value. Structured Cloud Event must set the content-type header of the record to application/cloudevents or prefix the value with application/cloudevents such as: application/cloudevents+json; charset=UTF-8 . To receive structured Cloud Events, your value deserializer must be: org.apache.kafka.common.serialization.StringDeserializer org.apache.kafka.common.serialization.ByteArrayDeserializer io.vertx.kafka.client.serialization.JsonObjectDeserializer As mentioned previously, the value must be a valid JSON object containing at least all the mandatory Cloud Events attributes. If the record is a structured Cloud Event, the created Message\u2019s payload is the Cloud Event data . The partitionkey attribute is mapped to the record\u2019s key if any.","title":"Structured Cloud Events"},{"location":"kafka/receiving-kafka-records/#consumer-rebalance-listener","text":"To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener. To achieve this, implement the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface, make the implementing class a bean, and add the @Identifier qualifier. A usual use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset. The listener is invoked every time the consumer topic/partition assignment changes. For example, when the application starts, it invokes the partitionsAssigned callback with the initial set of topics/partitions associated with the consumer. If, later, this set changes, it calls the partitionsRevoked and partitionsAssigned callbacks again, so you can implement custom logic. Note that the rebalance listener methods are called from the Kafka polling thread and must block the caller thread until completion. That\u2019s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier. When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and restarts once the rebalance completes. If the rebalance listener handles offset commit on behalf of the user (using the ignore commit strategy), the rebalance listener must commit the offset synchronously in the partitionsRevoked callback. We also recommend applying the same logic when the application stops. Unlike the ConsumerRebalanceListener from Apache Kafka, the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener methods pass the Kafka Consumer and the set of topics/partitions.","title":"Consumer Rebalance Listener"},{"location":"kafka/receiving-kafka-records/#example_1","text":"In this example we set-up a consumer that always starts on messages from at most 10 minutes ago (or offset 0). First we need to provide a bean that implements the io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener interface and is annotated with @Identifier . We then must configure our inbound connector to use this named bean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package kafka.inbound ; import java.util.Collection ; import java.util.HashMap ; import java.util.Map ; import java.util.logging.Logger ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.Consumer ; import org.apache.kafka.clients.consumer.OffsetAndTimestamp ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener ; @ApplicationScoped @Identifier ( \"rebalanced-example.rebalancer\" ) public class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener { private static final Logger LOGGER = Logger . getLogger ( KafkaRebalancedConsumerRebalanceListener . class . getName ()); /** * When receiving a list of partitions will search for the earliest offset within 10 minutes * and seek the consumer to it. * * @param consumer underlying consumer * @param partitions set of assigned topic partitions */ @Override public void onPartitionsAssigned ( Consumer <? , ?> consumer , Collection < org . apache . kafka . common . TopicPartition > partitions ) { long now = System . currentTimeMillis (); long shouldStartAt = now - 600_000L ; //10 minute ago Map < org . apache . kafka . common . TopicPartition , Long > request = new HashMap <> (); for ( org . apache . kafka . common . TopicPartition partition : partitions ) { LOGGER . info ( \"Assigned \" + partition ); request . put ( partition , shouldStartAt ); } Map < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > offsets = consumer . offsetsForTimes ( request ); for ( Map . Entry < org . apache . kafka . common . TopicPartition , OffsetAndTimestamp > position : offsets . entrySet ()) { long target = position . getValue () == null ? 0 L : position . getValue (). offset (); LOGGER . info ( \"Seeking position \" + target + \" for \" + position . getKey ()); consumer . seek ( position . getKey (), target ); } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package kafka.inbound ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Acknowledgment ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord ; @ApplicationScoped public class KafkaRebalancedConsumer { @Incoming ( \"rebalanced-example\" ) @Acknowledgment ( Acknowledgment . Strategy . NONE ) public CompletionStage < Void > consume ( IncomingKafkaRecord < Integer , String > message ) { // We don't need to ACK messages because in this example we set offset during consumer re-balance return CompletableFuture . completedFuture ( null ); } } To configure the inbound connector to use the provided listener we either set the consumer rebalance listener\u2019s name: mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer Or have the listener\u2019s name be the same as the group id: mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer Setting the consumer rebalance listener\u2019s name takes precedence over using the group id.","title":"Example"},{"location":"kafka/receiving-kafka-records/#receiving-kafka-records-in-batches","text":"By default, incoming methods receive each Kafka record individually. Under the hood, Kafka consumer clients poll the broker constantly and receive records in batches, presented inside the ConsumerRecords container. In batch mode, your application can receive all the records returned by the consumer poll in one go. To achieve this you need to set mp.messaging.incoming.$channel.batch=true and specify a compatible container type to receive all the data: 1 2 3 4 5 6 @Incoming ( \"prices\" ) public void consume ( List < Double > prices ) { for ( double price : prices ) { // process price } } The incoming method can also receive Message<List<Payload> , KafkaBatchRecords<Payload> ConsumerRecords<Key, Payload> types, They give access to record details such as offset or timestamp : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Incoming ( \"prices\" ) public CompletionStage < Void > consumeMessage ( KafkaRecordBatch < String , Double > records ) { for ( KafkaRecord < String , Double > record : records ) { record . getMetadata ( IncomingKafkaRecordMetadata . class ). ifPresent ( metadata -> { int partition = metadata . getPartition (); long offset = metadata . getOffset (); Instant timestamp = metadata . getTimestamp (); //... process messages }); } // ack will commit the latest offsets (per partition) of the batch. return records . ack (); } @Incoming ( \"prices\" ) public void consumeRecords ( ConsumerRecords < String , Double > records ) { for ( TopicPartition partition : records . partitions ()) { for ( ConsumerRecord < String , Double > record : records . records ( partition )) { //... process messages } } } Note that the successful processing of the incoming record batch will commit the latest offsets for each partition received inside the batch. The configured commit strategy will be applied for these records only. Conversely, if the processing throws an exception, all messages are nacked , applying the failure strategy for all the records inside the batch.","title":"Receiving Kafka Records in Batches"},{"location":"kafka/receiving-kafka-records/#stateful-processing-with-checkpointing","text":"Experimental Checkpointing is experimental, and APIs and features are subject to change in the future. The checkpoint commit strategy allows for a Kafka incoming channel to manage topic-partition offsets, not by committing on the Kafka broker, but by persisting consumers' advancement on a state store . In addition to that, if the consumer builds an internal state as a result of consumed records, the topic-partition offset persisted to the state store can be associated with a processing state , saving the local state to the persistent store. When a consumer restarts or consumer group instances scale, i.e. when new partitions get assigned to the consumer, the checkpointing works by resuming the processing from the latest offset and its saved state. The @Incoming channel consumer code can manipulate the processing state through the CheckpointMetadata API: 1 2 3 4 5 6 7 8 9 10 11 12 13 @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( KafkaRecord < String , Double > record ) { // Get the `CheckpointMetadata` from the incoming message CheckpointMetadata < Double > checkpoint = CheckpointMetadata . fromMessage ( record ); // `CheckpointMetadata` allows transforming the processing state // Applies the given function, starting from the value `0.0` when no previous state exists checkpoint . transform ( 0.0 , current -> current + record . getPayload (), /* persistOnAck */ true ); // `persistOnAck` flag set to true, ack will persist the processing state // associated with the latest offset (per partition). return record . ack (); } The transform method allows applying a transformation function to the current state, producing a changed state and registering it locally for checkpointing. By default, the local state is synced (persisted) to the state store periodically, period specified by auto.commit.interval.ms , (default: 5000). If persistOnAck flag is given, the latest state is persisted to the state store eagerly on message acknowledgment. The setNext method works similarly directly setting the latest state. The checkpoint commit strategy tracks when a processing state is last persisted for each topic-partition. If an outstanding state change can not be persisted for checkpoint.unsynced-state-max-age.ms (default: 10000), the channel is marked unhealthy. Where and how processing states are persisted is decided by the state store implementation. This can be configured on the incoming channel using checkpoint.state-store configuration property, using the state store factory identifier name. The serialization of state objects depends on the state store implementation. In order to instruct state stores for serialization can require configuring the type name of state objects using checkpoint.state-type property. In order to keep Smallrye Reactive Messaging free of persistence-related dependencies, this library includes only a default state store named file . It is based on Vert.x Filesystem API and stores the processing state in Json formatted files, in a local directory configured by the checkpoint.file.state-dir property. State files follow the naming scheme [consumer-group-id]:[topic]:[partition] .","title":"Stateful processing with Checkpointing"},{"location":"kafka/receiving-kafka-records/#implementing-state-stores","text":"State store implementations are required to implement CheckpointStateStore interface, and provide a managed bean implementing CheckpointStateStore.Factory , identified with @Identifier bean qualifier indicating the name of the state-store. The factory bean identifier indicates the name to configure on checkpoint.state-store config property. The factory is discovered as a CDI managed bean and state store is created once per channel: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 package kafka.inbound ; import java.util.Collection ; import java.util.Map ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.consumer.ConsumerConfig ; import org.apache.kafka.common.TopicPartition ; import io.smallrye.common.annotation.Identifier ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaConnectorIncomingConfiguration ; import io.smallrye.reactive.messaging.kafka.KafkaConsumer ; import io.smallrye.reactive.messaging.kafka.commit.CheckpointStateStore ; import io.smallrye.reactive.messaging.kafka.commit.ProcessingState ; import io.vertx.mutiny.core.Vertx ; public class MyCheckpointStateStore implements CheckpointStateStore { private final String consumerGroupId ; private final Class <?> stateType ; public MyCheckpointStateStore ( String consumerGroupId , Class <?> stateType ) { this . consumerGroupId = consumerGroupId ; this . stateType = stateType ; } /** * Can be used with * {@code mp.reactive.messaging.incoming.my-channel.commit-strategy=checkpoint} * {@code mp.reactive.messaging.incoming.my-channel.checkpoint.state-store=my-store} */ @ApplicationScoped @Identifier ( \"my-store\" ) public static class Factory implements CheckpointStateStore . Factory { @Override public CheckpointStateStore create ( KafkaConnectorIncomingConfiguration config , Vertx vertx , KafkaConsumer <? , ?> consumer , Class <?> stateType /* if configured, otherwise null */ ) { String consumerGroupId = ( String ) consumer . configuration (). get ( ConsumerConfig . GROUP_ID_CONFIG ); return new MyCheckpointStateStore ( consumerGroupId , stateType ); } } @Override public Uni < Map < TopicPartition , ProcessingState <?>>> fetchProcessingState ( Collection < TopicPartition > partitions ) { // Called on Vert.x event loop // Return a Uni completing with the map of topic-partition to processing state // The Uni will be subscribed also on Vert.x event loop return Uni . createFrom (). nullItem (); } @Override public Uni < Void > persistProcessingState ( Map < TopicPartition , ProcessingState <?>> state ) { // Called on Vert.x event loop // Return a Uni completing with void when the given states are persisted // The Uni will be subscribed also on Vert.x event loop return Uni . createFrom (). voidItem (); } @Override public void close () { /* Called when channel is closing, no-op by default */ } } The checkpoint commit strategy calls the state store in following events: fetchProcessingState : on partitions assigned, to seek the consumer to the latest offset. persistProcessingState on partitions revoked, to persist the state of last processed record. persistProcessingState on message acknowledgement, if a new state is set during the processing and persistOnAck flag is set. persistProcessingState on auto.commit.interval.ms intervals, if a new state is set during processing. persistProcessingState on channel shutdown. close on channel shutdown.","title":"Implementing State Stores"},{"location":"kafka/receiving-kafka-records/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default auto.offset.reset What to do when there is no initial offset in Kafka.Accepted values are earliest, latest and none string false latest batch Whether the Kafka records are consumed in batch. The channel injection point must consume a compatible type, such as List<Payload> or KafkaRecordBatch<Payload> . boolean false false bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false localhost:9092 broadcast Whether the Kafka records should be dispatched to multiple consumer boolean false false checkpoint.state-store While using the checkpoint commit-strategy, the name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.StateStore.Factory to specify the state store implementation. string false checkpoint.state-type While using the checkpoint commit-strategy, the fully qualified type name of the state object to persist in the state store. When provided, it can be used by the state store implementation to help persisting the processing state object. string false checkpoint.unsynced-state-max-age.ms While using the checkpoint commit-strategy, specify the max age in milliseconds that the processing state must be persisted before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false 10000 client-id-prefix Prefix for Kafka client client.id attribute. If defined configured or generated client.id will be prefixed with the given value. string false cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true commit-strategy Specify the commit strategy to apply when a message produced from a record is acknowledged. Values can be latest , ignore or throttled . If enable.auto.commit is true then the default is ignore otherwise it is throttled string false consumer-rebalance-listener.name The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener . If set, this rebalance listener is applied to the consumer. string false dead-letter-queue.key.serializer When the failure-strategy is set to dead-letter-queue indicates the key serializer to use. If not set the serializer associated to the key deserializer is used string false dead-letter-queue.producer-client-id When the failure-strategy is set to dead-letter-queue indicates what client id the generated producer should use. Defaults is kafka-dead-letter-topic-producer-$client-id string false dead-letter-queue.topic When the failure-strategy is set to dead-letter-queue indicates on which topic the record is sent. Defaults is dead-letter-topic-$channel string false dead-letter-queue.value.serializer When the failure-strategy is set to dead-letter-queue indicates the value serializer to use. If not set the serializer associated to the value deserializer is used string false enable.auto.commit If enabled, consumer's offset will be periodically committed in the background by the underlying Kafka client, ignoring the actual processing outcome of the records. It is recommended to NOT enable this setting and let Reactive Messaging handles the commit. boolean false false fail-on-deserialization-failure When no deserialization failure handler is set and a deserialization failure happens, report the failure and mark the application as unhealthy. If set to false and a deserialization failure happens, a null value is forwarded. boolean false true failure-strategy Specify the failure strategy to apply when a message produced from a record is acknowledged negatively (nack). Values can be fail (default), ignore , or dead-letter-queue string false fail fetch.min.bytes The minimum amount of data the server should return for a fetch request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. int false 1 graceful-shutdown Whether or not a graceful shutdown should be attempted when the application terminates. boolean false true group.id A unique string that identifies the consumer group the application belongs to. If not set, a unique, generated id is used string false health-enabled Whether health reporting is enabled (default) or disabled boolean false true health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false true health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false false health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false 2000 kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key-deserialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler . If set, deserialization failure happening when deserializing keys are delegated to this handler which may retry or provide a fallback value. string false key.deserializer The deserializer classname used to deserialize the record's key string false org.apache.kafka.common.serialization.StringDeserializer lazy-client Whether Kafka client is created lazily or eagerly. boolean false false max-queue-size-factor Multiplier factor to determine maximum number of records queued for processing, using max.poll.records * max-queue-size-factor . Defaults to 2. In batch mode max.poll.records is considered 1 . int false 2 partitions The number of partitions to be consumed concurrently. The connector creates the specified amount of Kafka consumers. It should match the number of partition of the targeted topic int false 1 pattern Indicate that the topic property is a regular expression. Must be used with the topic property. Cannot be used with the topics property boolean false false pause-if-no-requests Whether the polling must be paused when the application does not request items and resume when it does. This allows implementing back-pressure based on the application capacity. Note that polling is not stopped, but will not retrieve any records when paused. boolean false true poll-timeout The polling timeout in milliseconds. When polling records, the poll will wait at most that duration before returning records. Default is 1000ms int false 1000 requests When partitions is greater than 1, this attribute allows configuring how many records are requested by each consumers every time. int false 128 retry Whether or not the connection to the broker is re-attempted in case of failure boolean false true retry-attempts The maximum number of reconnection before failing. -1 means infinite retry int false -1 retry-max-wait The max delay (in seconds) between 2 reconnects int false 30 throttled.unprocessed-record-max-age.ms While using the throttled commit-strategy, specify the max age in milliseconds that an unprocessed message can be before the connector is marked as unhealthy. Setting this attribute to 0 disables this monitoring. int false 60000 topic The consumed / populated Kafka topic. If neither this property nor the topics properties are set, the channel name is used string false topics A comma-separating list of topics to be consumed. Cannot be used with the topic or pattern properties string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true value-deserialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.DeserializationFailureHandler . If set, deserialization failure happening when deserializing values are delegated to this handler which may retry or provide a fallback value. string false value.deserializer The deserializer classname used to deserialize the record's value string true You can also pass any property supported by the underlying Kafka consumer . For example, to configure the max.poll.records property, use: 1 mp.messaging.incoming.[channel].max.poll.records = 1000 Some consumer client properties are configured to sensible default values: If not set, reconnect.backoff.max.ms is set to 10000 to avoid high load on disconnection. If not set, key.deserializer is set to org.apache.kafka.common.serialization.StringDeserializer . The consumer client.id is configured according to the number of clients to create using mp.messaging.incoming.[channel].partitions property. If a client.id is provided, it is used as-is or suffixed with client index if partitions property is set. If a client.id is not provided, it is generated as kafka-consumer-[channel][-index] .","title":"Configuration Reference"},{"location":"kafka/test-companion/","text":"Kafka Companion Experimental Kafka Companion is experimental and APIs are subject to change in the future. The Kafka Companion is a separate Java library for helping to test Kafka applications. It is not intended to mock Kafka, but to the contrary, connect to a Kafka broker and provide high-level features. It is not limited to the SmallRye Reactive Messaging testing, but intends to improve the testability of applications using Kafka. Some of its features: Running In-container Kafka broker for tests via strimzi-test-container . Running the Kafka broker behind a toxiproxy for simulating network issues. Running embedded Kafka Kraft broker for tests. Base classes for tests to bootstrap tests. Companion classes for easily creating tasks to produce and consume Kafka records. Writing assertions for produce and consume tasks, state of consumers, topics, offsets etc. Getting started writing tests Easiest way of starting to write Kafka tests is to extend KafkaCompanionTestBase . It starts a single-node Kafka broker for the test suite using testcontainers and creates the KafkaCompanion to connect to this broker: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class KafkaWithBaseTest extends KafkaCompanionTestBase { @Test public void testWithBase () { // companion is created by the base class // produce 100 records to messages topic ProducerTask producerTask = companion . produceIntegers () . usingGenerator ( i -> new ProducerRecord <> ( \"messages\" , i ), 100 ); long msgCount = producerTask . awaitCompletion (). count (); Assertions . assertEquals ( msgCount , 100 ); // consume 100 records from messages topic ConsumerTask < String , Integer > consumerTask = companion . consumeIntegers () . fromTopics ( \"messages\" , 100 ); ConsumerRecord < String , Integer > lastRecord = consumerTask . awaitCompletion (). getLastRecord (); Assertions . assertEquals ( lastRecord . value (), 99 ); } } KafkaCompanion can be used on its own to connect to a broker: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Create companion with bootstrap servers and API timeout (default is 10 seconds) KafkaCompanion companion = new KafkaCompanion ( \"localhost:9092\" , Duration . ofSeconds ( 5 )); // Create producer and start producer task ProducerBuilder < String , Integer > producer = companion . produceIntegers () . withClientId ( \"my-producer\" ) . withProp ( \"max.block.ms\" , \"5000\" ); producer . usingGenerator ( i -> new ProducerRecord <> ( \"topic\" , i ), 100 ); // Create consumer and start consumer task ConsumerBuilder < String , Integer > consumer = companion . consumeIntegers () . withGroupId ( \"my-group\" ) . withCommitAsyncWhen ( record -> true ); ConsumerTask < String , Integer > records = consumer . fromTopics ( \"topic\" , Duration . ofSeconds ( 10 )); // Await completion and assert consumed record count Assertions . assertEquals ( records . awaitCompletion (). count (), 100 ); There are a couple of things to note on how Kafka companion handles producers , consumers and tasks : ProducerBuilder and ConsumerBuilder lazy descriptions of with which configuration to create a producer or a consumer. ProducerTask and ConsumerTask run asynchronously on dedicated threads and are started as soon as they are created. A task terminates when it is explicitly stopped, when it's predefined duration or number of records has been produced/consumed, or when it encounters an error. An exterior thread can await on records processed, or simply on termination of the task. At a given time produced or consumed records are accessible through the task. The actual creation of the producer or consumer happens when a task is started. When the task terminates the producer or the consumer is automatically closed. For example, in the previous example: We described a producer with a client id my-producer and max.block.ms of 5 seconds. Then we created a task to produce 100 records using the generator function, without waiting for its completion. We then described a consumer with group id my-group and which commits offset synchronously on every received record. Finally, we created a task to consume records for 10 seconds and await its completion. Producing records Produce from records Produce given records: 1 2 3 4 companion . produce ( byte [] . class ). fromRecords ( new ProducerRecord <> ( \"topic1\" , \"k1\" , \"1\" . getBytes ()), new ProducerRecord <> ( \"topic1\" , \"k2\" , \"2\" . getBytes ()), new ProducerRecord <> ( \"topic1\" , \"k3\" , \"3\" . getBytes ())); Produce from generator function Produce 10 records using the generator function: 1 companion . produceIntegers (). usingGenerator ( i -> new ProducerRecord <> ( \"topic\" , i ), 10 ); Produce from CSV file Given a comma-separated file records.csv with the following content 1 2 3 messages,0,a,asdf messages,1,b,asdf messages,3,c,asdf Produce records from the file: 1 companion . produceStrings (). fromCsv ( \"records.csv\" ); Consuming records Consume from topics 1 companion . consumeIntegers (). fromTopics ( \"topic1\" , \"topic2\" ); Consume from offsets 1 2 3 4 Map < TopicPartition , Long > offsets = new HashMap <> (); offsets . put ( new TopicPartition ( \"topic1\" , 0 ), 100L ); offsets . put ( new TopicPartition ( \"topic2\" , 0 ), 100L ); companion . consumeIntegers (). fromOffsets ( offsets , Duration . ofSeconds ( 10 )); Consumer assignment and offsets During execution of the consumer task, information about the underlying consumer's topic partition assignment, position or committed offsets can be accessed. 1 2 3 4 5 6 ConsumerBuilder < String , Integer > consumer = companion . consumeIntegers () . withAutoCommit (); consumer . fromTopics ( \"topic\" ); // ... await (). untilAsserted ( consumer :: waitForAssignment ); consumer . committed ( new TopicPartition ( \"topic\" , 1 )); Registering Custom Serdes KafkaCompanion handles Serializers and Deserializers for default types such as primitives, String, ByteBuffer, UUID. Serdes for custom types can be registered to the companion object, and will be used for producer and consumer tasks: 1 2 3 4 5 6 7 8 9 10 11 12 KafkaCompanion companion = new KafkaCompanion ( \"localhost:9092\" ); // Register serde to the companion companion . registerSerde ( Orders . class , new OrdersSerializer (), new OrdersDeserializer ()); // Companion will configure consumer accordingly ConsumerTask < Integer , Orders > orders = companion . consume ( Integer . class , Orders . class ) . fromTopics ( \"orders\" , 1000 ). awaitCompletion (); for ( ConsumerRecord < Integer , Orders > order : orders ) { // ... check consumed records } Topics Create, list, describe and delete topics: 1 2 3 4 5 companion . topics (). create ( \"topic1\" , 1 ); companion . topics (). createAndWait ( \"topic2\" , 3 ); Assertions . assertEquals ( companion . topics (). list (). size (), 2 ); companion . topics (). delete ( \"topic1\" ); Consumer Groups and Offsets List topic partition offsets: 1 2 TopicPartition topicPartition = KafkaCompanion . tp ( \"topic\" , 1 ); long latestOffset = companion . offsets (). get ( topicPartition , OffsetSpec . latest ()). offset (); List, describe, alter consumer groups and their offsets: 1 2 3 4 5 6 7 Collection < ConsumerGroupListing > consumerGroups = companion . consumerGroups (). list (); for ( ConsumerGroupListing consumerGroup : consumerGroups ) { // check consumer groups } TopicPartition topicPartition = KafkaCompanion . tp ( \"topic\" , 1 ); companion . consumerGroups (). resetOffsets ( \"consumer-group\" , topicPartition );","title":"Test Companion for Kafka"},{"location":"kafka/test-companion/#kafka-companion","text":"Experimental Kafka Companion is experimental and APIs are subject to change in the future. The Kafka Companion is a separate Java library for helping to test Kafka applications. It is not intended to mock Kafka, but to the contrary, connect to a Kafka broker and provide high-level features. It is not limited to the SmallRye Reactive Messaging testing, but intends to improve the testability of applications using Kafka. Some of its features: Running In-container Kafka broker for tests via strimzi-test-container . Running the Kafka broker behind a toxiproxy for simulating network issues. Running embedded Kafka Kraft broker for tests. Base classes for tests to bootstrap tests. Companion classes for easily creating tasks to produce and consume Kafka records. Writing assertions for produce and consume tasks, state of consumers, topics, offsets etc.","title":"Kafka Companion"},{"location":"kafka/test-companion/#getting-started-writing-tests","text":"Easiest way of starting to write Kafka tests is to extend KafkaCompanionTestBase . It starts a single-node Kafka broker for the test suite using testcontainers and creates the KafkaCompanion to connect to this broker: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class KafkaWithBaseTest extends KafkaCompanionTestBase { @Test public void testWithBase () { // companion is created by the base class // produce 100 records to messages topic ProducerTask producerTask = companion . produceIntegers () . usingGenerator ( i -> new ProducerRecord <> ( \"messages\" , i ), 100 ); long msgCount = producerTask . awaitCompletion (). count (); Assertions . assertEquals ( msgCount , 100 ); // consume 100 records from messages topic ConsumerTask < String , Integer > consumerTask = companion . consumeIntegers () . fromTopics ( \"messages\" , 100 ); ConsumerRecord < String , Integer > lastRecord = consumerTask . awaitCompletion (). getLastRecord (); Assertions . assertEquals ( lastRecord . value (), 99 ); } } KafkaCompanion can be used on its own to connect to a broker: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Create companion with bootstrap servers and API timeout (default is 10 seconds) KafkaCompanion companion = new KafkaCompanion ( \"localhost:9092\" , Duration . ofSeconds ( 5 )); // Create producer and start producer task ProducerBuilder < String , Integer > producer = companion . produceIntegers () . withClientId ( \"my-producer\" ) . withProp ( \"max.block.ms\" , \"5000\" ); producer . usingGenerator ( i -> new ProducerRecord <> ( \"topic\" , i ), 100 ); // Create consumer and start consumer task ConsumerBuilder < String , Integer > consumer = companion . consumeIntegers () . withGroupId ( \"my-group\" ) . withCommitAsyncWhen ( record -> true ); ConsumerTask < String , Integer > records = consumer . fromTopics ( \"topic\" , Duration . ofSeconds ( 10 )); // Await completion and assert consumed record count Assertions . assertEquals ( records . awaitCompletion (). count (), 100 ); There are a couple of things to note on how Kafka companion handles producers , consumers and tasks : ProducerBuilder and ConsumerBuilder lazy descriptions of with which configuration to create a producer or a consumer. ProducerTask and ConsumerTask run asynchronously on dedicated threads and are started as soon as they are created. A task terminates when it is explicitly stopped, when it's predefined duration or number of records has been produced/consumed, or when it encounters an error. An exterior thread can await on records processed, or simply on termination of the task. At a given time produced or consumed records are accessible through the task. The actual creation of the producer or consumer happens when a task is started. When the task terminates the producer or the consumer is automatically closed. For example, in the previous example: We described a producer with a client id my-producer and max.block.ms of 5 seconds. Then we created a task to produce 100 records using the generator function, without waiting for its completion. We then described a consumer with group id my-group and which commits offset synchronously on every received record. Finally, we created a task to consume records for 10 seconds and await its completion.","title":"Getting started writing tests"},{"location":"kafka/test-companion/#producing-records","text":"","title":"Producing records"},{"location":"kafka/test-companion/#produce-from-records","text":"Produce given records: 1 2 3 4 companion . produce ( byte [] . class ). fromRecords ( new ProducerRecord <> ( \"topic1\" , \"k1\" , \"1\" . getBytes ()), new ProducerRecord <> ( \"topic1\" , \"k2\" , \"2\" . getBytes ()), new ProducerRecord <> ( \"topic1\" , \"k3\" , \"3\" . getBytes ()));","title":"Produce from records"},{"location":"kafka/test-companion/#produce-from-generator-function","text":"Produce 10 records using the generator function: 1 companion . produceIntegers (). usingGenerator ( i -> new ProducerRecord <> ( \"topic\" , i ), 10 );","title":"Produce from generator function"},{"location":"kafka/test-companion/#produce-from-csv-file","text":"Given a comma-separated file records.csv with the following content 1 2 3 messages,0,a,asdf messages,1,b,asdf messages,3,c,asdf Produce records from the file: 1 companion . produceStrings (). fromCsv ( \"records.csv\" );","title":"Produce from CSV file"},{"location":"kafka/test-companion/#consuming-records","text":"","title":"Consuming records"},{"location":"kafka/test-companion/#consume-from-topics","text":"1 companion . consumeIntegers (). fromTopics ( \"topic1\" , \"topic2\" );","title":"Consume from topics"},{"location":"kafka/test-companion/#consume-from-offsets","text":"1 2 3 4 Map < TopicPartition , Long > offsets = new HashMap <> (); offsets . put ( new TopicPartition ( \"topic1\" , 0 ), 100L ); offsets . put ( new TopicPartition ( \"topic2\" , 0 ), 100L ); companion . consumeIntegers (). fromOffsets ( offsets , Duration . ofSeconds ( 10 ));","title":"Consume from offsets"},{"location":"kafka/test-companion/#consumer-assignment-and-offsets","text":"During execution of the consumer task, information about the underlying consumer's topic partition assignment, position or committed offsets can be accessed. 1 2 3 4 5 6 ConsumerBuilder < String , Integer > consumer = companion . consumeIntegers () . withAutoCommit (); consumer . fromTopics ( \"topic\" ); // ... await (). untilAsserted ( consumer :: waitForAssignment ); consumer . committed ( new TopicPartition ( \"topic\" , 1 ));","title":"Consumer assignment and offsets"},{"location":"kafka/test-companion/#registering-custom-serdes","text":"KafkaCompanion handles Serializers and Deserializers for default types such as primitives, String, ByteBuffer, UUID. Serdes for custom types can be registered to the companion object, and will be used for producer and consumer tasks: 1 2 3 4 5 6 7 8 9 10 11 12 KafkaCompanion companion = new KafkaCompanion ( \"localhost:9092\" ); // Register serde to the companion companion . registerSerde ( Orders . class , new OrdersSerializer (), new OrdersDeserializer ()); // Companion will configure consumer accordingly ConsumerTask < Integer , Orders > orders = companion . consume ( Integer . class , Orders . class ) . fromTopics ( \"orders\" , 1000 ). awaitCompletion (); for ( ConsumerRecord < Integer , Orders > order : orders ) { // ... check consumed records }","title":"Registering Custom Serdes"},{"location":"kafka/test-companion/#topics","text":"Create, list, describe and delete topics: 1 2 3 4 5 companion . topics (). create ( \"topic1\" , 1 ); companion . topics (). createAndWait ( \"topic2\" , 3 ); Assertions . assertEquals ( companion . topics (). list (). size (), 2 ); companion . topics (). delete ( \"topic1\" );","title":"Topics"},{"location":"kafka/test-companion/#consumer-groups-and-offsets","text":"List topic partition offsets: 1 2 TopicPartition topicPartition = KafkaCompanion . tp ( \"topic\" , 1 ); long latestOffset = companion . offsets (). get ( topicPartition , OffsetSpec . latest ()). offset (); List, describe, alter consumer groups and their offsets: 1 2 3 4 5 6 7 Collection < ConsumerGroupListing > consumerGroups = companion . consumerGroups (). list (); for ( ConsumerGroupListing consumerGroup : consumerGroups ) { // check consumer groups } TopicPartition topicPartition = KafkaCompanion . tp ( \"topic\" , 1 ); companion . consumerGroups (). resetOffsets ( \"consumer-group\" , topicPartition );","title":"Consumer Groups and Offsets"},{"location":"kafka/transactions/","text":"Kafka Transactions and Exactly-Once Processing Experimental Kafka Transactions is an experimental feature. Kafka transactions enable atomic writes to multiple Kafka topics and partitions. Inside a transaction, a producer writes records to the Kafka topic partitions as it would normally do. If the transaction completes successfully, all the records previously written to the broker inside that transaction will be committed , and will be readable for consumers. If an error during the transaction causes it to be aborted , none will be readable for consumers. There are a couple of fundamental things to consider before using transactions: Each transactional producer is configured with a unique identifier called the transactional.id . This is used to identify the same producer instance across application restarts. By default, it is not configured, and transactions cannot be used. When it is configured, the producer is limited to idempotent delivery, therefore enable.idempotence=true is implied. For only reading transactional messages, a consumer needs to explicitly configure its isolation.level property to read_committed . This will make sure that the consumer will deliver only records committed inside a transaction, and filter out messages from aborted transactions. It should also be noted that this does not mean all records atomically written inside a transaction will be read atomically by the consumer. Transactional producers can write to multiple topics and partitions inside a transaction, but consumers do not know where the transaction starts or ends. Not only multiple consumers inside a consumer group can share those partitions, all records which were part of a single transaction can be consumed in different batch of records by a consumer. Kafka connector provides KafkaTransactions custom emitter for writing records inside a transaction. Before using a transaction we need to make sure that transactional.id is configured on the channel properties. 1 mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer It can be used as a regular emitter @Channel : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package kafka.outbound ; import jakarta.enterprise.context.ApplicationScoped ; import jakarta.inject.Inject ; import org.eclipse.microprofile.reactive.messaging.Channel ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions ; @ApplicationScoped public class KafkaTransactionalProducer { @Inject @Channel ( \"tx-out-example\" ) KafkaTransactions < String > txProducer ; public Uni < Void > emitInTransaction () { return txProducer . withTransaction ( emitter -> { emitter . send ( KafkaRecord . of ( 1 , \"a\" )); emitter . send ( KafkaRecord . of ( 2 , \"b\" )); emitter . send ( KafkaRecord . of ( 3 , \"c\" )); return Uni . createFrom (). voidItem (); }); } } Note When transactional.id is provided KafkaProducer#initTransactions is called when the underlying Kafka producer is created. The function given to the withTransaction method receives a TransactionalEmitter for producing records, and returns a Uni that provides the result of the transaction. If the processing completes successfully, the producer is flushed and the transaction is committed. If the processing throws an exception, returns a failing Uni , or marks the TransactionalEmitter for abort, the transaction is aborted. If this method is called on a Vert.x context, the processing function is also called on that context. Otherwise, it is called on the sending thread of the producer. Important A transaction is considered in progress from the call to the withTransaction until the returned Uni results in success or failure. While a transaction is in progress, subsequent calls to the withTransaction , including nested ones inside the given function, will throw IllegalStateException . Note that in Reactive Messaging, the execution of processing methods is already serialized, unless @Blocking(ordered = false) is used. If withTransaction can be called concurrently, for example from a REST endpoint, it is recommended to limit the concurrency of the execution. This can be done using the @Bulkhead annotation from Microprofile Fault Tolerance. Exactly-Once Processing Kafka Transactions API also allows managing consumer offsets inside a transaction, together with produced messages. This in turn enables coupling a consumer with a transactional producer in a consume-transform-produce pattern, also known as exactly-once processing. It means that an application consumes messages from a topic-partition, processes them, publishes the results to a topic-partition, and commits offsets of the consumed messages in a transaction. The KafkaTransactions emitter also provides a way to apply exactly-once processing to an incoming Kafka message inside a transaction. The following example includes a batch of Kafka records inside a transaction. 1 2 3 4 mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer mp.messaging.outgoing.in-channel.batch=true mp.messaging.outgoing.in-channel.commit-strategy=ignore mp.messaging.outgoing.in-channel.failure-strategy=ignore 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package kafka.outbound ; import jakarta.enterprise.context.ApplicationScoped ; import jakarta.inject.Inject ; import org.eclipse.microprofile.reactive.messaging.Channel ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import io.smallrye.reactive.messaging.kafka.KafkaRecordBatch ; import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions ; @ApplicationScoped public class KafkaExactlyOnceProcessor { @Inject @Channel ( \"tx-out-example\" ) KafkaTransactions < Integer > txProducer ; @Incoming ( \"in-channel\" ) public Uni < Void > emitInTransaction ( KafkaRecordBatch < String , Integer > batch ) { return txProducer . withTransaction ( batch , emitter -> { for ( KafkaRecord < String , Integer > record : batch ) { emitter . send ( KafkaRecord . of ( record . getKey (), record . getPayload () + 1 )); } return Uni . createFrom (). voidItem (); }); } } If the processing completes successfully, before committing the transaction, the topic partition offsets of the given batch message will be committed to the transaction. If the processing needs to abort, after aborting the transaction, the consumer's position is reset to the last committed offset, effectively resuming the consumption from that offset. If no consumer offset has been committed, the consumer's position is reset to the beginning of the topic, even if the offset reset policy is latest . Important When using exactly-once processing, consumed message offset commits are handled by the transaction and therefore commit-strategy needs to be ignore . Any strategy can be employed for the failure-strategy . Note that the Uni returned from the #withTransaction will yield a failure if the transaction fails and is aborted. The application can choose to handle the error case, but for the message consumption to continue, Uni returned from the @Incoming method must not result in failure. KafkaTransactions#withTransactionAndAck method will ack and nack the message but will not stop the reactive stream. Ignoring the failure simply resets the consumer to the last committed offsets and resumes the processing from there. Note It is recommended to use exactly-once processing along with the batch consumption mode. While it is possible to use it with a single Kafka message, it'll have a significant performance impact.","title":"Kafka Transactions and Exactly-Once Processing"},{"location":"kafka/transactions/#kafka-transactions-and-exactly-once-processing","text":"Experimental Kafka Transactions is an experimental feature. Kafka transactions enable atomic writes to multiple Kafka topics and partitions. Inside a transaction, a producer writes records to the Kafka topic partitions as it would normally do. If the transaction completes successfully, all the records previously written to the broker inside that transaction will be committed , and will be readable for consumers. If an error during the transaction causes it to be aborted , none will be readable for consumers. There are a couple of fundamental things to consider before using transactions: Each transactional producer is configured with a unique identifier called the transactional.id . This is used to identify the same producer instance across application restarts. By default, it is not configured, and transactions cannot be used. When it is configured, the producer is limited to idempotent delivery, therefore enable.idempotence=true is implied. For only reading transactional messages, a consumer needs to explicitly configure its isolation.level property to read_committed . This will make sure that the consumer will deliver only records committed inside a transaction, and filter out messages from aborted transactions. It should also be noted that this does not mean all records atomically written inside a transaction will be read atomically by the consumer. Transactional producers can write to multiple topics and partitions inside a transaction, but consumers do not know where the transaction starts or ends. Not only multiple consumers inside a consumer group can share those partitions, all records which were part of a single transaction can be consumed in different batch of records by a consumer. Kafka connector provides KafkaTransactions custom emitter for writing records inside a transaction. Before using a transaction we need to make sure that transactional.id is configured on the channel properties. 1 mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer It can be used as a regular emitter @Channel : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package kafka.outbound ; import jakarta.enterprise.context.ApplicationScoped ; import jakarta.inject.Inject ; import org.eclipse.microprofile.reactive.messaging.Channel ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions ; @ApplicationScoped public class KafkaTransactionalProducer { @Inject @Channel ( \"tx-out-example\" ) KafkaTransactions < String > txProducer ; public Uni < Void > emitInTransaction () { return txProducer . withTransaction ( emitter -> { emitter . send ( KafkaRecord . of ( 1 , \"a\" )); emitter . send ( KafkaRecord . of ( 2 , \"b\" )); emitter . send ( KafkaRecord . of ( 3 , \"c\" )); return Uni . createFrom (). voidItem (); }); } } Note When transactional.id is provided KafkaProducer#initTransactions is called when the underlying Kafka producer is created. The function given to the withTransaction method receives a TransactionalEmitter for producing records, and returns a Uni that provides the result of the transaction. If the processing completes successfully, the producer is flushed and the transaction is committed. If the processing throws an exception, returns a failing Uni , or marks the TransactionalEmitter for abort, the transaction is aborted. If this method is called on a Vert.x context, the processing function is also called on that context. Otherwise, it is called on the sending thread of the producer. Important A transaction is considered in progress from the call to the withTransaction until the returned Uni results in success or failure. While a transaction is in progress, subsequent calls to the withTransaction , including nested ones inside the given function, will throw IllegalStateException . Note that in Reactive Messaging, the execution of processing methods is already serialized, unless @Blocking(ordered = false) is used. If withTransaction can be called concurrently, for example from a REST endpoint, it is recommended to limit the concurrency of the execution. This can be done using the @Bulkhead annotation from Microprofile Fault Tolerance.","title":"Kafka Transactions and Exactly-Once Processing"},{"location":"kafka/transactions/#exactly-once-processing","text":"Kafka Transactions API also allows managing consumer offsets inside a transaction, together with produced messages. This in turn enables coupling a consumer with a transactional producer in a consume-transform-produce pattern, also known as exactly-once processing. It means that an application consumes messages from a topic-partition, processes them, publishes the results to a topic-partition, and commits offsets of the consumed messages in a transaction. The KafkaTransactions emitter also provides a way to apply exactly-once processing to an incoming Kafka message inside a transaction. The following example includes a batch of Kafka records inside a transaction. 1 2 3 4 mp.messaging.outgoing.tx-out-example.transactional.id=example-tx-producer mp.messaging.outgoing.in-channel.batch=true mp.messaging.outgoing.in-channel.commit-strategy=ignore mp.messaging.outgoing.in-channel.failure-strategy=ignore 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package kafka.outbound ; import jakarta.enterprise.context.ApplicationScoped ; import jakarta.inject.Inject ; import org.eclipse.microprofile.reactive.messaging.Channel ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import io.smallrye.mutiny.Uni ; import io.smallrye.reactive.messaging.kafka.KafkaRecord ; import io.smallrye.reactive.messaging.kafka.KafkaRecordBatch ; import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions ; @ApplicationScoped public class KafkaExactlyOnceProcessor { @Inject @Channel ( \"tx-out-example\" ) KafkaTransactions < Integer > txProducer ; @Incoming ( \"in-channel\" ) public Uni < Void > emitInTransaction ( KafkaRecordBatch < String , Integer > batch ) { return txProducer . withTransaction ( batch , emitter -> { for ( KafkaRecord < String , Integer > record : batch ) { emitter . send ( KafkaRecord . of ( record . getKey (), record . getPayload () + 1 )); } return Uni . createFrom (). voidItem (); }); } } If the processing completes successfully, before committing the transaction, the topic partition offsets of the given batch message will be committed to the transaction. If the processing needs to abort, after aborting the transaction, the consumer's position is reset to the last committed offset, effectively resuming the consumption from that offset. If no consumer offset has been committed, the consumer's position is reset to the beginning of the topic, even if the offset reset policy is latest . Important When using exactly-once processing, consumed message offset commits are handled by the transaction and therefore commit-strategy needs to be ignore . Any strategy can be employed for the failure-strategy . Note that the Uni returned from the #withTransaction will yield a failure if the transaction fails and is aborted. The application can choose to handle the error case, but for the message consumption to continue, Uni returned from the @Incoming method must not result in failure. KafkaTransactions#withTransactionAndAck method will ack and nack the message but will not stop the reactive stream. Ignoring the failure simply resets the consumer to the last committed offsets and resumes the processing from there. Note It is recommended to use exactly-once processing along with the batch consumption mode. While it is possible to use it with a single Kafka message, it'll have a significant performance impact.","title":"Exactly-Once Processing"},{"location":"kafka/writing-kafka-records/","text":"Writing Kafka Records The Kafka Connector can write Reactive Messaging Messages as Kafka Records. Example Let\u2019s imagine you have a Kafka broker running, and accessible using the kafka:9092 address (by default it would use localhost:9092 ). Configure your application to write the messages from the prices channel into a Kafka topic as follows: 1 2 3 4 5 kafka.bootstrap.servers = kafka:9092 # <1> mp.messaging.outgoing.prices-out.connector = smallrye-kafka # <2> mp.messaging.outgoing.prices-out.value.serializer = org.apache.kafka.common.serialization.DoubleSerializer # <3> mp.messaging.outgoing.prices-out.topic = prices # <4> 1. Configure the broker location. You can configure it globally or per channel 2. Configure the connector to manage the prices channel 3. Sets the (Kafka) serializer to encode the message payload into the record\u2019s value 4. Make sure the topic name is prices (and not the default prices-out ) Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package kafka.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class KafkaPriceProducer { private final Random random = new Random (); @Outgoing ( \"prices-out\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package kafka.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class KafkaPriceMessageProducer { private final Random random = new Random (); @Outgoing ( \"prices-out\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } } Serialization The serialization is handled by the underlying Kafka Client. You need to configure the: mp.messaging.outgoing.[channel-name].value.serializer to configure the value serializer (mandatory) mp.messaging.outgoing.[channel-name].key.serializer to configure the key serializer (optional, default to String ) If you want to use a custom serializer, add it to your CLASSPATH and configure the associate attribute. By default, the written record contains: the Message payload as value no key, or the key configured using the key attribute or the key passed in the metadata attached to the Message the timestamp computed for the system clock ( now ) or the timestamp passed in the metadata attached to the Message Sending key/value pairs In the Kafka world, it\u2019s often necessary to send records , i.e. a key/value pair. The connector provides the io.smallrye.reactive.messaging.kafka.Record class that you can use to send a pair: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Record < String , String > process ( String in ) { return Record . of ( \"my-key\" , in ); } When the connector receives a message with a Record payload, it extracts the key and value from it. The configured serializers for the key and the value must be compatible with the record\u2019s key and value. Note that the key and the value can be null . It is also possible to create a record with a null key AND a null value. If you need more control on the written records, use OutgoingKafkaRecordMetadata . Outbound Metadata When sending Messages , you can add an instance of OutgoingKafkaRecordMetadata to influence how the message is going to be written to Kafka. For example, you can add Kafka headers, configure the record key\u2026 1 2 3 4 5 6 7 8 9 10 // Creates an OutgoingKafkaRecordMetadata // The type parameter is the type of the record's key OutgoingKafkaRecordMetadata < String > metadata = OutgoingKafkaRecordMetadata . < String > builder () . withKey ( \"my-key\" ) . withHeaders ( new RecordHeaders (). add ( \"my-header\" , \"value\" . getBytes ())) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); Propagating Record Key When processing messages, you can propagate incoming record key to the outgoing record. Consider the following example method, which consumes messages from the channel in , transforms the payload, and writes the result to the channel out . 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public double process ( int in ) { return in * 0.88 ; } Enabled with mp.messaging.outgoing.[channel-name].propagate-record-key=true configuration, record key propagation produces the outgoing record with the same key as the incoming record. If the outgoing record already contains a key , it won\u2019t be overridden by the incoming record key. If the incoming record does have a null key, the mp.messaging.outgoing.[channel-name].key property is used. Propagating Record headers You can also propagate incoming record headers to the outgoing record, by specifying the list of headers to be considered. mp.messaging.outgoing.[channel-name].propagate-headers=Authorization,Proxy-Authorization If the ougoing record already defines a header with the same key, it won't be overriden by the incoming header. Dynamic topic names Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the topic inside your application configuration file, but instead, use the outbound metadata to set the name of the topic. For example, you can route to a dynamic topic based on the incoming message: 1 2 3 4 5 6 7 8 String topicName = selectTopicFromIncommingMessage ( incoming ); OutgoingKafkaRecordMetadata < String > metadata = OutgoingKafkaRecordMetadata . < String > builder () . withTopic ( topicName ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata ); Acknowledgement Kafka acknowledgement can take times depending on the configuration. Also, it stores in-memory the records that cannot be written. By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received Message ). You can disable this by setting the waitForWriteCompletion attribute to false . Note that the acks attribute has a huge impact on the record acknowledgement. If a record cannot be written, the message is nacked . Back-pressure and inflight records The Kafka outbound connector handles back-pressure monitoring the number of in-flight messages waiting to be written to the Kafka broker. The number of in-flight messages is configured using the max-inflight-messages attribute and defaults to 1024. The connector only sends that amount of messages concurrently. No other messages will be sent until at least one in-flight message gets acknowledged by the broker. Then, the connector writes a new message to Kafka when one of the broker\u2019s in-flight messages get acknowledged. Be sure to configure Kafka\u2019s batch.size and linger.ms accordingly. You can also remove the limit of inflight messages by setting max-inflight-messages to 0 . However, note that the Kafka Producer may block if the number of requests reaches max.in.flight.requests.per.connection . Handling serialization failures For Kafka producer client serialization failures are not recoverable, thus the message dispatch is not retried. However, using schema registries, serialization may intermittently fail to contact the registry. In these cases you may need to apply a failure strategy for the serializer. To achieve this, create a CDI bean implementing the SerializationFailureHandler interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ApplicationScoped @Identifier ( \"failure-fallback\" ) // Set the name of the failure handler public class MySerializationFailureHandler implements SerializationFailureHandler < JsonObject > { // Specify the expected type @Override public byte [] decorateSerialization ( Uni < byte []> serialization , String topic , boolean isKey , String serializer , Object data , Headers headers ) { return serialization . onFailure (). retry (). atMost ( 3 ) . await (). atMost ( Duration . ofMillis ( 200 )); } } The bean must be exposed with the @Identifier qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute: mp.messaging.incoming.$channel.key-serialization-failure-handler : name of the bean handling serialization failures happening for the record\u2019s key mp.messaging.incoming.$channel.value-serialization-failure-handler : name of the bean handling serialization failures happening for the record\u2019s value, The handler is called with the serialization action as a Uni , the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers. Failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the serialized byte array. Alternatively, the handler can implement decorateSerialization method which can return a fallback value. Sending Cloud Events The Kafka connector supports Cloud Events . The connector sends the outbound record as Cloud Events if: the message metadata contains an io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instance, the channel configuration defines the cloud-events-type and cloud-events-source attributes. You can create io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instances using: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package kafka.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class KafkaCloudEventProcessor { @Outgoing ( \"cloud-events\" ) public Message < String > toCloudEvents ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greetings\" ) . withSource ( URI . create ( \"http://example.com\" )) . withSubject ( \"greeting-message\" ) . build ()); } } If the metadata does not contain an id, the connector generates one (random UUID). The type and source can be configured per message or at the channel level using the cloud-events-type and cloud-events-source attributes. Other attributes are also configurable. The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package kafka.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class KafkaCloudEventMultipleProcessors { @Incoming ( \"source\" ) @Outgoing ( \"processed\" ) public Message < String > process ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greeting\" ) . build ()); } @SuppressWarnings ( \"unchecked\" ) @Incoming ( \"processed\" ) @Outgoing ( \"cloud-events\" ) public Message < String > process2 ( Message < String > in ) { OutgoingCloudEventMetadata < String > metadata = in . getMetadata ( OutgoingCloudEventMetadata . class ) . orElseGet (() -> OutgoingCloudEventMetadata . builder (). build ()); return in . addMetadata ( OutgoingCloudEventMetadata . from ( metadata ) . withSource ( URI . create ( \"source://me\" )) . withSubject ( \"test\" ) . build ()); } } By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the cloud-events-mode to structured . Only JSON is supported, so the created records had its content-type header set to application/cloudevents+json; charset=UTF-8 When using the structured mode, the value serializer must be set to org.apache.kafka.common.serialization.StringSerializer , otherwise the connector reports the error. In addition, in structured , the connector maps the message\u2019s payload to JSON, except for String passed directly. The record\u2019s key can be set in the channel configuration ( key attribute), in the OutgoingKafkaRecordMetadata or using the partitionkey Cloud Event attribute. Note you can disable the Cloud Event support by setting the cloud-events attribute to false Using ProducerRecord Kafka built-in type ProducerRecord\\ can also be used for producing messages: 1 2 3 4 @Outgoing ( \"out\" ) public ProducerRecord < String , String > generate () { return new ProducerRecord <> ( \"my-topic\" , \"key\" , \"value\" ); } Warning This is an advanced feature. The ProducerRecord is sent to Kafka as is. Any possible metadata attached through Message<ProducerRecord<K, V>> are ignored and lost. Producer Interceptors Producer interceptors can be configured for Kafka producer clients using the standard interceptor.classes property. Configured classes will be instantiated by the Kafka producer on client creation. Alternatively, producer clients can be configured with a CDI managed-bean implementing org.apache.kafka.clients.producer.ProducerInterceptor interface: To achieve this, the bean must be exposed with the @Identifier qualifier specifying the name of the bean: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package kafka.outbound ; import java.util.Map ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.producer.ProducerInterceptor ; import org.apache.kafka.clients.producer.ProducerRecord ; import org.apache.kafka.clients.producer.RecordMetadata ; import io.smallrye.common.annotation.Identifier ; @ApplicationScoped @Identifier ( \"my-producer-interceptor\" ) public class ProducerInterceptorBeanExample implements ProducerInterceptor < Integer , String > { @Override public ProducerRecord < Integer , String > onSend ( ProducerRecord < Integer , String > producerRecord ) { // called before send return producerRecord ; } @Override public void onAcknowledgement ( RecordMetadata recordMetadata , Exception e ) { // called on send acknowledgement callback } @Override public void close () { // called on client close } @Override public void configure ( Map < String , ?> map ) { // called on client configuration } } Then, in the channel configuration, specify the following attribute: mp.messaging.incoming.$channel.interceptor-bean=my-producer-interceptor . Warning The onSend method will be called on the producer sending thread and onAcknowledgement will be called on the Kafka producer I/O thread . In both cases if implementations are not fast, sending of messages could be delayed. Configuration Reference Attribute ( alias ) Description Type Mandatory Default acks The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. Accepted values are: 0, 1, all string false 1 bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false localhost:9092 buffer.memory The total bytes of memory the producer can use to buffer records waiting to be sent to the server. long false 33554432 client-id-prefix Prefix for Kafka client client.id attribute. If defined configured or generated client.id will be prefixed with the given value. string false close-timeout The amount of milliseconds waiting for a graceful shutdown of the Kafka producer int false 10000 cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default datacontenttype attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the datacontenttype attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default dataschema attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the dataschema attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the time attribute into the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the time attribute itself boolean false true cloud-events-mode The Cloud Event mode ( structured or binary (default)). Indicates how are written the cloud events in the outgoing record string false binary cloud-events-source (cloud-events-default-source) Configure the default source attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the source attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default subject attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the subject attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default type attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the type attribute itself string false health-enabled Whether health reporting is enabled (default) or disabled boolean false true health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false true health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false false health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false 2000 interceptor-bean The name set in @Identifier of a bean that implements org.apache.kafka.clients.producer.ProducerInterceptor . If set, the identified bean will be used as the producer interceptor. string false kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key A key to used when writing the record string false key-serialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.SerializationFailureHandler . If set, serialization failure happening when serializing keys are delegated to this handler which may provide a fallback value. string false key.serializer The serializer classname used to serialize the record's key string false org.apache.kafka.common.serialization.StringSerializer lazy-client Whether Kafka client is created lazily or eagerly. boolean false false max-inflight-messages The maximum number of messages to be written to Kafka concurrently. It limits the number of messages waiting to be written and acknowledged by the broker. You can set this attribute to 0 remove the limit long false 1024 merge Whether the connector should allow multiple upstreams boolean false false partition The target partition id. -1 to let the client determine the partition int false -1 propagate-headers A comma-separating list of incoming record headers to be propagated to the outgoing record string false `` propagate-record-key Propagate incoming record key to the outgoing record boolean false false retries If set to a positive number, the connector will try to resend any record that was not delivered successfully (with a potentially transient error) until the number of retries is reached. If set to 0, retries are disabled. If not set, the connector tries to resend any record that failed to be delivered (because of a potentially transient error) during an amount of time configured by delivery.timeout.ms . long false 2147483647 topic The consumed / populated Kafka topic. If neither this property nor the topics properties are set, the channel name is used string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true value-serialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.SerializationFailureHandler . If set, serialization failure happening when serializing values are delegated to this handler which may provide a fallback value. string false value.serializer The serializer classname used to serialize the payload string true waitForWriteCompletion Whether the client waits for Kafka to acknowledge the written record before acknowledging the message boolean false true You can also pass any property supported by the underlying Kafka producer . For example, to configure the batch.size property, use: 1 mp.messaging.outgoing.[channel].batch.size = 32768 Some producer client properties are configured to sensible default values: If not set, reconnect.backoff.max.ms is set to 10000 to avoid high load on disconnection. If not set, key.serializer is set to org.apache.kafka.common.serialization.StringSerializer . If not set, producer client.id is generated as kafka-producer-[channel] .","title":"Writing records"},{"location":"kafka/writing-kafka-records/#writing-kafka-records","text":"The Kafka Connector can write Reactive Messaging Messages as Kafka Records.","title":"Writing Kafka Records"},{"location":"kafka/writing-kafka-records/#example","text":"Let\u2019s imagine you have a Kafka broker running, and accessible using the kafka:9092 address (by default it would use localhost:9092 ). Configure your application to write the messages from the prices channel into a Kafka topic as follows: 1 2 3 4 5 kafka.bootstrap.servers = kafka:9092 # <1> mp.messaging.outgoing.prices-out.connector = smallrye-kafka # <2> mp.messaging.outgoing.prices-out.value.serializer = org.apache.kafka.common.serialization.DoubleSerializer # <3> mp.messaging.outgoing.prices-out.topic = prices # <4> 1. Configure the broker location. You can configure it globally or per channel 2. Configure the connector to manage the prices channel 3. Sets the (Kafka) serializer to encode the message payload into the record\u2019s value 4. Make sure the topic name is prices (and not the default prices-out ) Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package kafka.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class KafkaPriceProducer { private final Random random = new Random (); @Outgoing ( \"prices-out\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package kafka.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class KafkaPriceMessageProducer { private final Random random = new Random (); @Outgoing ( \"prices-out\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } }","title":"Example"},{"location":"kafka/writing-kafka-records/#serialization","text":"The serialization is handled by the underlying Kafka Client. You need to configure the: mp.messaging.outgoing.[channel-name].value.serializer to configure the value serializer (mandatory) mp.messaging.outgoing.[channel-name].key.serializer to configure the key serializer (optional, default to String ) If you want to use a custom serializer, add it to your CLASSPATH and configure the associate attribute. By default, the written record contains: the Message payload as value no key, or the key configured using the key attribute or the key passed in the metadata attached to the Message the timestamp computed for the system clock ( now ) or the timestamp passed in the metadata attached to the Message","title":"Serialization"},{"location":"kafka/writing-kafka-records/#sending-keyvalue-pairs","text":"In the Kafka world, it\u2019s often necessary to send records , i.e. a key/value pair. The connector provides the io.smallrye.reactive.messaging.kafka.Record class that you can use to send a pair: 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public Record < String , String > process ( String in ) { return Record . of ( \"my-key\" , in ); } When the connector receives a message with a Record payload, it extracts the key and value from it. The configured serializers for the key and the value must be compatible with the record\u2019s key and value. Note that the key and the value can be null . It is also possible to create a record with a null key AND a null value. If you need more control on the written records, use OutgoingKafkaRecordMetadata .","title":"Sending key/value pairs"},{"location":"kafka/writing-kafka-records/#outbound-metadata","text":"When sending Messages , you can add an instance of OutgoingKafkaRecordMetadata to influence how the message is going to be written to Kafka. For example, you can add Kafka headers, configure the record key\u2026 1 2 3 4 5 6 7 8 9 10 // Creates an OutgoingKafkaRecordMetadata // The type parameter is the type of the record's key OutgoingKafkaRecordMetadata < String > metadata = OutgoingKafkaRecordMetadata . < String > builder () . withKey ( \"my-key\" ) . withHeaders ( new RecordHeaders (). add ( \"my-header\" , \"value\" . getBytes ())) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata );","title":"Outbound Metadata"},{"location":"kafka/writing-kafka-records/#propagating-record-key","text":"When processing messages, you can propagate incoming record key to the outgoing record. Consider the following example method, which consumes messages from the channel in , transforms the payload, and writes the result to the channel out . 1 2 3 4 5 @Incoming ( \"in\" ) @Outgoing ( \"out\" ) public double process ( int in ) { return in * 0.88 ; } Enabled with mp.messaging.outgoing.[channel-name].propagate-record-key=true configuration, record key propagation produces the outgoing record with the same key as the incoming record. If the outgoing record already contains a key , it won\u2019t be overridden by the incoming record key. If the incoming record does have a null key, the mp.messaging.outgoing.[channel-name].key property is used.","title":"Propagating Record Key"},{"location":"kafka/writing-kafka-records/#propagating-record-headers","text":"You can also propagate incoming record headers to the outgoing record, by specifying the list of headers to be considered. mp.messaging.outgoing.[channel-name].propagate-headers=Authorization,Proxy-Authorization If the ougoing record already defines a header with the same key, it won't be overriden by the incoming header.","title":"Propagating Record headers"},{"location":"kafka/writing-kafka-records/#dynamic-topic-names","text":"Sometimes it is desirable to select the destination of a message dynamically. In this case, you should not configure the topic inside your application configuration file, but instead, use the outbound metadata to set the name of the topic. For example, you can route to a dynamic topic based on the incoming message: 1 2 3 4 5 6 7 8 String topicName = selectTopicFromIncommingMessage ( incoming ); OutgoingKafkaRecordMetadata < String > metadata = OutgoingKafkaRecordMetadata . < String > builder () . withTopic ( topicName ) . build (); // Create a new message from the `incoming` message // Add `metadata` to the metadata from the `incoming` message. return incoming . addMetadata ( metadata );","title":"Dynamic topic names"},{"location":"kafka/writing-kafka-records/#acknowledgement","text":"Kafka acknowledgement can take times depending on the configuration. Also, it stores in-memory the records that cannot be written. By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received Message ). You can disable this by setting the waitForWriteCompletion attribute to false . Note that the acks attribute has a huge impact on the record acknowledgement. If a record cannot be written, the message is nacked .","title":"Acknowledgement"},{"location":"kafka/writing-kafka-records/#back-pressure-and-inflight-records","text":"The Kafka outbound connector handles back-pressure monitoring the number of in-flight messages waiting to be written to the Kafka broker. The number of in-flight messages is configured using the max-inflight-messages attribute and defaults to 1024. The connector only sends that amount of messages concurrently. No other messages will be sent until at least one in-flight message gets acknowledged by the broker. Then, the connector writes a new message to Kafka when one of the broker\u2019s in-flight messages get acknowledged. Be sure to configure Kafka\u2019s batch.size and linger.ms accordingly. You can also remove the limit of inflight messages by setting max-inflight-messages to 0 . However, note that the Kafka Producer may block if the number of requests reaches max.in.flight.requests.per.connection .","title":"Back-pressure and inflight records"},{"location":"kafka/writing-kafka-records/#handling-serialization-failures","text":"For Kafka producer client serialization failures are not recoverable, thus the message dispatch is not retried. However, using schema registries, serialization may intermittently fail to contact the registry. In these cases you may need to apply a failure strategy for the serializer. To achieve this, create a CDI bean implementing the SerializationFailureHandler interface: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @ApplicationScoped @Identifier ( \"failure-fallback\" ) // Set the name of the failure handler public class MySerializationFailureHandler implements SerializationFailureHandler < JsonObject > { // Specify the expected type @Override public byte [] decorateSerialization ( Uni < byte []> serialization , String topic , boolean isKey , String serializer , Object data , Headers headers ) { return serialization . onFailure (). retry (). atMost ( 3 ) . await (). atMost ( Duration . ofMillis ( 200 )); } } The bean must be exposed with the @Identifier qualifier specifying the name of the bean. Then, in the connector configuration, specify the following attribute: mp.messaging.incoming.$channel.key-serialization-failure-handler : name of the bean handling serialization failures happening for the record\u2019s key mp.messaging.incoming.$channel.value-serialization-failure-handler : name of the bean handling serialization failures happening for the record\u2019s value, The handler is called with the serialization action as a Uni , the record\u2019s topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers. Failure strategies like retry, providing a fallback value or applying timeout can be implemented. Note that the method must await on the result and return the serialized byte array. Alternatively, the handler can implement decorateSerialization method which can return a fallback value.","title":"Handling serialization failures"},{"location":"kafka/writing-kafka-records/#sending-cloud-events","text":"The Kafka connector supports Cloud Events . The connector sends the outbound record as Cloud Events if: the message metadata contains an io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instance, the channel configuration defines the cloud-events-type and cloud-events-source attributes. You can create io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata instances using: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package kafka.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class KafkaCloudEventProcessor { @Outgoing ( \"cloud-events\" ) public Message < String > toCloudEvents ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greetings\" ) . withSource ( URI . create ( \"http://example.com\" )) . withSubject ( \"greeting-message\" ) . build ()); } } If the metadata does not contain an id, the connector generates one (random UUID). The type and source can be configured per message or at the channel level using the cloud-events-type and cloud-events-source attributes. Other attributes are also configurable. The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package kafka.outbound ; import java.net.URI ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata ; @ApplicationScoped public class KafkaCloudEventMultipleProcessors { @Incoming ( \"source\" ) @Outgoing ( \"processed\" ) public Message < String > process ( Message < String > in ) { return in . addMetadata ( OutgoingCloudEventMetadata . builder () . withId ( \"id-\" + in . getPayload ()) . withType ( \"greeting\" ) . build ()); } @SuppressWarnings ( \"unchecked\" ) @Incoming ( \"processed\" ) @Outgoing ( \"cloud-events\" ) public Message < String > process2 ( Message < String > in ) { OutgoingCloudEventMetadata < String > metadata = in . getMetadata ( OutgoingCloudEventMetadata . class ) . orElseGet (() -> OutgoingCloudEventMetadata . builder (). build ()); return in . addMetadata ( OutgoingCloudEventMetadata . from ( metadata ) . withSource ( URI . create ( \"source://me\" )) . withSubject ( \"test\" ) . build ()); } } By default, the connector sends the Cloud Events using the binary format. You can write structured Cloud Events by setting the cloud-events-mode to structured . Only JSON is supported, so the created records had its content-type header set to application/cloudevents+json; charset=UTF-8 When using the structured mode, the value serializer must be set to org.apache.kafka.common.serialization.StringSerializer , otherwise the connector reports the error. In addition, in structured , the connector maps the message\u2019s payload to JSON, except for String passed directly. The record\u2019s key can be set in the channel configuration ( key attribute), in the OutgoingKafkaRecordMetadata or using the partitionkey Cloud Event attribute. Note you can disable the Cloud Event support by setting the cloud-events attribute to false","title":"Sending Cloud Events"},{"location":"kafka/writing-kafka-records/#using-producerrecord","text":"Kafka built-in type ProducerRecord\\ can also be used for producing messages: 1 2 3 4 @Outgoing ( \"out\" ) public ProducerRecord < String , String > generate () { return new ProducerRecord <> ( \"my-topic\" , \"key\" , \"value\" ); } Warning This is an advanced feature. The ProducerRecord is sent to Kafka as is. Any possible metadata attached through Message<ProducerRecord<K, V>> are ignored and lost.","title":"Using ProducerRecord"},{"location":"kafka/writing-kafka-records/#producer-interceptors","text":"Producer interceptors can be configured for Kafka producer clients using the standard interceptor.classes property. Configured classes will be instantiated by the Kafka producer on client creation. Alternatively, producer clients can be configured with a CDI managed-bean implementing org.apache.kafka.clients.producer.ProducerInterceptor interface: To achieve this, the bean must be exposed with the @Identifier qualifier specifying the name of the bean: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package kafka.outbound ; import java.util.Map ; import jakarta.enterprise.context.ApplicationScoped ; import org.apache.kafka.clients.producer.ProducerInterceptor ; import org.apache.kafka.clients.producer.ProducerRecord ; import org.apache.kafka.clients.producer.RecordMetadata ; import io.smallrye.common.annotation.Identifier ; @ApplicationScoped @Identifier ( \"my-producer-interceptor\" ) public class ProducerInterceptorBeanExample implements ProducerInterceptor < Integer , String > { @Override public ProducerRecord < Integer , String > onSend ( ProducerRecord < Integer , String > producerRecord ) { // called before send return producerRecord ; } @Override public void onAcknowledgement ( RecordMetadata recordMetadata , Exception e ) { // called on send acknowledgement callback } @Override public void close () { // called on client close } @Override public void configure ( Map < String , ?> map ) { // called on client configuration } } Then, in the channel configuration, specify the following attribute: mp.messaging.incoming.$channel.interceptor-bean=my-producer-interceptor . Warning The onSend method will be called on the producer sending thread and onAcknowledgement will be called on the Kafka producer I/O thread . In both cases if implementations are not fast, sending of messages could be delayed.","title":"Producer Interceptors"},{"location":"kafka/writing-kafka-records/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default acks The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. Accepted values are: 0, 1, all string false 1 bootstrap.servers (kafka.bootstrap.servers) A comma-separated list of host:port to use for establishing the initial connection to the Kafka cluster. string false localhost:9092 buffer.memory The total bytes of memory the producer can use to buffer records waiting to be sent to the server. long false 33554432 client-id-prefix Prefix for Kafka client client.id attribute. If defined configured or generated client.id will be prefixed with the given value. string false close-timeout The amount of milliseconds waiting for a graceful shutdown of the Kafka producer int false 10000 cloud-events Enables (default) or disables the Cloud Event support. If enabled on an incoming channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an outgoing , the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata. boolean false true cloud-events-data-content-type (cloud-events-default-data-content-type) Configure the default datacontenttype attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the datacontenttype attribute itself string false cloud-events-data-schema (cloud-events-default-data-schema) Configure the default dataschema attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the dataschema attribute itself string false cloud-events-insert-timestamp (cloud-events-default-timestamp) Whether or not the connector should insert automatically the time attribute into the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the time attribute itself boolean false true cloud-events-mode The Cloud Event mode ( structured or binary (default)). Indicates how are written the cloud events in the outgoing record string false binary cloud-events-source (cloud-events-default-source) Configure the default source attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the source attribute itself string false cloud-events-subject (cloud-events-default-subject) Configure the default subject attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the subject attribute itself string false cloud-events-type (cloud-events-default-type) Configure the default type attribute of the outgoing Cloud Event. Requires cloud-events to be set to true . This value is used if the message does not configure the type attribute itself string false health-enabled Whether health reporting is enabled (default) or disabled boolean false true health-readiness-enabled Whether readiness health reporting is enabled (default) or disabled boolean false true health-readiness-timeout deprecated - During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. Deprecated: Use 'health-topic-verification-timeout' instead. long false health-readiness-topic-verification deprecated - Whether the readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin connection. Deprecated: Use 'health-topic-verification-enabled' instead. boolean false health-topic-verification-enabled Whether the startup and readiness check should verify that topics exist on the broker. Default to false. Enabling it requires an admin client connection. boolean false false health-topic-verification-timeout During the startup and readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready. long false 2000 interceptor-bean The name set in @Identifier of a bean that implements org.apache.kafka.clients.producer.ProducerInterceptor . If set, the identified bean will be used as the producer interceptor. string false kafka-configuration Identifier of a CDI bean that provides the default Kafka consumer/producer configuration for this channel. The channel configuration can still override any attribute. The bean must have a type of Map and must use the @io.smallrye.common.annotation.Identifier qualifier to set the identifier. string false key A key to used when writing the record string false key-serialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.SerializationFailureHandler . If set, serialization failure happening when serializing keys are delegated to this handler which may provide a fallback value. string false key.serializer The serializer classname used to serialize the record's key string false org.apache.kafka.common.serialization.StringSerializer lazy-client Whether Kafka client is created lazily or eagerly. boolean false false max-inflight-messages The maximum number of messages to be written to Kafka concurrently. It limits the number of messages waiting to be written and acknowledged by the broker. You can set this attribute to 0 remove the limit long false 1024 merge Whether the connector should allow multiple upstreams boolean false false partition The target partition id. -1 to let the client determine the partition int false -1 propagate-headers A comma-separating list of incoming record headers to be propagated to the outgoing record string false `` propagate-record-key Propagate incoming record key to the outgoing record boolean false false retries If set to a positive number, the connector will try to resend any record that was not delivered successfully (with a potentially transient error) until the number of retries is reached. If set to 0, retries are disabled. If not set, the connector tries to resend any record that failed to be delivered (because of a potentially transient error) during an amount of time configured by delivery.timeout.ms . long false 2147483647 topic The consumed / populated Kafka topic. If neither this property nor the topics properties are set, the channel name is used string false tracing-enabled Whether tracing is enabled (default) or disabled boolean false true value-serialization-failure-handler The name set in @Identifier of a bean that implements io.smallrye.reactive.messaging.kafka.SerializationFailureHandler . If set, serialization failure happening when serializing values are delegated to this handler which may provide a fallback value. string false value.serializer The serializer classname used to serialize the payload string true waitForWriteCompletion Whether the client waits for Kafka to acknowledge the written record before acknowledging the message boolean false true You can also pass any property supported by the underlying Kafka producer . For example, to configure the batch.size property, use: 1 mp.messaging.outgoing.[channel].batch.size = 32768 Some producer client properties are configured to sensible default values: If not set, reconnect.backoff.max.ms is set to 10000 to avoid high load on disconnection. If not set, key.serializer is set to org.apache.kafka.common.serialization.StringSerializer . If not set, producer client.id is generated as kafka-producer-[channel] .","title":"Configuration Reference"},{"location":"mqtt/client-customization/","text":"Customizing the underlying MQTT client You can customize the underlying MQTT Client configuration by producing an instance of io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Produces @Identifier ( \"my-options\" ) public MqttClientSessionOptions getOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return new MqttClientSessionOptions () . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name = my-options","title":"Customizing the MQTT client"},{"location":"mqtt/client-customization/#customizing-the-underlying-mqtt-client","text":"You can customize the underlying MQTT Client configuration by producing an instance of io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Produces @Identifier ( \"my-options\" ) public MqttClientSessionOptions getOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return new MqttClientSessionOptions () . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name = my-options","title":"Customizing the underlying MQTT client"},{"location":"mqtt/mqtt/","text":"MQTT Connector The MQTT connector adds support for MQTT to Reactive Messaging. It lets you receive messages from an MQTT server or broker as well as send MQTT messages. The MQTT connector is based on the Vert.x MQTT Client . Introduction MQTT is a machine-to-machine (M2M)/\"Internet of Things\" connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport. The MQTT Connector allows consuming messages from MQTT as well as sending MQTT messages. Using the MQTT connector To you the MQTT Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-mqtt </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-mqtt . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-mqtt # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-mqtt","title":"MQTT Connector"},{"location":"mqtt/mqtt/#mqtt-connector","text":"The MQTT connector adds support for MQTT to Reactive Messaging. It lets you receive messages from an MQTT server or broker as well as send MQTT messages. The MQTT connector is based on the Vert.x MQTT Client .","title":"MQTT Connector"},{"location":"mqtt/mqtt/#introduction","text":"MQTT is a machine-to-machine (M2M)/\"Internet of Things\" connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport. The MQTT Connector allows consuming messages from MQTT as well as sending MQTT messages.","title":"Introduction"},{"location":"mqtt/mqtt/#using-the-mqtt-connector","text":"To you the MQTT Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-mqtt </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-mqtt . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-mqtt # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-mqtt","title":"Using the MQTT connector"},{"location":"mqtt/receiving-mqtt-messages/","text":"Receiving messages from MQTT The MQTT Connector connects to a MQTT broker or router, and forward the messages to the Reactive Messaging application. It maps each of them into Reactive Messaging Messages . Example Let\u2019s imagine you have a MQTT server/broker running, and accessible using the mqtt:1883 address (by default it would use localhost:1883 ). Configure your application to receive MQTT messages on the prices channel as follows: 1 2 3 mp.messaging.incoming.prices.connector = smallrye-mqtt # <1> mp.messaging.incoming.prices.host = mqtt # <2> mp.messaging.incoming.prices.port = 1883 # <3> 1. Sets the connector for the prices channel 2. Configure the broker/server host name. 3. Configure the broker/server port. 1883 is the default. Note You don\u2019t need to set the MQTT topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. Note It is generally recommended to set the client-id . By default, the connector is generating a unique client-id . Important Message coming from MQTT have a byte[] payload. Then, your application receives Message<byte[]> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package mqtt.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class MqttPriceConsumer { @Incoming ( \"prices\" ) public void consume ( byte [] raw ) { double price = Double . parseDouble ( new String ( raw )); // process your price. } } Or, you can retrieve the Message<byte[]> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package mqtt.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class MqttPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < byte []> price ) { // process your price. // Acknowledge the incoming message return price . ack (); } } The inbound topic can use the MQTT wildcards ( + and # ). Deserialization The MQTT Connector does not handle the deserialization and creates a Message<byte[]> . Inbound Metadata The MQTT connector does not provide inbound metadata. Failure Management If a message produced from a MQTT message is nacked , a failure strategy is applied. The MQTT connector supports 3 strategies: fail - fail the application, no more MQTT messages will be processed. (default) The offset of the record that has not been processed correctly is not committed. ignore - the failure is logged, but the processing continue. Configuration Reference Attribute ( alias ) Description Type Mandatory Default auto-clean-session Set to start with a clean session ( true by default) boolean false true auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false true auto-keep-alive Set if the MQTT client must handle PINGREQ automatically boolean false true broadcast Whether or not the messages should be dispatched to multiple consumers boolean false false buffer-size The size buffer of incoming messages waiting to be processed int false 128 client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean ( io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions ) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false 60 failure-strategy Specify the failure strategy to apply when a message produced from a MQTT message is nacked. Values can be fail (default), or ignore string false fail health-enabled Whether health reporting is enabled (default) or disabled boolean false true host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false 30 max-inflight-queue Set max count of unacknowledged messages int false 10 max-message-size Set max MQTT message size in bytes int false 8092 password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false 0 reconnect-interval-seconds Set the reconnect interval in seconds int false 1 server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false false ssl.keystore.location Set the keystore location. In case of pem type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of pem type this is the key path string false ssl.keystore.type Set the keystore type [ pkcs12 , jks , pem ] string false pkcs12 ssl.truststore.location Set the truststore location. In case of pem type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of pem type this is not necessary string false ssl.truststore.type Set the truststore type [ pkcs12 , jks , pem ] string false pkcs12 topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false false unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false false username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false false will-qos Set the QoS level for the will message int false 0 will-retain Set if the will message must be retained boolean false false The MQTT connector is based on the Vert.x MQTT client . So you can pass any attribute supported by this client. Important A single instance of MqttClient and a single connection is used for each host / port / server-name / client-id . This client is reused for both the inbound and outbound connectors. Important Using auto-clean-session=false the MQTT Connector send Subscribe requests to the broken only if a Persistent Session is not present (like on the first connection). This means that if a Session is already present (maybe for a previous run) and you add a new incoming channel, this will not be subscribed. Beware to check always the subscription present on Broker when use auto-clean-session=false .","title":"Receiving MQTT messages"},{"location":"mqtt/receiving-mqtt-messages/#receiving-messages-from-mqtt","text":"The MQTT Connector connects to a MQTT broker or router, and forward the messages to the Reactive Messaging application. It maps each of them into Reactive Messaging Messages .","title":"Receiving messages from MQTT"},{"location":"mqtt/receiving-mqtt-messages/#example","text":"Let\u2019s imagine you have a MQTT server/broker running, and accessible using the mqtt:1883 address (by default it would use localhost:1883 ). Configure your application to receive MQTT messages on the prices channel as follows: 1 2 3 mp.messaging.incoming.prices.connector = smallrye-mqtt # <1> mp.messaging.incoming.prices.host = mqtt # <2> mp.messaging.incoming.prices.port = 1883 # <3> 1. Sets the connector for the prices channel 2. Configure the broker/server host name. 3. Configure the broker/server port. 1883 is the default. Note You don\u2019t need to set the MQTT topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. Note It is generally recommended to set the client-id . By default, the connector is generating a unique client-id . Important Message coming from MQTT have a byte[] payload. Then, your application receives Message<byte[]> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package mqtt.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class MqttPriceConsumer { @Incoming ( \"prices\" ) public void consume ( byte [] raw ) { double price = Double . parseDouble ( new String ( raw )); // process your price. } } Or, you can retrieve the Message<byte[]> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package mqtt.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class MqttPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < byte []> price ) { // process your price. // Acknowledge the incoming message return price . ack (); } } The inbound topic can use the MQTT wildcards ( + and # ).","title":"Example"},{"location":"mqtt/receiving-mqtt-messages/#deserialization","text":"The MQTT Connector does not handle the deserialization and creates a Message<byte[]> .","title":"Deserialization"},{"location":"mqtt/receiving-mqtt-messages/#inbound-metadata","text":"The MQTT connector does not provide inbound metadata.","title":"Inbound Metadata"},{"location":"mqtt/receiving-mqtt-messages/#failure-management","text":"If a message produced from a MQTT message is nacked , a failure strategy is applied. The MQTT connector supports 3 strategies: fail - fail the application, no more MQTT messages will be processed. (default) The offset of the record that has not been processed correctly is not committed. ignore - the failure is logged, but the processing continue.","title":"Failure Management"},{"location":"mqtt/receiving-mqtt-messages/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default auto-clean-session Set to start with a clean session ( true by default) boolean false true auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false true auto-keep-alive Set if the MQTT client must handle PINGREQ automatically boolean false true broadcast Whether or not the messages should be dispatched to multiple consumers boolean false false buffer-size The size buffer of incoming messages waiting to be processed int false 128 client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean ( io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions ) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false 60 failure-strategy Specify the failure strategy to apply when a message produced from a MQTT message is nacked. Values can be fail (default), or ignore string false fail health-enabled Whether health reporting is enabled (default) or disabled boolean false true host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false 30 max-inflight-queue Set max count of unacknowledged messages int false 10 max-message-size Set max MQTT message size in bytes int false 8092 password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false 0 reconnect-interval-seconds Set the reconnect interval in seconds int false 1 server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false false ssl.keystore.location Set the keystore location. In case of pem type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of pem type this is the key path string false ssl.keystore.type Set the keystore type [ pkcs12 , jks , pem ] string false pkcs12 ssl.truststore.location Set the truststore location. In case of pem type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of pem type this is not necessary string false ssl.truststore.type Set the truststore type [ pkcs12 , jks , pem ] string false pkcs12 topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false false unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false false username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false false will-qos Set the QoS level for the will message int false 0 will-retain Set if the will message must be retained boolean false false The MQTT connector is based on the Vert.x MQTT client . So you can pass any attribute supported by this client. Important A single instance of MqttClient and a single connection is used for each host / port / server-name / client-id . This client is reused for both the inbound and outbound connectors. Important Using auto-clean-session=false the MQTT Connector send Subscribe requests to the broken only if a Persistent Session is not present (like on the first connection). This means that if a Session is already present (maybe for a previous run) and you add a new incoming channel, this will not be subscribed. Beware to check always the subscription present on Broker when use auto-clean-session=false .","title":"Configuration Reference"},{"location":"mqtt/sending-messages-to-mqtt/","text":"Sending messages to MQTT The MQTT Connector can write Reactive Messaging Messages as MQTT Message. Example Let\u2019s imagine you have a MQTT server/broker running, and accessible using the mqtt:1883 address (by default it would use localhost:1883 ). Configure your application to write the messages from the prices channel into a MQTT Messages as follows: 1 2 3 mp.messaging.outgoing.prices.type = smallrye-mqtt mp.messaging.outgoing.prices.host = mqtt mp.messaging.outgoing.prices.port = 1883 Sets the connector for the prices channel Configure the broker/server host name. Configure the broker/server port. 1883 is the default. Note You don\u2019t need to set the MQTT topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. NOTE: It is generally recommended to set the client-id . By default, the connector is generating a unique client-id . Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package mqtt.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class MqttPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package mqtt.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class MqttPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } } Serialization The Message sent to MQTT can have various payload types: JsonObject : JSON string encoded as byte[] JsonArray : JSON string encoded as byte[] java.lang.String and Java primitive types: toString encoded as byte[] byte[] complex objects: The objects are encoded to JSON and passed as byte[] Outbound Metadata The MQTT connector does not provide outbound metadata. Acknowledgement MQTT acknowledgement depends on the QoS level. The message is acknowledged when the broker indicated the successful reception of the message (or immediately if the level of QoS does not support acknowledgment). If a MQTT message cannot be sent to the broker, the message is nacked . Configuration Reference Attribute ( alias ) Description Type Mandatory Default auto-clean-session Set to start with a clean session ( true by default) boolean false true auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false true auto-keep-alive Set if the MQTT client must handle PINGREQ automatically boolean false true client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean ( io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions ) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false 60 health-enabled Whether health reporting is enabled (default) or disabled boolean false true host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false 30 max-inflight-queue Set max count of unacknowledged messages int false 10 max-message-size Set max MQTT message size in bytes int false 8092 merge Whether the connector should allow multiple upstreams boolean false false password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false 0 reconnect-interval-seconds Set the reconnect interval in seconds int false 1 server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false false ssl.keystore.location Set the keystore location. In case of pem type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of pem type this is the key path string false ssl.keystore.type Set the keystore type [ pkcs12 , jks , pem ] string false pkcs12 ssl.truststore.location Set the truststore location. In case of pem type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of pem type this is not necessary string false ssl.truststore.type Set the truststore type [ pkcs12 , jks , pem ] string false pkcs12 topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false false unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false false username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false false will-qos Set the QoS level for the will message int false 0 will-retain Set if the will message must be retained boolean false false The MQTT connector is based on the Vert.x MQTT client . So you can pass any attribute supported by this client. Important A single instance of MqttClient and a single connection is used for each host / port / server-name / client-id . This client is reused for both the inbound and outbound connectors.","title":"Sending MQTT messages"},{"location":"mqtt/sending-messages-to-mqtt/#sending-messages-to-mqtt","text":"The MQTT Connector can write Reactive Messaging Messages as MQTT Message.","title":"Sending messages to MQTT"},{"location":"mqtt/sending-messages-to-mqtt/#example","text":"Let\u2019s imagine you have a MQTT server/broker running, and accessible using the mqtt:1883 address (by default it would use localhost:1883 ). Configure your application to write the messages from the prices channel into a MQTT Messages as follows: 1 2 3 mp.messaging.outgoing.prices.type = smallrye-mqtt mp.messaging.outgoing.prices.host = mqtt mp.messaging.outgoing.prices.port = 1883 Sets the connector for the prices channel Configure the broker/server host name. Configure the broker/server port. 1883 is the default. Note You don\u2019t need to set the MQTT topic. By default, it uses the channel name ( prices ). You can configure the topic attribute to override it. NOTE: It is generally recommended to set the client-id . By default, the connector is generating a unique client-id . Then, your application must send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package mqtt.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class MqttPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package mqtt.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class MqttPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble ())); } }","title":"Example"},{"location":"mqtt/sending-messages-to-mqtt/#serialization","text":"The Message sent to MQTT can have various payload types: JsonObject : JSON string encoded as byte[] JsonArray : JSON string encoded as byte[] java.lang.String and Java primitive types: toString encoded as byte[] byte[] complex objects: The objects are encoded to JSON and passed as byte[]","title":"Serialization"},{"location":"mqtt/sending-messages-to-mqtt/#outbound-metadata","text":"The MQTT connector does not provide outbound metadata.","title":"Outbound Metadata"},{"location":"mqtt/sending-messages-to-mqtt/#acknowledgement","text":"MQTT acknowledgement depends on the QoS level. The message is acknowledged when the broker indicated the successful reception of the message (or immediately if the level of QoS does not support acknowledgment). If a MQTT message cannot be sent to the broker, the message is nacked .","title":"Acknowledgement"},{"location":"mqtt/sending-messages-to-mqtt/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default auto-clean-session Set to start with a clean session ( true by default) boolean false true auto-generated-client-id Set if the MQTT client must generate clientId automatically boolean false true auto-keep-alive Set if the MQTT client must handle PINGREQ automatically boolean false true client-id Set the client identifier string false client-options-name (mqtt-client-options-name) The name of the MQTT Client Option bean ( io.smallrye.reactive.messaging.mqtt.session.MqttClientSessionOptions ) used to customize the MQTT client configuration string false connect-timeout-seconds Set the connect timeout (in seconds) int false 60 health-enabled Whether health reporting is enabled (default) or disabled boolean false true host Set the MQTT server host name/IP string true keep-alive-seconds Set the keep alive timeout in seconds int false 30 max-inflight-queue Set max count of unacknowledged messages int false 10 max-message-size Set max MQTT message size in bytes int false 8092 merge Whether the connector should allow multiple upstreams boolean false false password Set the password to connect to the server string false port Set the MQTT server port. Default to 8883 if ssl is enabled, or 1883 without ssl int false qos Set the QoS level when subscribing to the topic or when sending a message int false 0 reconnect-interval-seconds Set the reconnect interval in seconds int false 1 server-name Set the SNI server name string false ssl Set whether SSL/TLS is enabled boolean false false ssl.keystore.location Set the keystore location. In case of pem type this is the server ca cert path string false ssl.keystore.password Set the keystore password. In case of pem type this is the key path string false ssl.keystore.type Set the keystore type [ pkcs12 , jks , pem ] string false pkcs12 ssl.truststore.location Set the truststore location. In case of pem type this is the client cert path string false ssl.truststore.password Set the truststore password. In case of pem type this is not necessary string false ssl.truststore.type Set the truststore type [ pkcs12 , jks , pem ] string false pkcs12 topic Set the MQTT topic. If not set, the channel name is used string false trust-all Set whether all server certificates should be trusted boolean false false unsubscribe-on-disconnection This flag restore the old behavior to unsubscribe from the broken on disconnection boolean false false username Set the username to connect to the server string false will-flag Set if will information are provided on connection boolean false false will-qos Set the QoS level for the will message int false 0 will-retain Set if the will message must be retained boolean false false The MQTT connector is based on the Vert.x MQTT client . So you can pass any attribute supported by this client. Important A single instance of MqttClient and a single connection is used for each host / port / server-name / client-id . This client is reused for both the inbound and outbound connectors.","title":"Configuration Reference"},{"location":"rabbitmq/rabbitmq-client-customization/","text":"Customizing the underlying RabbitMQ client You can customize the underlying RabbitMQ Client configuration by producing an instance of RabbitMQOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Produces @Identifier ( \"my-named-options\" ) public RabbitMQOptions getNamedOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return ( RabbitMQOptions ) new RabbitMQOptions () . setUser ( \"admin\" ) . setPassword ( \"test\" ) . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name=my-named-options","title":"Client Customization"},{"location":"rabbitmq/rabbitmq-client-customization/#customizing-the-underlying-rabbitmq-client","text":"You can customize the underlying RabbitMQ Client configuration by producing an instance of RabbitMQOptions : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Produces @Identifier ( \"my-named-options\" ) public RabbitMQOptions getNamedOptions () { // You can use the produced options to configure the TLS connection PemKeyCertOptions keycert = new PemKeyCertOptions () . addCertPath ( \"./tls/tls.crt\" ) . addKeyPath ( \"./tls/tls.key\" ); PemTrustOptions trust = new PemTrustOptions (). addCertPath ( \"./tlc/ca.crt\" ); return ( RabbitMQOptions ) new RabbitMQOptions () . setUser ( \"admin\" ) . setPassword ( \"test\" ) . setSsl ( true ) . setPemKeyCertOptions ( keycert ) . setPemTrustOptions ( trust ) . setHostnameVerificationAlgorithm ( \"\" ) . setConnectTimeout ( 30000 ) . setReconnectInterval ( 5000 ); } This instance is retrieved and used to configure the client used by the connector. You need to indicate the name of the client using the client-options-name attribute: 1 mp.messaging.incoming.prices.client-options-name=my-named-options","title":"Customizing the underlying RabbitMQ client"},{"location":"rabbitmq/rabbitmq-cloud/","text":"Connecting to managed instances This section describes the connector configuration to use managed RabbitMQ instances (hosted on the Cloud). Cloud AMQP To connect to an instance of RabbitMQ hosted on Cloud AMQP , use the following configuration: 1 2 3 4 5 6 rabbitmq-host = host-name rabbitmq-port = 5671 rabbitmq-username = user-name rabbitmq-password = password rabbitmq-virtual-host = user-name rabbitmq-ssl = true You can extract the values from the AMQPS url displayed on the administration portal: 1 amqps://user-name:password@host/user-name Amazon MQ Amazon MQ can host RabbitMQ brokers (as well as AMQP 1.0 brokers). To connect to a RabbitMQ instance hosted on Amazon MQ, use the following configuration: 1 2 3 4 5 rabbitmq-host = host-name rabbitmq-port = 5671 rabbitmq-username = user-name rabbitmq-password = password rabbitmq-ssl = true You can extract the host value from the AMQPS url displayed on the administration console: 1 amqps://foobarbaz.mq.us-east-2.amazonaws.com:5671 The username and password are configured during the broker creation.","title":"Connecting to managed instances"},{"location":"rabbitmq/rabbitmq-cloud/#connecting-to-managed-instances","text":"This section describes the connector configuration to use managed RabbitMQ instances (hosted on the Cloud).","title":"Connecting to managed instances"},{"location":"rabbitmq/rabbitmq-cloud/#cloud-amqp","text":"To connect to an instance of RabbitMQ hosted on Cloud AMQP , use the following configuration: 1 2 3 4 5 6 rabbitmq-host = host-name rabbitmq-port = 5671 rabbitmq-username = user-name rabbitmq-password = password rabbitmq-virtual-host = user-name rabbitmq-ssl = true You can extract the values from the AMQPS url displayed on the administration portal: 1 amqps://user-name:password@host/user-name","title":"Cloud AMQP"},{"location":"rabbitmq/rabbitmq-cloud/#amazon-mq","text":"Amazon MQ can host RabbitMQ brokers (as well as AMQP 1.0 brokers). To connect to a RabbitMQ instance hosted on Amazon MQ, use the following configuration: 1 2 3 4 5 rabbitmq-host = host-name rabbitmq-port = 5671 rabbitmq-username = user-name rabbitmq-password = password rabbitmq-ssl = true You can extract the host value from the AMQPS url displayed on the administration console: 1 amqps://foobarbaz.mq.us-east-2.amazonaws.com:5671 The username and password are configured during the broker creation.","title":"Amazon MQ"},{"location":"rabbitmq/rabbitmq-health/","text":"Health reporting The RabbitMQ connector reports the readiness and liveness of each channel managed by the connector. On the inbound side (receiving messages from RabbitMQ), the check verifies that the receiver is connected to the broker. On the outbound side (sending records to RabbitMQ), the check verifies that the sender is not disconnected from the broker; the sender may still be in an initialized state (connection not yet attempted), but this is regarded as live/ready.","title":"Health Checks"},{"location":"rabbitmq/rabbitmq-health/#health-reporting","text":"The RabbitMQ connector reports the readiness and liveness of each channel managed by the connector. On the inbound side (receiving messages from RabbitMQ), the check verifies that the receiver is connected to the broker. On the outbound side (sending records to RabbitMQ), the check verifies that the sender is not disconnected from the broker; the sender may still be in an initialized state (connection not yet attempted), but this is regarded as live/ready.","title":"Health reporting"},{"location":"rabbitmq/rabbitmq/","text":"RabbitMQ Connector The RabbitMQ Connector adds support for RabbitMQ to Reactive Messaging, based on the AMQP 0-9-1 Protocol Specification. Advanced Message Queuing Protocol 0-9-1 ( AMQP 0-9-1 ) is an open standard for passing business messages between applications or organizations. With this connector, your application can: receive messages from a RabbitMQ queue send messages to a RabbitMQ exchange The RabbitMQ connector is based on the Vert.x RabbitMQ Client . Important The AMQP connector supports the AMQP 1.0 protocol, which is very different from AMQP 0-9-1. You can use the AMQP connector with RabbitMQ provided that the latter has the AMQP 1.0 Plugin installed, albeit with reduced functionality. Using the RabbitMQ connector To use the RabbitMQ Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-rabbitmq </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-rabbitmq . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-rabbitmq # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-rabbitmq","title":"RabbitMQ Connector"},{"location":"rabbitmq/rabbitmq/#rabbitmq-connector","text":"The RabbitMQ Connector adds support for RabbitMQ to Reactive Messaging, based on the AMQP 0-9-1 Protocol Specification. Advanced Message Queuing Protocol 0-9-1 ( AMQP 0-9-1 ) is an open standard for passing business messages between applications or organizations. With this connector, your application can: receive messages from a RabbitMQ queue send messages to a RabbitMQ exchange The RabbitMQ connector is based on the Vert.x RabbitMQ Client . Important The AMQP connector supports the AMQP 1.0 protocol, which is very different from AMQP 0-9-1. You can use the AMQP connector with RabbitMQ provided that the latter has the AMQP 1.0 Plugin installed, albeit with reduced functionality.","title":"RabbitMQ Connector"},{"location":"rabbitmq/rabbitmq/#using-the-rabbitmq-connector","text":"To use the RabbitMQ Connector, add the following dependency to your project: 1 2 3 4 5 <dependency> <groupId> io.smallrye.reactive </groupId> <artifactId> smallrye-reactive-messaging-rabbitmq </artifactId> <version> 4.3.0 </version> </dependency> The connector name is: smallrye-rabbitmq . So, to indicate that a channel is managed by this connector you need: 1 2 3 4 5 # Inbound mp.messaging.incoming.[channel-name].connector = smallrye-rabbitmq # Outbound mp.messaging.outgoing.[channel-name].connector = smallrye-rabbitmq","title":"Using the RabbitMQ connector"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/","text":"Receiving messages from RabbitMQ The RabbitMQ connector lets you retrieve messages from a RabbitMQ broker . The RabbitMQ connector retrieves RabbitMQ Messages and maps each of them into Reactive Messaging Messages . Note In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Queue . Example Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the rabbitmq:5672 address (by default it would use localhost:5672 ). Configure your application to receive RabbitMQ Messages on the prices channel as follows: 1 2 3 4 5 6 7 8 rabbitmq-host = rabbitmq # <1> rabbitmq-port = 5672 # <2> rabbitmq-username = my-username # <3> rabbitmq-password = my-password # <4> mp.messaging.incoming.prices.connector = smallrye-rabbitmq # <5> mp.messaging.incoming.prices.queue.name = my-queue # <6> mp.messaging.incoming.prices.routing-keys = urgent # <7> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using rabbitmq-host . Configures the broker/router port. You can do it per channel (using the port attribute) or globally using rabbitmq-port . The default is 5672. Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using rabbitmq-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using rabbitmq-password . Instructs the prices channel to be managed by the RabbitMQ connector. Configures the RabbitMQ queue to read messages from. Configures the binding between the RabbitMQ exchange and the RabbitMQ queue using a routing key. The default is # (all messages will be forwarded from the exchange to the queue) but in general this can be a comma-separated list of one or more keys. Then, your application receives Message<String> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package rabbitmq.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class RabbitMQPriceConsumer { @Incoming ( \"prices\" ) public void consume ( String price ) { // process your price. } } Or, you can retrieve the Message<String> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package rabbitmq.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class RabbitMQPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < String > price ) { // process your price. // Acknowledge the incoming message, marking the RabbitMQ message as `accepted`. return price . ack (); } } Note Whether you need to explicitly acknowledge the message depends on the auto-acknowledgement channel setting; if that is set to true then your message will be automatically acknowledged on receipt. Deserialization The connector converts incoming RabbitMQ Messages into Reactive Messaging Message<T> instances. The payload type T depends on the value of the RabbitMQ received message Envelope content_type and content_encoding properties. content_encoding content_type Type Value present n/a byte[] No value text/plain String No value application/json a JSON element which can be a JsonArray , JsonObject , String , ... if the buffer contains an array, object, string,... No value Anything else byte[] If you send objects with this RabbitMQ connector (outbound connector), they are encoded as JSON and sent with content_type set to application/json . You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @ApplicationScoped public static class Generator { @Outgoing ( \"to-rabbitmq\" ) public Multi < Price > prices () { // <1> AtomicInteger count = new AtomicInteger (); return Multi . createFrom (). ticks (). every ( Duration . ofMillis ( 1000 )) . map ( l -> new Price (). setPrice ( count . incrementAndGet ())) . onOverflow (). drop (); } } @ApplicationScoped public static class Consumer { List < Price > prices = new CopyOnWriteArrayList <> (); @Incoming ( \"from-rabbitmq\" ) public void consume ( JsonObject p ) { // <2> Price price = p . mapTo ( Price . class ); // <3> prices . add ( price ); } public List < Price > list () { return prices ; } } The Price instances are automatically encoded to JSON by the connector You can receive it using a JsonObject Then, you can reconstruct the instance using the mapTo method Inbound Metadata Messages coming from RabbitMQ contain an instance of IncomingRabbitMQMetadata in the metadata. RabbitMQ message headers can be accessed from the metadata either by calling getHeader(String header, Class<T> type) to retrieve a single header value. or getHeaders() to get a map of all header values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 final Optional < IncomingRabbitMQMetadata > metadata = incomingMessage . getMetadata ( IncomingRabbitMQMetadata . class ); metadata . ifPresent ( meta -> { final Optional < String > contentEncoding = meta . getContentEncoding (); final Optional < String > contentType = meta . getContentType (); final Optional < String > correlationId = meta . getCorrelationId (); final Optional < ZonedDateTime > timestamp = meta . getTimestamp ( ZoneId . systemDefault ()); final Optional < Integer > priority = meta . getPriority (); final Optional < String > replyTo = meta . getReplyTo (); final Optional < String > userId = meta . getUserId (); // Access a single String-valued header final Optional < String > stringHeader = meta . getHeader ( \"my-header\" , String . class ); // Access all headers final Map < String , Object > headers = meta . getHeaders (); // ... }); The type <T> of the header value depends on the RabbitMQ type used for the header: RabbitMQ Header Type T String String Boolean Boolean Number Number List java.util.List Acknowledgement When a Reactive Messaging Message associated with a RabbitMQ Message is acknowledged, it informs the broker that the message has been accepted . Whether you need to explicitly acknowledge the message depends on the auto-acknowledgement setting for the channel; if that is set to true then your message will be automatically acknowledged on receipt. Failure Management If a message produced from a RabbitMQ message is nacked , a failure strategy is applied. The RabbitMQ connector supports three strategies, controlled by the failure-strategy channel setting: fail - fail the application; no more RabbitMQ messages will be processed. The RabbitMQ message is marked as rejected. accept - this strategy marks the RabbitMQ message as accepted. The processing continues ignoring the failure. reject - this strategy marks the RabbitMQ message as rejected (default). The processing continues with the next message. Configuration Reference Attribute ( alias ) Description Type Mandatory Default auto-acknowledgement Whether the received RabbitMQ messages must be acknowledged when received; if true then delivery constitutes acknowledgement boolean false false auto-bind-dlq Whether to automatically declare the DLQ and bind it to the binder DLX boolean false false automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false false automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false true broadcast Whether the received RabbitMQ messages must be dispatched to multiple subscribers boolean false false client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-count The number of RabbitMQ connections to create for consuming from this queue. This might be necessary to consume from a sharded queue with a single client. int false 1 connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false 60000 content-type-override Override the content_type attribute of the incoming message, should be a valid MINE type string false credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false dead-letter-exchange A DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false DLX dead-letter-exchange-type The type of the DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false direct dead-letter-queue-mode If automatically declare DLQ, we can choose different modes of DLQ [lazy, default] string false dead-letter-queue-name The name of the DLQ; if not supplied will default to the queue name with '.dlq' appended string false dead-letter-queue-type If automatically declare DLQ, we can choose different types of DLQ [quorum, classic, stream] string false dead-letter-routing-key A dead letter routing key to assign to the queue; if not supplied will default to the queue name string false dlx.declare Whether to declare the dead letter exchange binding. Relevant only if auto-bind-dlq is true; set to false if these are expected to be set up independently boolean false false exchange.auto-delete Whether the exchange should be deleted after use boolean false false exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false true exchange.durable Whether the exchange is durable boolean false true exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false topic failure-strategy The failure strategy to apply when a RabbitMQ message is nacked. Accepted values are fail , accept , reject (default) string false reject handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false 10000 host (rabbitmq-host) The broker hostname string false localhost include-properties Whether to include properties when a broker message is passed on the event bus boolean false false keep-most-recent Whether to discard old messages instead of recent ones boolean false false max-incoming-internal-queue-size The maximum size of the incoming internal queue int false 500000 max-outstanding-messages The maximum number of outstanding/unacknowledged messages being processed by the connector at a time; must be a positive number int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false 5000 password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false 5672 queue.auto-delete Whether the queue should be deleted after use boolean false false queue.declare Whether to declare the queue and binding; set to false if these are expected to be set up independently boolean false true queue.durable Whether the queue is durable boolean false true queue.exclusive Whether the queue is for exclusive use boolean false false queue.name The queue from which messages are consumed. string true queue.single-active-consumer If set to true, only one consumer can actively consume messages boolean false queue.ttl If specified, the time (ms) for which a message can remain in the queue undelivered before it is dead long false queue.x-max-priority Define priority level queue consumer int false queue.x-queue-mode If automatically declare queue, we can choose different modes of queue [lazy, default] string false queue.x-queue-type If automatically declare queue, we can choose different types of queue [quorum, classic, stream] string false reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false 10 requested-channel-max The initially requested maximum channel number int false 2047 requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false 60 routing-keys A comma-separated list of routing keys to bind the queue to the exchange string false # ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false false tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false true trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false false trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false false user The user name to use when connecting to the broker string false guest username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false / To use an existing queue , you need to configure the queue.name attribute. For example, if you have a RabbitMQ broker already configured with a queue called peopleQueue that you wish to read messages from, you need the following configuration: 1 2 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.queue.name = peopleQueue If you want RabbitMQ to create the queue for you but bind it to an existing topic exchange people , you need the following configuration: 1 2 3 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.queue.name = peopleQueue mp.messaging.incoming.people.queue.declare = true Note In the above the channel name people is implicitly assumed to be the name of the exchange; if this is not the case you would need to name the exchange explicitly using the exchange.name property. Note The connector supports RabbitMQ's \"Server-named Queues\" feature to create an exclusive, auto-deleting, non-durable and randomly named queue. To enable this feature you set the queue name to exactly (server.auto) . Using this name not only enables the server named queue feature but also automatically makes ths queue exclusive, auto-deleting, and non-durable; therefore ignoring any values provided for the exclusive , auto-delete and durable options. If you want RabbitMQ to create the people exchange, queue and binding, you need the following configuration: 1 2 3 4 5 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.exchange.declare = true mp.messaging.incoming.people.queue.name = peopleQueue mp.messaging.incoming.people.queue.declare = true mp.messaging.incoming.people.queue.routing-keys = tall,short In the above we have used an explicit list of routing keys rather than the default ( # ). Each component of the list creates a separate binding between the queue and the exchange, so in the case above we would have two bindings; one based on a routing key of tall , the other based on one of short . Note The default value of routing-keys is # (indicating a match against all possible routing keys) which is only appropriate for topic Exchanges. If you are using other types of exchange and/or need to declare queue bindings, you\u2019ll need to supply a valid value for the exchange in question.","title":"Receiving messages"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#receiving-messages-from-rabbitmq","text":"The RabbitMQ connector lets you retrieve messages from a RabbitMQ broker . The RabbitMQ connector retrieves RabbitMQ Messages and maps each of them into Reactive Messaging Messages . Note In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Queue .","title":"Receiving messages from RabbitMQ"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#example","text":"Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the rabbitmq:5672 address (by default it would use localhost:5672 ). Configure your application to receive RabbitMQ Messages on the prices channel as follows: 1 2 3 4 5 6 7 8 rabbitmq-host = rabbitmq # <1> rabbitmq-port = 5672 # <2> rabbitmq-username = my-username # <3> rabbitmq-password = my-password # <4> mp.messaging.incoming.prices.connector = smallrye-rabbitmq # <5> mp.messaging.incoming.prices.queue.name = my-queue # <6> mp.messaging.incoming.prices.routing-keys = urgent # <7> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using rabbitmq-host . Configures the broker/router port. You can do it per channel (using the port attribute) or globally using rabbitmq-port . The default is 5672. Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using rabbitmq-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using rabbitmq-password . Instructs the prices channel to be managed by the RabbitMQ connector. Configures the RabbitMQ queue to read messages from. Configures the binding between the RabbitMQ exchange and the RabbitMQ queue using a routing key. The default is # (all messages will be forwarded from the exchange to the queue) but in general this can be a comma-separated list of one or more keys. Then, your application receives Message<String> . You can consume the payload directly: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package rabbitmq.inbound ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; @ApplicationScoped public class RabbitMQPriceConsumer { @Incoming ( \"prices\" ) public void consume ( String price ) { // process your price. } } Or, you can retrieve the Message<String> : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package rabbitmq.inbound ; import java.util.concurrent.CompletionStage ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Incoming ; import org.eclipse.microprofile.reactive.messaging.Message ; @ApplicationScoped public class RabbitMQPriceMessageConsumer { @Incoming ( \"prices\" ) public CompletionStage < Void > consume ( Message < String > price ) { // process your price. // Acknowledge the incoming message, marking the RabbitMQ message as `accepted`. return price . ack (); } } Note Whether you need to explicitly acknowledge the message depends on the auto-acknowledgement channel setting; if that is set to true then your message will be automatically acknowledged on receipt.","title":"Example"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#deserialization","text":"The connector converts incoming RabbitMQ Messages into Reactive Messaging Message<T> instances. The payload type T depends on the value of the RabbitMQ received message Envelope content_type and content_encoding properties. content_encoding content_type Type Value present n/a byte[] No value text/plain String No value application/json a JSON element which can be a JsonArray , JsonObject , String , ... if the buffer contains an array, object, string,... No value Anything else byte[] If you send objects with this RabbitMQ connector (outbound connector), they are encoded as JSON and sent with content_type set to application/json . You can receive this payload using (Vert.x) JSON Objects, and then map it to the object class you want: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @ApplicationScoped public static class Generator { @Outgoing ( \"to-rabbitmq\" ) public Multi < Price > prices () { // <1> AtomicInteger count = new AtomicInteger (); return Multi . createFrom (). ticks (). every ( Duration . ofMillis ( 1000 )) . map ( l -> new Price (). setPrice ( count . incrementAndGet ())) . onOverflow (). drop (); } } @ApplicationScoped public static class Consumer { List < Price > prices = new CopyOnWriteArrayList <> (); @Incoming ( \"from-rabbitmq\" ) public void consume ( JsonObject p ) { // <2> Price price = p . mapTo ( Price . class ); // <3> prices . add ( price ); } public List < Price > list () { return prices ; } } The Price instances are automatically encoded to JSON by the connector You can receive it using a JsonObject Then, you can reconstruct the instance using the mapTo method","title":"Deserialization"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#inbound-metadata","text":"Messages coming from RabbitMQ contain an instance of IncomingRabbitMQMetadata in the metadata. RabbitMQ message headers can be accessed from the metadata either by calling getHeader(String header, Class<T> type) to retrieve a single header value. or getHeaders() to get a map of all header values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 final Optional < IncomingRabbitMQMetadata > metadata = incomingMessage . getMetadata ( IncomingRabbitMQMetadata . class ); metadata . ifPresent ( meta -> { final Optional < String > contentEncoding = meta . getContentEncoding (); final Optional < String > contentType = meta . getContentType (); final Optional < String > correlationId = meta . getCorrelationId (); final Optional < ZonedDateTime > timestamp = meta . getTimestamp ( ZoneId . systemDefault ()); final Optional < Integer > priority = meta . getPriority (); final Optional < String > replyTo = meta . getReplyTo (); final Optional < String > userId = meta . getUserId (); // Access a single String-valued header final Optional < String > stringHeader = meta . getHeader ( \"my-header\" , String . class ); // Access all headers final Map < String , Object > headers = meta . getHeaders (); // ... }); The type <T> of the header value depends on the RabbitMQ type used for the header: RabbitMQ Header Type T String String Boolean Boolean Number Number List java.util.List","title":"Inbound Metadata"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#acknowledgement","text":"When a Reactive Messaging Message associated with a RabbitMQ Message is acknowledged, it informs the broker that the message has been accepted . Whether you need to explicitly acknowledge the message depends on the auto-acknowledgement setting for the channel; if that is set to true then your message will be automatically acknowledged on receipt.","title":"Acknowledgement"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#failure-management","text":"If a message produced from a RabbitMQ message is nacked , a failure strategy is applied. The RabbitMQ connector supports three strategies, controlled by the failure-strategy channel setting: fail - fail the application; no more RabbitMQ messages will be processed. The RabbitMQ message is marked as rejected. accept - this strategy marks the RabbitMQ message as accepted. The processing continues ignoring the failure. reject - this strategy marks the RabbitMQ message as rejected (default). The processing continues with the next message.","title":"Failure Management"},{"location":"rabbitmq/receiving-messages-from-rabbitmq/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default auto-acknowledgement Whether the received RabbitMQ messages must be acknowledged when received; if true then delivery constitutes acknowledgement boolean false false auto-bind-dlq Whether to automatically declare the DLQ and bind it to the binder DLX boolean false false automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false false automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false true broadcast Whether the received RabbitMQ messages must be dispatched to multiple subscribers boolean false false client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-count The number of RabbitMQ connections to create for consuming from this queue. This might be necessary to consume from a sharded queue with a single client. int false 1 connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false 60000 content-type-override Override the content_type attribute of the incoming message, should be a valid MINE type string false credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false dead-letter-exchange A DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false DLX dead-letter-exchange-type The type of the DLX to assign to the queue. Relevant only if auto-bind-dlq is true string false direct dead-letter-queue-mode If automatically declare DLQ, we can choose different modes of DLQ [lazy, default] string false dead-letter-queue-name The name of the DLQ; if not supplied will default to the queue name with '.dlq' appended string false dead-letter-queue-type If automatically declare DLQ, we can choose different types of DLQ [quorum, classic, stream] string false dead-letter-routing-key A dead letter routing key to assign to the queue; if not supplied will default to the queue name string false dlx.declare Whether to declare the dead letter exchange binding. Relevant only if auto-bind-dlq is true; set to false if these are expected to be set up independently boolean false false exchange.auto-delete Whether the exchange should be deleted after use boolean false false exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false true exchange.durable Whether the exchange is durable boolean false true exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false topic failure-strategy The failure strategy to apply when a RabbitMQ message is nacked. Accepted values are fail , accept , reject (default) string false reject handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false 10000 host (rabbitmq-host) The broker hostname string false localhost include-properties Whether to include properties when a broker message is passed on the event bus boolean false false keep-most-recent Whether to discard old messages instead of recent ones boolean false false max-incoming-internal-queue-size The maximum size of the incoming internal queue int false 500000 max-outstanding-messages The maximum number of outstanding/unacknowledged messages being processed by the connector at a time; must be a positive number int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false 5000 password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false 5672 queue.auto-delete Whether the queue should be deleted after use boolean false false queue.declare Whether to declare the queue and binding; set to false if these are expected to be set up independently boolean false true queue.durable Whether the queue is durable boolean false true queue.exclusive Whether the queue is for exclusive use boolean false false queue.name The queue from which messages are consumed. string true queue.single-active-consumer If set to true, only one consumer can actively consume messages boolean false queue.ttl If specified, the time (ms) for which a message can remain in the queue undelivered before it is dead long false queue.x-max-priority Define priority level queue consumer int false queue.x-queue-mode If automatically declare queue, we can choose different modes of queue [lazy, default] string false queue.x-queue-type If automatically declare queue, we can choose different types of queue [quorum, classic, stream] string false reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false 10 requested-channel-max The initially requested maximum channel number int false 2047 requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false 60 routing-keys A comma-separated list of routing keys to bind the queue to the exchange string false # ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false false tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false true trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false false trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false false user The user name to use when connecting to the broker string false guest username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false / To use an existing queue , you need to configure the queue.name attribute. For example, if you have a RabbitMQ broker already configured with a queue called peopleQueue that you wish to read messages from, you need the following configuration: 1 2 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.queue.name = peopleQueue If you want RabbitMQ to create the queue for you but bind it to an existing topic exchange people , you need the following configuration: 1 2 3 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.queue.name = peopleQueue mp.messaging.incoming.people.queue.declare = true Note In the above the channel name people is implicitly assumed to be the name of the exchange; if this is not the case you would need to name the exchange explicitly using the exchange.name property. Note The connector supports RabbitMQ's \"Server-named Queues\" feature to create an exclusive, auto-deleting, non-durable and randomly named queue. To enable this feature you set the queue name to exactly (server.auto) . Using this name not only enables the server named queue feature but also automatically makes ths queue exclusive, auto-deleting, and non-durable; therefore ignoring any values provided for the exclusive , auto-delete and durable options. If you want RabbitMQ to create the people exchange, queue and binding, you need the following configuration: 1 2 3 4 5 mp.messaging.incoming.people.connector = smallrye-rabbitmq mp.messaging.incoming.people.exchange.declare = true mp.messaging.incoming.people.queue.name = peopleQueue mp.messaging.incoming.people.queue.declare = true mp.messaging.incoming.people.queue.routing-keys = tall,short In the above we have used an explicit list of routing keys rather than the default ( # ). Each component of the list creates a separate binding between the queue and the exchange, so in the case above we would have two bindings; one based on a routing key of tall , the other based on one of short . Note The default value of routing-keys is # (indicating a match against all possible routing keys) which is only appropriate for topic Exchanges. If you are using other types of exchange and/or need to declare queue bindings, you\u2019ll need to supply a valid value for the exchange in question.","title":"Configuration Reference"},{"location":"rabbitmq/sending-messages-to-rabbitmq/","text":"Sending messages to RabbitMQ The RabbitMQ connector can write Reactive Messaging Messages as RabbitMQ Messages. Note In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Exchange . Example Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the rabbitmq:5672 address (by default it would use localhost:5672 ). Configure your application to send the messages from the prices channel as a RabbitMQ Message as follows: 1 2 3 4 5 6 7 rabbitmq-host=rabbitmq # <1> rabbitmq-port=5672 # <2> rabbitmq-username=my-username # <3> rabbitmq-password=my-password # <4> mp.messaging.outgoing.prices.connector=smallrye-rabbitmq # <5> mp.messaging.outgoing.prices.default-routing-key=normal # <6> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using rabbitmq-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using rabbitmq-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using rabbitmq-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using rabbitmq-password . Instructs the prices channel to be managed by the RabbitMQ connector Supplies the default routing key to be included in outbound messages; this will be if the \"raw payload\" form of message sending is used (see below). Note You don\u2019t need to set the RabbitMQ exchange name. By default, it uses the channel name ( prices ) as the name of the exchange to send messages to. You can configure the exchange.name attribute to override it. Then, your application can send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package rabbitmq.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class RabbitMQPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> , which affords the opportunity to explicitly specify metadata on the outgoing message: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package rabbitmq.outbound ; import java.time.Duration ; import java.time.ZonedDateTime ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Metadata ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; import io.smallrye.reactive.messaging.rabbitmq.OutgoingRabbitMQMetadata ; @ApplicationScoped public class RabbitMQPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble (), Metadata . of ( new OutgoingRabbitMQMetadata . Builder () . withRoutingKey ( \"normal\" ) . withTimestamp ( ZonedDateTime . now ()) . build ()))); } } Serialization When sending a Message<T> , the connector converts the message into a RabbitMQ Message. The payload is converted to the RabbitMQ Message body. T RabbitMQ Message Body primitive types or UUID / String String value with content_type set to text/plain JsonObject or JsonArray Serialized String payload with content_type set to application/json io.vertx.mutiny.core.buffer.Buffer Binary content, with content_type set to application/octet-stream byte[] Binary content, with content_type set to application/octet-stream Any other class The payload is converted to JSON (using a Json Mapper) then serialized with content_type set to application/json If the message payload cannot be serialized to JSON, the message is nacked . Outbound Metadata When sending Messages , you can add an instance of OutgoingRabbitMQMetadata to influence how the message is handled by RabbitMQ. For example, you can configure the routing key, timestamp and headers: 1 2 3 4 5 6 7 8 final OutgoingRabbitMQMetadata metadata = new OutgoingRabbitMQMetadata . Builder () . withHeader ( \"my-header\" , \"xyzzy\" ) . withRoutingKey ( \"urgent\" ) . withTimestamp ( ZonedDateTime . now ()) . build (); // Add `metadata` to the metadata of the outgoing message. return Message . of ( \"Hello\" , Metadata . of ( metadata )); Acknowledgement By default, the Reactive Messaging Message is acknowledged when the broker acknowledges the message. Configuration Reference Attribute ( alias ) Description Type Mandatory Default automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false false automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false true client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false 60000 credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false default-routing-key The default routing key to use when sending messages to the exchange string false `` default-ttl If specified, the time (ms) sent messages can remain in queues undelivered before they are dead long false exchange.auto-delete Whether the exchange should be deleted after use boolean false false exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false true exchange.durable Whether the exchange is durable boolean false true exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false topic handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false 10000 host (rabbitmq-host) The broker hostname string false localhost include-properties Whether to include properties when a broker message is passed on the event bus boolean false false max-inflight-messages The maximum number of messages to be written to RabbitMQ concurrently; must be a positive number long false 1024 max-outgoing-internal-queue-size The maximum size of the outgoing internal queue int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false 5000 password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false 5672 reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false 10 requested-channel-max The initially requested maximum channel number int false 2047 requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false 60 ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false false tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false true trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false false trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false false user The user name to use when connecting to the broker string false guest username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false / Using existing destinations To use an existing exchange , you may need to configure the exchange.name attribute. For example, if you have a RabbitMQ broker already configured with an exchange called people that you wish to send messages to, you need the following configuration: 1 mp.messaging.outgoing.people.connector = smallrye-rabbitmq You would need to configure the exchange.name attribute, if the exchange name were not the channel name: 1 2 mp.messaging.outgoing.people-out.connector = smallrye-rabbitmq mp.messaging.outgoing.people-out.exchange.name = people If you want RabbitMQ to create the people exchange, you need the following configuration: 1 2 3 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.exchange.name = people mp.messaging.outgoing.people-out.exchange.declare = true Note The above example will create a topic exchange and use an empty default routing-key (unless overridden programatically using outgoing metadata for the message). If you want to create a different type of exchange or have a different default routing key, then the exchange.type and default-routing-key properties need to be explicitly specified. Sending to specific queues via the default exchange To send a message to a specific queue (usually a reply queue), you have to configure the default exchange as an outgoing channel and set the name of the queue as routing key in the message metadata. The name of the exchange needs to be set to \"\" . 1 2 mp.messaging.outgoing.channel-name-for-default-exchange.connector = smallrye-rabbitmq mp.messaging.outgoing.channel-name-for-default-exchange.exchange.name = \"\"","title":"Sending messages"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#sending-messages-to-rabbitmq","text":"The RabbitMQ connector can write Reactive Messaging Messages as RabbitMQ Messages. Note In this context, the reactive messaging concept of a Channel is realised as a RabbitMQ Exchange .","title":"Sending messages to RabbitMQ"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#example","text":"Let\u2019s imagine you have a RabbitMQ broker running, and accessible using the rabbitmq:5672 address (by default it would use localhost:5672 ). Configure your application to send the messages from the prices channel as a RabbitMQ Message as follows: 1 2 3 4 5 6 7 rabbitmq-host=rabbitmq # <1> rabbitmq-port=5672 # <2> rabbitmq-username=my-username # <3> rabbitmq-password=my-password # <4> mp.messaging.outgoing.prices.connector=smallrye-rabbitmq # <5> mp.messaging.outgoing.prices.default-routing-key=normal # <6> Configures the broker/router host name. You can do it per channel (using the host attribute) or globally using rabbitmq-host Configures the broker/router port. You can do it per channel (using the port attribute) or globally using rabbitmq-port . The default is 5672 . Configures the broker/router username if required. You can do it per channel (using the username attribute) or globally using rabbitmq-username . Configures the broker/router password if required. You can do it per channel (using the password attribute) or globally using rabbitmq-password . Instructs the prices channel to be managed by the RabbitMQ connector Supplies the default routing key to be included in outbound messages; this will be if the \"raw payload\" form of message sending is used (see below). Note You don\u2019t need to set the RabbitMQ exchange name. By default, it uses the channel name ( prices ) as the name of the exchange to send messages to. You can configure the exchange.name attribute to override it. Then, your application can send Message<Double> to the prices channel. It can use double payloads as in the following snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package rabbitmq.outbound ; import java.time.Duration ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; @ApplicationScoped public class RabbitMQPriceProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Double > generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> random . nextDouble ()); } } Or, you can send Message<Double> , which affords the opportunity to explicitly specify metadata on the outgoing message: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package rabbitmq.outbound ; import java.time.Duration ; import java.time.ZonedDateTime ; import java.util.Random ; import jakarta.enterprise.context.ApplicationScoped ; import org.eclipse.microprofile.reactive.messaging.Message ; import org.eclipse.microprofile.reactive.messaging.Metadata ; import org.eclipse.microprofile.reactive.messaging.Outgoing ; import io.smallrye.mutiny.Multi ; import io.smallrye.reactive.messaging.rabbitmq.OutgoingRabbitMQMetadata ; @ApplicationScoped public class RabbitMQPriceMessageProducer { private Random random = new Random (); @Outgoing ( \"prices\" ) public Multi < Message < Double >> generate () { // Build an infinite stream of random prices // It emits a price every second return Multi . createFrom (). ticks (). every ( Duration . ofSeconds ( 1 )) . map ( x -> Message . of ( random . nextDouble (), Metadata . of ( new OutgoingRabbitMQMetadata . Builder () . withRoutingKey ( \"normal\" ) . withTimestamp ( ZonedDateTime . now ()) . build ()))); } }","title":"Example"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#serialization","text":"When sending a Message<T> , the connector converts the message into a RabbitMQ Message. The payload is converted to the RabbitMQ Message body. T RabbitMQ Message Body primitive types or UUID / String String value with content_type set to text/plain JsonObject or JsonArray Serialized String payload with content_type set to application/json io.vertx.mutiny.core.buffer.Buffer Binary content, with content_type set to application/octet-stream byte[] Binary content, with content_type set to application/octet-stream Any other class The payload is converted to JSON (using a Json Mapper) then serialized with content_type set to application/json If the message payload cannot be serialized to JSON, the message is nacked .","title":"Serialization"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#outbound-metadata","text":"When sending Messages , you can add an instance of OutgoingRabbitMQMetadata to influence how the message is handled by RabbitMQ. For example, you can configure the routing key, timestamp and headers: 1 2 3 4 5 6 7 8 final OutgoingRabbitMQMetadata metadata = new OutgoingRabbitMQMetadata . Builder () . withHeader ( \"my-header\" , \"xyzzy\" ) . withRoutingKey ( \"urgent\" ) . withTimestamp ( ZonedDateTime . now ()) . build (); // Add `metadata` to the metadata of the outgoing message. return Message . of ( \"Hello\" , Metadata . of ( metadata ));","title":"Outbound Metadata"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#acknowledgement","text":"By default, the Reactive Messaging Message is acknowledged when the broker acknowledges the message.","title":"Acknowledgement"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#configuration-reference","text":"Attribute ( alias ) Description Type Mandatory Default automatic-recovery-enabled Whether automatic connection recovery is enabled boolean false false automatic-recovery-on-initial-connection Whether automatic recovery on initial connections is enabled boolean false true client-options-name (rabbitmq-client-options-name) The name of the RabbitMQ Client Option bean used to customize the RabbitMQ client configuration string false connection-timeout The TCP connection timeout (ms); 0 is interpreted as no timeout int false 60000 credentials-provider-name (rabbitmq-credentials-provider-name) The name of the RabbitMQ Credentials Provider bean used to provide dynamic credentials to the RabbitMQ client string false default-routing-key The default routing key to use when sending messages to the exchange string false `` default-ttl If specified, the time (ms) sent messages can remain in queues undelivered before they are dead long false exchange.auto-delete Whether the exchange should be deleted after use boolean false false exchange.declare Whether to declare the exchange; set to false if the exchange is expected to be set up independently boolean false true exchange.durable Whether the exchange is durable boolean false true exchange.name The exchange that messages are published to or consumed from. If not set, the channel name is used. If set to \"\", the default exchange is used. string false exchange.type The exchange type: direct, fanout, headers or topic (default) string false topic handshake-timeout The AMQP 0-9-1 protocol handshake timeout (ms) int false 10000 host (rabbitmq-host) The broker hostname string false localhost include-properties Whether to include properties when a broker message is passed on the event bus boolean false false max-inflight-messages The maximum number of messages to be written to RabbitMQ concurrently; must be a positive number long false 1024 max-outgoing-internal-queue-size The maximum size of the outgoing internal queue int false network-recovery-interval How long (ms) will automatic recovery wait before attempting to reconnect int false 5000 password (rabbitmq-password) The password used to authenticate to the broker string false port (rabbitmq-port) The broker port int false 5672 reconnect-attempts (rabbitmq-reconnect-attempts) The number of reconnection attempts int false 100 reconnect-interval (rabbitmq-reconnect-interval) The interval (in seconds) between two reconnection attempts int false 10 requested-channel-max The initially requested maximum channel number int false 2047 requested-heartbeat The initially requested heartbeat interval (seconds), zero for none int false 60 ssl (rabbitmq-ssl) Whether or not the connection should use SSL boolean false false tracing.attribute-headers A comma-separated list of headers that should be recorded as span attributes. Relevant only if tracing.enabled=true string false `` tracing.enabled Whether tracing is enabled (default) or disabled boolean false true trust-all (rabbitmq-trust-all) Whether to skip trust certificate verification boolean false false trust-store-password (rabbitmq-trust-store-password) The password of the JKS trust store string false trust-store-path (rabbitmq-trust-store-path) The path to a JKS trust store string false use-nio Whether usage of NIO Sockets is enabled boolean false false user The user name to use when connecting to the broker string false guest username (rabbitmq-username) The username used to authenticate to the broker string false virtual-host (rabbitmq-virtual-host) The virtual host to use when connecting to the broker string false /","title":"Configuration Reference"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#using-existing-destinations","text":"To use an existing exchange , you may need to configure the exchange.name attribute. For example, if you have a RabbitMQ broker already configured with an exchange called people that you wish to send messages to, you need the following configuration: 1 mp.messaging.outgoing.people.connector = smallrye-rabbitmq You would need to configure the exchange.name attribute, if the exchange name were not the channel name: 1 2 mp.messaging.outgoing.people-out.connector = smallrye-rabbitmq mp.messaging.outgoing.people-out.exchange.name = people If you want RabbitMQ to create the people exchange, you need the following configuration: 1 2 3 mp.messaging.outgoing.people-out.connector = smallrye-amqp mp.messaging.outgoing.people-out.exchange.name = people mp.messaging.outgoing.people-out.exchange.declare = true Note The above example will create a topic exchange and use an empty default routing-key (unless overridden programatically using outgoing metadata for the message). If you want to create a different type of exchange or have a different default routing key, then the exchange.type and default-routing-key properties need to be explicitly specified.","title":"Using existing destinations"},{"location":"rabbitmq/sending-messages-to-rabbitmq/#sending-to-specific-queues-via-the-default-exchange","text":"To send a message to a specific queue (usually a reply queue), you have to configure the default exchange as an outgoing channel and set the name of the queue as routing key in the message metadata. The name of the exchange needs to be set to \"\" . 1 2 mp.messaging.outgoing.channel-name-for-default-exchange.connector = smallrye-rabbitmq mp.messaging.outgoing.channel-name-for-default-exchange.exchange.name = \"\"","title":"Sending to specific queues via the default exchange"}]}