[#kafka-inbound]
== Receiving Kafka Records

The Kafka Connector retrieves Kafka Records from Kafka Brokers and maps each of them to Reactive Messaging `Messages`.

=== Example

Let's imagine you have a Kafka broker running, and accessible using the `kafka:9092` address (by default it would use `localhost:9092`).
Configure your application to receive Kafka records from a Kafka _topic_ on the `prices` channel as follows:

[source]
----
kafka.bootstrap.servers=kafka:9092      # <1>

mp.messaging.incoming.prices.connector=smallrye-kafka       # <2>
mp.messaging.incoming.prices.value.deserializer=org.apache.kafka.common.serialization.DoubleDeserializer    # <3>
mp.messaging.incoming.prices.broadcast=true     # <4>
----
1. Configure the broker location. You can configure it globally or per channel
2. Configure the connector to manage the `prices` channel
3. Sets the (Kafka) deserializer to read the record's value
4. Make sure that we can receive from more that one consumer (see `KafkaPriceConsumer` and `KafkaPriceMessageConsumer` below)

NOTE: You don't need to set the Kafka topic. By default, it uses the channel name (`prices`). You can configure the `topic` attribute to override it.

Then, your application receives `Message<Double>`.
You can consume the payload directly:

[source, java]
----
include::example$inbound/KafkaPriceConsumer.java[]
----

Or, you can retrieve the `Message<Double>`:

[source, java]
----
include::example$inbound/KafkaPriceMessageConsumer.java[]
----

=== Deserialization

The deserialization is handled by the underlying Kafka Client.
You need to configure the:

* `mp.messaging.incoming.[channel-name].value.deserializer` to configure the value deserializer (mandatory)
* `mp.messaging.incoming.[channel-name].key.deserializer` to configure the key deserializer (optional, default to `String`)

If you want to use a custom deserializer, add it to your `CLASSPATH` and configure the associate attribute.

In addition, the Kafka Connector also provides a set of _message converters_.
So you can receive _payloads_ representing records from Kafka using:

* {javadoc-base}/io/smallrye/reactive/messaging/kafka/Record.html[Record<K,V>] - a pair key/value
* https://kafka.apache.org/26/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html[KafkaConsumer<K,V>] - a structure representing the record with all its metadata

[source, java]
----
include::example$inbound/Converters.java[tags=code]
----

=== Inbound Metadata

Messages coming from Kafka contains an instance of {javadoc-base}/io/smallrye/reactive/messaging/kafka/IncomingKafkaRecordMetadata.html[IncomingKafkaRecordMetadata<K, T>] in the metadata.
`K` is the type of the record's key.
`T` is the type of the record's value.
It provides the key, topic, partitions, headers and so on:

[source, java]
----
include::example$inbound/KafkaMetadataExample.java[tags=code]
----

=== Acknowledgement


If a message produced from a Kafka record is _acked_, a commit strategy is applied.
The Kafka connector supports 3 strategies:

* `latest` - Will commit the record offset received by the Kafka consumer (if higher than the previously committed offset).
This strategy provides at-least-once delivery if the channel processes the message without performing  any asynchronous processing.
This strategy should not be used on high-load as offset commit is expensive.
* `ignore` - Performs no commit.
Is the default when `enable.auto.commit` is `true`.
This strategy provides _at-least-once delivery_ if the channel processes the message without performing any asynchronous operations and when `enable.auto.commit` is `true`.
* `throttled` -  Will keep track of received messages and commit to the next offset after the latest _acked_ message in sequence.
Will commit periodically as defined by `auto.commit.interval.ms` (default: 5000). T
his strategy mimics the behavior of the kafka consumer when `enable.auto.commit` is `true`.
The connector will be marked as unhealthy in the presence of any received record that has gone too long without being processed as defined by `throttled.unprocessed-record-max-age.ms` (default: 60000).
If `throttled.unprocessed-record-max-age.ms` is set to less than or equal to 0 then will not perform any health check (this might lead to running out of memory).
This strategy guarantees _at-least-once delivery_ even if the channel performs asynchronous processing.

IMPORTANT: The Kafka connector disables the Kafka _auto commit_ is not explicitly enabled.
This behavior differs from the traditional Kafka consumer.

If high-throughout is important for you, and not limited by the downstream, we recommend to either:

* Use the `throttled` policy
* or set `enable.auto.commit` to `true` and annotate the consuming method with `@Acknowledgment(Acknowledgment.Strategy.NONE)`

=== Failure Management

If a message produced from a Kafka record is _nacked_, a failure strategy is applied.
The Kafka connector supports 3 strategies:

* `fail` - fail the application, no more records will be processed. (default)
The offset of the record that has not been processed correctly is not committed.
* `ignore` - the failure is logged, but the processing continue.
The offset of the record that has not been processed correctly is committed.
* `dead-letter-queue` - the offset of the record that has not been processed correctly is committed, but the record is written to a (Kafka) _dead letter topic_.

The strategy is selected using the `failure-strategy` attribute.

In the case of `dead-letter-queue`, you can configure the following attributes:

* `dead-letter-queue.topic`: the topic to use to write the records not processed correctly, default is `dead-letter-topic-$channel`, with `$channel` being the name of the channel.
* `dead-letter-queue.key.serializer`: the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer.
* `dead-letter-queue.value.serializer`: the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer.

The record written on the dead letter queue contains the `dead-letter-reason` header with the nack reason (message from the exception passed to the `nack` method).
It may also contain the `dead-letter-cause` with the message from the cause, if any.

=== Receiving Cloud Events

The Kafka connector supports https://cloudevents.io/[Cloud Events].
When the connector detects a _structured_ or _binary_ Cloud Events, it adds a  {javadoc-base}/io/smallrye/reactive/messaging/kafka/IncomingKafkaCloudEventMetadata.html[IncomingKafkaRecordMetadata<K, T>] in the metadata of the Message.
`IncomingKafkaCloudEventMetadata` contains the various (mandatory and optional) Cloud Event attributes.

If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.

==== Binary Cloud Events

For `binary` Cloud Events, **all** mandatory Cloud Event attributes must be set in the record header, prefixed by `ce_` (as mandated by the https://github.com/cloudevents/spec/blob/master/kafka-protocol-binding.md[protocol binding]).
The connector considers headers starting with the `ce_` prefix but not listed in the specification as extensions.
You can access them using the `getExtension` method from `IncomingKafkaCloudEventMetadata`.
You can retrieve them as `String`.

The `datacontenttype` attribute is mapped to the `content-type` header of the record.
The `partitionkey` attribute is mapped to the record's key, if any.

Note that all headers are read as UTF-8.

With binary Cloud Events, the record's key and value can use any deserializer.

==== Structured Cloud Events

For `structured` Cloud Events, the event is encoded in the record's value.
Only JSON is supported, so your event must be encoded as JSON in the record's value.

Structured Cloud Event must set the `content-type` header of the record to `application/cloudevents` or prefix the value with `application/cloudevents` such as: `application/cloudevents+json; charset=UTF-8`.

To receive structured Cloud Events, your value deserializer must be:

* `org.apache.kafka.common.serialization.StringDeserializer`
* `org.apache.kafka.common.serialization.ByteArrayDeserializer`
* `io.vertx.kafka.client.serialization.JsonObjectDeserializer`

As mentioned previously, the value must be a valid JSON object containing at least all the mandatory Cloud Events attributes.

If the record is a structured Cloud Event, the created Message's payload is the Cloud Event `data`.

The `partitionkey` attribute is mapped to the record's key if any.


=== Configuration Reference

include::connectors:partial$META-INF/connector/smallrye-kafka-incoming.adoc[]

You can also pass any property supported by the https://vertx.io/docs/vertx-kafka-client/java/[Vert.x Kafka client] as attribute.

include::consumer-rebalance-listener.adoc[]
