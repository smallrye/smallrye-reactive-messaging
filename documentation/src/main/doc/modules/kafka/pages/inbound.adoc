[#kafka-inbound]
== Receiving Kafka Records

The Kafka Connector retrieves Kafka Records from Kafka Brokers and maps each of them to Reactive Messaging `Messages`.

=== Example

Let's imagine you have a Kafka broker running, and accessible using the `kafka:9092` address (by default it would use `localhost:9092`).
Configure your application to receive Kafka records from a Kafka _topic_ on the `prices` channel as follows:

[source]
----
kafka.bootstrap.servers=kafka:9092      # <1>

mp.messaging.incoming.prices.connector=smallrye-kafka       # <2>
mp.messaging.incoming.prices.value.deserializer=org.apache.kafka.common.serialization.DoubleDeserializer    # <3>
mp.messaging.incoming.prices.broadcast=true     # <4>
----
1. Configure the broker location. You can configure it globally or per channel
2. Configure the connector to manage the `prices` channel
3. Sets the (Kafka) deserializer to read the record's value
4. Make sure that we can receive from more that one consumer (see `KafkaPriceConsumer` and `KafkaPriceMessageConsumer` below)

NOTE: You don't need to set the Kafka topic. By default, it uses the channel name (`prices`). You can configure the `topic` attribute to override it.

Then, your application receives `Message<Double>`.
You can consume the payload directly:

[source, java]
----
include::example$inbound/KafkaPriceConsumer.java[]
----

Or, you can retrieve the `Message<Double>`:

[source, java]
----
include::example$inbound/KafkaPriceMessageConsumer.java[]
----

=== Deserialization

The deserialization is handled by the underlying Kafka Client.
You need to configure the:

* `mp.messaging.incoming.[channel-name].value.deserializer` to configure the value deserializer (mandatory)
* `mp.messaging.incoming.[channel-name].key.deserializer` to configure the key deserializer (optional, default to `String`)

If you want to use a custom deserializer, add it to your `CLASSPATH` and configure the associate attribute.

In addition, the Kafka Connector also provides a set of _message converters_.
So you can receive _payloads_ representing records from Kafka using:

* {javadoc-base}/io/smallrye/reactive/messaging/kafka/Record.html[Record<K,V>] - a pair key/value
* https://kafka.apache.org/26/javadoc/index.html?org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord<K,V>] - a structure representing the record with all its metadata

[source, java, indent=0]
----
include::example$inbound/Converters.java[tags=code]
----

=== Inbound Metadata

Messages coming from Kafka contains an instance of {javadoc-base}/io/smallrye/reactive/messaging/kafka/api/IncomingKafkaRecordMetadata.html[IncomingKafkaRecordMetadata<K, T>] in the metadata.
`K` is the type of the record's key.
`T` is the type of the record's value.
It provides the key, topic, partitions, headers and so on:

[source, java, indent=0]
----
include::example$inbound/KafkaMetadataExample.java[tags=code]
----

=== Acknowledgement

When a message produced from a Kafka record is acknowledged, the connector invokes a _commit strategy_.
These strategies decide when the consumer offset for a specific topic/partition is committed.
Committing an offset indicates that all previous records have been processed.
It is also the position where the application would restart the processing after a crash recovery or a restart.

Committing every offset has performance penalties as Kafka offset management can be slow.
However, not committing the offset often enough may lead to message duplication if the application crashes between two commits.

The Kafka connector supports three strategies:

* `throttled` keeps track of received messages and commit to the next offset after the latest _acked_ message in sequence.
This strategy guarantees _at-least-once delivery_ even if the channel performs asynchronous processing.
The connector tracks the received records and periodically (period specified by `auto.commit.interval.ms` (default: 5000)) commits the highest consecutive offset.
The connector will be marked as unhealthy if a message associated with a record is not acknowledged in `throttled.unprocessed-record-max-age.ms` (default: 60000).
Indeed, this strategy cannot commit the offset as soon as a single record processing fails (see failure-strategy to configure what happens on failing processing).
If `throttled.unprocessed-record-max-age.ms` is set to less than or equal to 0, it does not perform any health check verification. Such a setting might lead to running out of memory if there are poison pill messages.
This strategy is the default if `enable.auto.commit` is not explicitly set to `true`.

* `latest` commits the record offset received by the Kafka consumer as soon as the associated message is acknowledged (if the offset is higher than the previously committed offset).
This strategy provides _at-least-once_ delivery if the channel processes the message without performing any asynchronous processing.
This strategy should not be used on high-load as offset commit is expensive.
However, it reduces the risk of duplicates.

* `ignore` performs no commit.
This strategy is the default strategy when the consumer is explicitly configured with `enable.auto.commit` to `true`.
It delegates the offset commit to the Kafka client.
This strategy provides _at-least-once delivery_ if the channel processes the message without performing any asynchronous operations and when `enable.auto.commit` is set to `true`.
However, if the processing failed between two commits, messages received after the commit and before the failure will be re-processed.

IMPORTANT: The Kafka connector disables the Kafka _auto commit_ is not explicitly enabled.
This behavior differs from the traditional Kafka consumer.

If high-throughout is important for you, and not limited by the downstream, we recommend to either:

* Use the `throttled` policy
* or set `enable.auto.commit` to `true` and annotate the consuming method with `@Acknowledgment(Acknowledgment.Strategy.NONE)`

=== Failure Management

If a message produced from a Kafka record is _nacked_, a failure strategy is applied.
The Kafka connector supports 3 strategies:

* `fail` - fail the application, no more records will be processed. (default)
The offset of the record that has not been processed correctly is not committed.
* `ignore` - the failure is logged, but the processing continue.
The offset of the record that has not been processed correctly is committed.
* `dead-letter-queue` - the offset of the record that has not been processed correctly is committed, but the record is written to a (Kafka) _dead letter queue_ topic.

The strategy is selected using the `failure-strategy` attribute.

In the case of `dead-letter-queue`, you can configure the following attributes:

* `dead-letter-queue.topic`: the topic to use to write the records not processed correctly, default is `dead-letter-topic-$channel`, with `$channel` being the name of the channel.
* `dead-letter-queue.key.serializer`: the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer.
* `dead-letter-queue.value.serializer`: the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer.

The record written on the dead letter topic contains the original record's headers, as well as a set of additional headers about the original record:

* `dead-letter-reason` - the reason of the failure (the `Throwable` passed to `nack()`)
* `dead-letter-cause` - the cause of the failure (the `getCause()` of the `Throwable` passed to `nack()`), if any
* `dead-letter-topic` - the original topic of the record
* `dead-letter-partition` - the original partition of the record (integer mapped to String)
* `dead-letter-offset` - the original offset of the record (long mapped to String)

When using `dead-letter-queue`, it is also possible to change some metadata of the record that is sent to the dead letter topic.
To do that, use the `Message.nack(Throwable, Metadata)` method:

[source, java, indent=0]
----
include::example$inbound/KafkaDeadLetterExample.java[tags=code]
----

The `Metadata` may contain an instance of `OutgoingKafkaRecordMetadata`.
If the instance is present, the following properties will be used:

* key; if not present, the original record's key will be used
* topic; if not present, the configured dead letter topic will be used
* partition; if not present, partition will be assigned automatically
* headers; combined with the original record's headers, as well as the `dead-letter-*` headers described above

=== Handling deserialization failures

Because deserialization happens before creating a `Message`, the failure strategy presented above cannot be applied.
However, when a deserialization failure occurs, you can intercept it and provide a fallback value.
If you don't, `null` will be used as fallback value.

To achieve this, create a CDI bean implementing the {javadoc-base}/io/smallrye/reactive/messaging/kafka/DeserializationFailureHandler.html[DeserializationFailureHandler<T>] interface:

[source, java]
----
@ApplicationScoped
@Identifier("failure-fallback") // Set the name of the failure handler
public class MyDeserializationFailureHandler
    implements DeserializationFailureHandler<JsonObject> { // Specify the expected type

    @Override
    public JsonObject handleDeserializationFailure(String topic, boolean isKey,
            String deserializer, byte[] data,
            Exception exception, Headers headers) {
        return fallback;
    }
}
----

The bean must be exposed with the `@Identifier` qualifier specifying the name of the bean.
Then, in the connector configuration, specify the following attribute:

* `mp.messaging.incoming.$channel.key-deserialization-failure-handler`: name of the bean handling deserialization failures happening for the record's key
* `mp.messaging.incoming.$channel.value-deserialization-failure-handler`: name of the bean handling deserialization failures happening for the record's value,

The handler is called with the record's topic, a boolean indicating whether the failure happened on a key, the class name of the deserializer that throws the exception, the corrupted data, the exception, and the records headers augmented with headers describing the failure (which ease the write to a dead letter).
The handler can return `null` (which would be as if there were no handlers).
However, if the handler throws an exception, the application would be marked unhealthy.

=== Receiving Cloud Events

The Kafka connector supports https://cloudevents.io/[Cloud Events].
When the connector detects a _structured_ or _binary_ Cloud Events, it adds a  {javadoc-base}/io/smallrye/reactive/messaging/kafka/IncomingKafkaCloudEventMetadata.html[IncomingKafkaCloudEventMetadata<K, T>] in the metadata of the Message.
`IncomingKafkaCloudEventMetadata` contains the various (mandatory and optional) Cloud Event attributes.

If the connector cannot extract the Cloud Event metadata, it sends the Message without the metadata.

==== Binary Cloud Events

For `binary` Cloud Events, **all** mandatory Cloud Event attributes must be set in the record header, prefixed by `ce_` (as mandated by the https://github.com/cloudevents/spec/blob/v1.0/kafka-protocol-binding.md[protocol binding]).
The connector considers headers starting with the `ce_` prefix but not listed in the specification as extensions.
You can access them using the `getExtension` method from `IncomingKafkaCloudEventMetadata`.
You can retrieve them as `String`.

The `datacontenttype` attribute is mapped to the `content-type` header of the record.
The `partitionkey` attribute is mapped to the record's key, if any.

Note that all headers are read as UTF-8.

With binary Cloud Events, the record's key and value can use any deserializer.

==== Structured Cloud Events

For `structured` Cloud Events, the event is encoded in the record's value.
Only JSON is supported, so your event must be encoded as JSON in the record's value.

Structured Cloud Event must set the `content-type` header of the record to `application/cloudevents` or prefix the value with `application/cloudevents` such as: `application/cloudevents+json; charset=UTF-8`.

To receive structured Cloud Events, your value deserializer must be:

* `org.apache.kafka.common.serialization.StringDeserializer`
* `org.apache.kafka.common.serialization.ByteArrayDeserializer`
* `io.vertx.kafka.client.serialization.JsonObjectDeserializer`

As mentioned previously, the value must be a valid JSON object containing at least all the mandatory Cloud Events attributes.

If the record is a structured Cloud Event, the created Message's payload is the Cloud Event `data`.

The `partitionkey` attribute is mapped to the record's key if any.


=== Configuration Reference

include::connectors:partial$META-INF/connector/smallrye-kafka-incoming.adoc[]

You can also pass any property supported by the https://vertx.io/docs/vertx-kafka-client/java/[Vert.x Kafka client] as attribute.

include::consumer-rebalance-listener.adoc[]
