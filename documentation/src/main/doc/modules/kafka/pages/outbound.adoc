[#kafka-outbound]
== Writing Kafka Records

The Kafka Connector can write Reactive Messaging `Messages` as Kafka Records.

=== Example

Let's imagine you have a Kafka broker running, and accessible using the `kafka:9092` address (by default it would use `localhost:9092`).
Configure your application to write the messages from the `prices` channel into a Kafka _topic_ as follows:

[source]
----
kafka.bootstrap.servers=kafka:9092      # <1>

mp.messaging.outgoing.prices-out.connector=smallrye-kafka   # <2>
mp.messaging.outgoing.prices-out.value.serializer=org.apache.kafka.common.serialization.DoubleSerializer  # <3>
mp.messaging.outgoing.prices-out.topic=prices   # <4>
----
1. Configure the broker location. You can configure it globally or per channel
2. Configure the connector to manage the `prices` channel
3. Sets the (Kafka) serializer to encode the message payload into the record's value
4. Make sure the topic name is `prices` (and not the default `prices-out`)

Then, your application must send `Message<Double>` to the `prices` channel.
It can use `double` payloads as in the following snippet:

[source, java]
----
include::example$outbound/KafkaPriceProducer.java[]
----

Or, you can send `Message<Double>`:

[source, java]
----
include::example$outbound/KafkaPriceMessageProducer.java[]
----

=== Serialization

The serialization is handled by the underlying Kafka Client.
You need to configure the:

* `mp.messaging.outgoing.[channel-name].value.serializer` to configure the value serializer (mandatory)
* `mp.messaging.outgoing.[channel-name].key.serializer` to configure the key serializer (optional, default to `String`)

If you want to use a custom serializer, add it to your `CLASSPATH` and configure the associate attribute.

By default, the written record contains:

* the `Message` payload as _value_
* no key, or the key configured using the `key` attribute or the key passed in the metadata attached to the `Message`
* the timestamp computed for the system clock (`now`) or the timestamp passed in the metadata attached to the `Message`

=== Outbound Metadata

When sending `Messages`, you can add an instance of {javadoc-base}/apidocs/io/smallrye/reactive/messaging/kafka/OutgoingKafkaRecordMetadata.html[`OutgoingKafkaRecordMetadata`] to influence how the message is going to written to Kafka.
For example, you can configure the topic directly in the message, or add Kafka headers, configure the record key...

[source, java]
----
include::example$outbound/KafkaOutboundMetadataExample.java[tag=code]
----

=== Acknowledgement

Kafka acknowledgement can take times depending on the configuration.
Also, it stores in-memory the records that cannot be written.

By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received `Message`).
You can disable this by setting the `waitForWriteCompletion` attribute to `false`.

Note that the `acks` attribute has a huge impact on the record acknowledgement.

If a record cannot be written, the message is `nacked`.

=== Back-pressure and inflight records

The Kafka outbound connector handles back-pressure using the number of in-flight messages waiting for being acknowledged by the Kafka broker.
The number of in-flight messages is configured using the `max-inflight-messages`  attribute and default to 5 (which is the Kafka default).

The connector only handles that amount of messages concurrently, requesting a new one when one of the in-flight message has been acknowledged by the broker.

=== Sending Cloud Events

The Kafka connector supports https://cloudevents.io/[Cloud Events].
The connector sends the outbound record as Cloud Events if:

* the message metadata contains an `io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata` instance,
* the channel configuration defines the `cloud-events-type` and `cloud-events-source` attribute.

You can create `io.smallrye.reactive.messaging.ce.OutgoingCloudEventMetadata` instances using:

[source, java]
----
include::example$outbound/KafkaCloudEventProcessor.java[]
----

If the metadata does not contain an id, the connector generates one (random UUID).
The `type` and `source` can be configured per message or at the channel level using the `cloud-events-type` and `cloud-events-source` attributes.
Other attributes are also configurable.

The metadata can be contributed by multiple methods, however, you must always retrieve the already existing metadata to avoid overriding the values:

[source, java]
----
include::example$outbound/KafkaCloudEventMultipleProcessors.java[]
----

By default, the connector sends the Cloud Events using the _binary_ format.
You can write _structured_ Cloud Events by setting the `cloud-events-mode` to `structured`.
Only JSON is supported, so the created records had it's `content-type` header set to `application/cloudevents+json; charset=UTF-8`
When using the _structured_ mode, the value serializer must be set to `org.apache.kafka.common.serialization.StringSerializer`, otherwise the connector reports the error.
In addition, in _structured_, the connector maps the message's payload to JSON, except for `String` passed directly.

The record's key can be set in the channel configuration (`key` attribute), in the `OutgoingKafkaRecordMetadata` or using the `partitionkey` Cloud Event attribute.

NOTE: you can disable the Cloud Event support by setting the `cloud-events` attribute to `false`

=== Configuration Reference

include::connectors:partial$META-INF/connector/smallrye-kafka-outgoing.adoc[]

You can also pass any property supported by the https://vertx.io/docs/vertx-kafka-client/java/[Vert.x Kafka client].
